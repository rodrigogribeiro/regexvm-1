\documentclass[oneside,12pt]{scrbook}

\makeatletter
\DeclareOldFontCommand{\rm}{\normalfont\rmfamily}{\mathrm}
\DeclareOldFontCommand{\sf}{\normalfont\sffamily}{\mathsf}
\DeclareOldFontCommand{\tt}{\normalfont\ttfamily}{\mathtt}
\DeclareOldFontCommand{\bf}{\normalfont\bfseries}{\mathbf}
\DeclareOldFontCommand{\it}{\normalfont\itshape}{\mathit}
\DeclareOldFontCommand{\sl}{\normalfont\slshape}{\@nomath\sl}
\DeclareOldFontCommand{\sc}{\normalfont\scshape}{\@nomath\sc}
\makeatother

\usepackage{booktabs} % For formal tables
\usepackage{graphicx,hyperref}
\usepackage{float}
\usepackage{proof,tikz}
\usepackage{amssymb,amsthm,stmaryrd, minted}

\usetikzlibrary{arrows}

\makeatletter
\@ifundefined{lhs2tex.lhs2tex.sty.read}%
{\@namedef{lhs2tex.lhs2tex.sty.read}{}%
	\newcommand\SkipToFmtEnd{}%
	\newcommand\EndFmtInput{}%
	\long\def\SkipToFmtEnd#1\EndFmtInput{}%
}\SkipToFmtEnd

\newcommand\ReadOnlyOnce[1]{\@ifundefined{#1}{\@namedef{#1}{}}\SkipToFmtEnd}
\usepackage{amstext}
\usepackage{amssymb}
\usepackage{stmaryrd}
\DeclareFontFamily{OT1}{cmtex}{}
\DeclareFontShape{OT1}{cmtex}{m}{n}
{<5><6><7><8>cmtex8
	<9>cmtex9
	<10><10.95><12><14.4><17.28><20.74><24.88>cmtex10}{}
\DeclareFontShape{OT1}{cmtex}{m}{it}
{<-> ssub * cmtt/m/it}{}
\newcommand{\texfamily}{\fontfamily{cmtex}\selectfont}
\DeclareFontShape{OT1}{cmtt}{bx}{n}
{<5><6><7><8>cmtt8
	<9>cmbtt9
	<10><10.95><12><14.4><17.28><20.74><24.88>cmbtt10}{}
\DeclareFontShape{OT1}{cmtex}{bx}{n}
{<-> ssub * cmtt/bx/n}{}
\newcommand{\tex}[1]{\text{\texfamily#1}}	% NEU

\newcommand{\Sp}{\hskip.33334em\relax}


\newcommand{\Conid}[1]{\mathit{#1}}
\newcommand{\Varid}[1]{\mathit{#1}}
\newcommand{\anonymous}{\kern0.06em \vbox{\hrule\@width.5em}}
\newcommand{\plus}{\mathbin{+\!\!\!+}}
\newcommand{\bind}{\mathbin{>\!\!\!>\mkern-6.7mu=}}
\newcommand{\rbind}{\mathbin{=\mkern-6.7mu<\!\!\!<}}% suggested by Neil Mitchell
\newcommand{\sequ}{\mathbin{>\!\!\!>}}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\usepackage{polytable}

%mathindent has to be defined
\@ifundefined{mathindent}%
{\newdimen\mathindent\mathindent\leftmargini}%
{}%

\def\resethooks{%
	\global\let\SaveRestoreHook\empty
	\global\let\ColumnHook\empty}
\newcommand*{\savecolumns}[1][default]%
{\g@addto@macro\SaveRestoreHook{\savecolumns[#1]}}
\newcommand*{\restorecolumns}[1][default]%
{\g@addto@macro\SaveRestoreHook{\restorecolumns[#1]}}
\newcommand*{\aligncolumn}[2]%
{\g@addto@macro\ColumnHook{\column{#1}{#2}}}

\resethooks

\newcommand{\onelinecommentchars}{\quad-{}- }
\newcommand{\commentbeginchars}{\enskip\{-}
\newcommand{\commentendchars}{-\}\enskip}

\newcommand{\visiblecomments}{%
	\let\onelinecomment=\onelinecommentchars
	\let\commentbegin=\commentbeginchars
	\let\commentend=\commentendchars}

\newcommand{\invisiblecomments}{%
	\let\onelinecomment=\empty
	\let\commentbegin=\empty
	\let\commentend=\empty}

\visiblecomments

\newlength{\blanklineskip}
\setlength{\blanklineskip}{0.66084ex}

\newcommand{\haskell}[1]{\mintinline{haskell}|#1|}

\newcommand{\hsindent}[1]{\quad}% default is fixed indentation
\let\hspre\empty
\let\hspost\empty
\newcommand{\NB}{\textbf{NB}}
\newcommand{\Todo}[1]{$\langle$\textbf{To do:}~#1$\rangle$}

\EndFmtInput
\makeatother
%
%
%
%
%
%
% This package provides two environments suitable to take the place
% of hscode, called "plainhscode" and "arrayhscode". 
%
% The plain environment surrounds each code block by vertical space,
% and it uses \abovedisplayskip and \belowdisplayskip to get spacing
% similar to formulas. Note that if these dimensions are changed,
% the spacing around displayed math formulas changes as well.
% All code is indented using \leftskip.
%
% Changed 19.08.2004 to reflect changes in colorcode. Should work with
% CodeGroup.sty.
%
\ReadOnlyOnce{polycode.fmt}%
\makeatletter

\newcommand{\hsnewpar}[1]%
{{\parskip=0pt\parindent=0pt\par\vskip #1\noindent}}

% can be used, for instance, to redefine the code size, by setting the
% command to \small or something alike
\newcommand{\hscodestyle}{}

% The command \sethscode can be used to switch the code formatting
% behaviour by mapping the hscode environment in the subst directive
% to a new LaTeX environment.

\newcommand{\sethscode}[1]%
{\expandafter\let\expandafter\hscode\csname #1\endcsname
	\expandafter\let\expandafter\endhscode\csname end#1\endcsname}

% "compatibility" mode restores the non-polycode.fmt layout.

\newenvironment{compathscode}%
{\par\noindent
	\advance\leftskip\mathindent
	\hscodestyle
	\let\\=\@normalcr
	\let\hspre\(\let\hspost\)%
	\pboxed}%
{\endpboxed\)%
	\par\noindent
	\ignorespacesafterend}

\newcommand{\compaths}{\sethscode{compathscode}}

% "plain" mode is the proposed default.
% It should now work with \centering.
% This required some changes. The old version
% is still available for reference as oldplainhscode.

\newenvironment{plainhscode}%
{\hsnewpar\abovedisplayskip
	\advance\leftskip\mathindent
	\hscodestyle
	\let\hspre\(\let\hspost\)%
	\pboxed}%
{\endpboxed%
	\hsnewpar\belowdisplayskip
	\ignorespacesafterend}

\newenvironment{oldplainhscode}%
{\hsnewpar\abovedisplayskip
	\advance\leftskip\mathindent
	\hscodestyle
	\let\\=\@normalcr
	\(\pboxed}%
{\endpboxed\)%
	\hsnewpar\belowdisplayskip
	\ignorespacesafterend}

% Here, we make plainhscode the default environment.

\newcommand{\plainhs}{\sethscode{plainhscode}}
\newcommand{\oldplainhs}{\sethscode{oldplainhscode}}
\plainhs

% The arrayhscode is like plain, but makes use of polytable's
% parray environment which disallows page breaks in code blocks.

\newenvironment{arrayhscode}%
{\hsnewpar\abovedisplayskip
	\advance\leftskip\mathindent
	\hscodestyle
	\let\\=\@normalcr
	\(\parray}%
{\endparray\)%
	\hsnewpar\belowdisplayskip
	\ignorespacesafterend}

\newcommand{\arrayhs}{\sethscode{arrayhscode}}

% The mathhscode environment also makes use of polytable's parray 
% environment. It is supposed to be used only inside math mode 
% (I used it to typeset the type rules in my thesis).

\newenvironment{mathhscode}%
{\parray}{\endparray}

\newcommand{\mathhs}{\sethscode{mathhscode}}

% texths is similar to mathhs, but works in text mode.

\newenvironment{texthscode}%
{\(\parray}{\endparray\)}

\newcommand{\texths}{\sethscode{texthscode}}

% The framed environment places code in a framed box.

\def\codeframewidth{\arrayrulewidth}
\RequirePackage{calc}

\newenvironment{framedhscode}%
{\parskip=\abovedisplayskip\par\noindent
	\hscodestyle
	\arrayrulewidth=\codeframewidth
	\tabular{@{}|p{\linewidth-2\arraycolsep-2\arrayrulewidth-2pt}|@{}}%
	\hline\framedhslinecorrect\\{-1.5ex}%
	\let\endoflinesave=\\
	\let\\=\@normalcr
	\(\pboxed}%
{\endpboxed\)%
	\framedhslinecorrect\endoflinesave{.5ex}\hline
	\endtabular
	\parskip=\belowdisplayskip\par\noindent
	\ignorespacesafterend}

\newcommand{\framedhslinecorrect}[2]%
{#1[#2]}

\newcommand{\framedhs}{\sethscode{framedhscode}}

% The inlinehscode environment is an experimental environment
% that can be used to typeset displayed code inline.

\newenvironment{inlinehscode}%
{\(\def\column##1##2{}%
	\let\>\undefined\let\<\undefined\let\\\undefined
	\newcommand\>[1][]{}\newcommand\<[1][]{}\newcommand\\[1][]{}%
	\def\fromto##1##2##3{##3}%
	\def\nextline{}}{\) }%

\newcommand{\inlinehs}{\sethscode{inlinehscode}}

% The joincode environment is a separate environment that
% can be used to surround and thereby connect multiple code
% blocks.

\newenvironment{joincode}%
{\let\orighscode=\hscode
	\let\origendhscode=\endhscode
	\def\endhscode{\def\hscode{\endgroup\def\@currenvir{hscode}\\}\begingroup}
	%\let\SaveRestoreHook=\empty
	%\let\ColumnHook=\empty
	%\let\resethooks=\empty
	\orighscode\def\hscode{\endgroup\def\@currenvir{hscode}}}%
{\origendhscode
	\global\let\hscode=\orighscode
	\global\let\endhscode=\origendhscode}%

\makeatother
\EndFmtInput
%

\DeclareMathAlphabet{\mathkw}{OT1}{cmss}{bx}{n}

\usetikzlibrary{automata}
\usetikzlibrary{shapes}
\usetikzlibrary{backgrounds}
\usetikzlibrary{positioning}

\newtheorem{Lemma}{Lemma}
\newtheorem{Theorem}{Theorem}
\theoremstyle{definition}
\newtheorem{Example}{Example}

\usepackage{color}
\newcommand{\redFG}[1]{\textcolor[rgb]{0.6,0,0}{#1}}
\newcommand{\greenFG}[1]{\textcolor[rgb]{0,0.4,0}{#1}}
\newcommand{\blueFG}[1]{\textcolor[rgb]{0,0,0.8}{#1}}
\newcommand{\orangeFG}[1]{\textcolor[rgb]{0.8,0.4,0}{#1}}
\newcommand{\purpleFG}[1]{\textcolor[rgb]{0.4,0,0.4}{#1}}
\newcommand{\yellowFG}[1]{\textcolor{yellow}{#1}}
\newcommand{\brownFG}[1]{\textcolor[rgb]{0.5,0.2,0.2}{#1}}
\newcommand{\blackFG}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\whiteFG}[1]{\textcolor[rgb]{1,1,1}{#1}}
\newcommand{\yellowBG}[1]{\colorbox[rgb]{1,1,0.2}{#1}}
\newcommand{\brownBG}[1]{\colorbox[rgb]{1.0,0.7,0.4}{#1}}

\newcommand{\ColourStuff}{
	\newcommand{\red}{\redFG}
	\newcommand{\green}{\greenFG}
	\newcommand{\blue}{\blueFG}
	\newcommand{\orange}{\orangeFG}
	\newcommand{\purple}{\purpleFG}
	\newcommand{\yellow}{\yellowFG}
	\newcommand{\brown}{\brownFG}
	\newcommand{\black}{\blackFG}
	\newcommand{\white}{\whiteFG}
}

\newcommand{\MonochromeStuff}{
	\newcommand{\red}{\blackFG}
	\newcommand{\green}{\blackFG}
	\newcommand{\blue}{\blackFG}
	\newcommand{\orange}{\blackFG}
	\newcommand{\purple}{\blackFG}
	\newcommand{\yellow}{\blackFG}
	\newcommand{\brown}{\blackFG}
	\newcommand{\black}{\blackFG}
	\newcommand{\white}{\blackFG}
}

\ColourStuff

\newcommand{\D}[1]{\blue{\mathsf{#1}}}
\newcommand{\C}[1]{\red{\mathsf{#1}}}
\newcommand{\F}[1]{\green{\mathsf{#1}}}
\newcommand{\V}[1]{\black{\mathsf{#1}}}
\newcommand{\TC}[1]{\purple{\mathsf{#1}}}


\newcommand{\Eps}{\textit{Eps}}
\newcommand{\Chr}{\textit{Chr}}
\newcommand{\Catt}{\textit{Cat}}
\newcommand{\Left}{\textit{Left}}
\newcommand{\Right}{\textit{Right}}
\newcommand{\StarBase}{\textit{StarBase}}
\newcommand{\StarRec}{\textit{StarRec}}
\newcommand{\sembrackets}[1]{\ensuremath{\llbracket #1 \rrbracket}}
\newcommand{\flatten}{\ensuremath{\texttt{flatten}}}
\newcommand{\inl}{\ensuremath{\texttt{inl}}}
\newcommand{\inr}{\ensuremath{\texttt{inr}}}
\newcommand{\code}{\ensuremath{\texttt{code}}}
\newcommand{\decodee}{\ensuremath{\texttt{decode}}}
\newcommand{\decodeo}{\ensuremath{\texttt{decode1}}}
\newcommand{\leti}{\ensuremath{\textbf{\texttt{let}}}}
\newcommand{\iin}{\ensuremath{\textbf{\texttt{in}}}}
\newcommand{\iif}{\ensuremath{\textbf{\texttt{if}}}}
\newcommand{\tthen}{\ensuremath{\textbf{\texttt{then}}}}
\newcommand{\eelse}{\ensuremath{\textbf{\texttt{else}}}}
\newcommand{\eerror}{\ensuremath{\textbf{\texttt{error}}}}
\newcommand{\nullable}{\ensuremath{\texttt{nullable}}}
\newcommand{\emptyy}{\ensuremath{\texttt{empty}}}
\newcommand{\fin}{\ensuremath{\texttt{f$_{\texttt{in}}$}}}
\newcommand{\fout}{\ensuremath{\texttt{f$_{\texttt{out}}$}}}
\newcommand{\size}{\ensuremath{\texttt{size}}}

\newcommand{\coq}[1]{\mintinline{coq}|#1|}

\newcommand{\conf}[1]{\ensuremath{\langle #1 \rangle}}

\usepackage{scrhack}

\usepackage{ucs}
\usepackage[utf8x]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{url}
\usepackage{stmaryrd}
\usepackage{ifpdf, proof}
\ifpdf
\usepackage{hyperref}
\fi

\usepackage{amsthm}

\theoremstyle{plain}

\usepackage{setspace}
%\usepackage{scrpage2}
\usepackage{listings,epigraph}
\usepackage{verbatim}
\usepackage{tikz}
\usetikzlibrary{automata,arrows,positioning,trees}

%\pagestyle{scrheadings}
%\ihead{}
%\chead{}
%\ohead[]{\headmark}
%\cfoot[]{}
%\ofoot[\pagemark]{\pagemark}
%
%\renewcommand*{\headfont}{\normalfont \itshape}
%\renewcommand*{\pnumfont}{\normalfont \bfseries}

\DeclareMathAlphabet{\mathkw}{OT1}{cmss}{bx}{n}

%include lhs2TeX.fmt
%include lhs2TeX.sty
%include polycode.fmt

%subst keyword a = "\mathkw{" a "}"
%subst conid a = "\V{" a "}"
%subst varid a = "\V{" a "}"
%subst numeral a = "\C{" a "}"
%subst comment a = "\orange{\texttt{--" a "}}"

\theoremstyle{definition}
\newtheorem{Definition}{Definition}

\usepackage{color}

\begin{document}

\begin{titlepage}
\centering
\vspace*{1in}
\begin{Large}\bfseries
%Certified Virtual Machine-based Parsing of Regular Expressions\par
Certified Virtual Machine Based Regular Expression Parsing \par
\end{Large}
\vspace{1.5in}
\begin{large}\bfseries
Thales Antônio Delfino\par
\end{large}
\vfill
A Dissertation submitted for the degree of Master in Computer Science.
\par
\vspace{0.5in}
Departamento de Computação
\par
Universidade Federal de Ouro Preto
\par
\vspace{0.5in}
\today
\par
\end{titlepage}


\frontmatter
\tableofcontents

%----------------------------------------------------------------------------------------------

\chapter{Abstract}\label{chapter:abstract}

%Old abstract
\begin{comment}
Regular expressions (REs) are pervasive in computing. We use RE in text editors, string search tools (like GNU-Grep) and lexical analyzers generators. Most of these tools rely on converting REs to its corresponding finite state machine or use REs derivatives for directly
parse an input string. In this work, we want to investigate the suitability of another approach: instead of using derivatives or generate a finite state machine for a given RE, we will develop a certified virtual machine-based algorithm for parsing REs, in such a way that a RE is merely a program executed by the VM over the input string. First, we present a small-step semantics for the proposed algorithm, showing all possible cases for RE matching. Then we implement that semantics in Haskell and test it with counterexamples in order to find errors that could make the task of proving the algorithm's correctness harder - and even impossible. Finally, we use a proof assistant to verify theoretical properties of the developed VM-based algorithm's semantics, proving its correctness.
\end{comment}

%New Abstract
Regular expressions (REs) are pervasive in computing. We use REs in text editors, string search tools (like GNU-Grep) and lexical analyzers generators. Most of these tools rely on converting REs to its corresponding finite state machine or use REs derivatives for directly parse an input string. In this work, we investigated the suitability of another approach: instead of using derivatives or generating a finite state machine for a given RE, we developed a certified virtual machine-based algorithm (VM) for parsing REs, in such a way that a RE is merely a program executed by the VM over the input string. First, we developed a primary semantics for the algorithm (a small-step semantics), implemented it in Haskell, tested it using QuickCheck and provided proof sketches of its correctness with respect to RE standard inductive semantics. After that, we developed another semantics (a big-step semantics). Unlike the previous one, the new semantics can deal with problematic REs. We showed that the new semantics for our VM is also sound and complete with regard to a standard inductive semantics for REs and that the evidence produced by it denotes a valid parsing result. All of our results are formalized in Coq proof assistant and from it we extract a certified algorithm which we use to build a RE parsing tool using Haskell programming language. Experiments comparing the efficiency of our algorithm with other approaches implemented using Haskell are reported.

%----------------------------------------------------------------------------------------------

\mainmatter

\chapter{Introduction}\label{chapter:introduction}

\epigraph{Correctness is clearly the prime quality. If a system does
  not do what it is supposed to do, then everything else about it
  matters little.}{Bertrand Meyer, Designer of Eiffel Programming Language.}


We name parsing the process of analyzing if a sequence of symbols matches a given set of rules.
Such rules are usually specified in a formal notation, like a grammar. If a string can be obtained
from those rules, we have success: we can build some evidence that the input is in the language
described by the underlying formalism. Otherwise, we have a failure: no such evidence exists.

In this work, we focus on the parsing problem for regular expressions (REs), which are an algebraic
and compact way of defining regular languages (RLs), i.e., languages that can be recognized by
(non-)deterministic finite automata and equivalent formalisms. REs are widely used in string search
tools, lexical analyzer generators and XML schema languages~\cite{Frisch2004}. Since RE parsing
is pervasive in computing, its correctness is crucial. Nowadays, with the recent
development of languages with dependent types and proof assistants it has become
possible to represent algorithmic properties as program types which are verified
by the compiler. The usage of proof assistants to verify RE parsing / matching algorithms
were the subject of study of several recent research works (e.g ~\cite{Firsov13,Ribeiro2017,Lopes2016,Asperti10}).

Approaches for RE parsing can use representations of finite state machines (e.g.~\cite{Firsov13}),
derivatives (e.g.~\cite{Ribeiro2017,Lopes2018,Lopes2016}) or the so-called pointed RE's or its
variants~\cite{Asperti10,Fischer2010}. Another approach for parsing is based on the so-called
parsing machines, which dates back to 70's with Knuth's work
on top-down syntax analysis for context-free languages~\cite{Knuth71}. Recently, some works
have tried to revive the use of such machines for parsing: Cox~\cite{Cox2009} defined a VM
for which a RE can be seen as ``high-level programs'' that can be compiled to a sequence of
such VM instructions and Lua library LPEG~\cite{Ierusalimschy2009} defines a VM whose instruction
set can be used to compile Parser Expressions Grammars (PEGs)~\cite{Ford04}. Such renewed research
interest is motivated by the fact that is possible to include new features by just adding and
implementing new machine instructions.

Since LPEG VM is designed with PEGs in mind, it is not appropriate for RE parsing, since the ``star''
operator for PEGs has a greedy semantics which differs from the conventional RE semantics for this operator. 
Also, Cox's work on VM-based RE parsing has problems. First, it is poorly specified: both the VM semantics and the RE
compilation process are described only informally and no correctness guarantees are even mentioned. Second,
\begin{comment}
it does not provide a specific disambiguation strategy for dealing with ambiguous REs: both the ``star'' and ``choice''
operators for REs can introduce more than one possible way to process a given string. In such circumstances,
the parsing algorithm must choose one way to process the input. Two well-known disambiguation strategies are
the greedy~\cite{Frisch2004} and POSIX~\cite{Sulzmann14}.
 \end{comment}
it does not provide an evidence for matching, which could be used to characterize a disambiguation strategy, like
Greedy~\cite{Frisch2004} and POSIX~\cite{Sulzmann14}. To the best of our knowledge, no 
previous work has formally defined a VM for RE parsing that produces evidence (parse trees) for successful matches.
The objective of this work is to give a first step in filling this gap. More specifically, we are interested in formally
specify and prove the correctness of a VM based semantics for RE parsing which produces bit-codes as
a memory efficient representation of parse-trees. As pointed by~\cite{Lasse2011}, bit-codes are useful because they
are not only smaller than the parse tree, but also smaller than the string being parsed and they can be combined with methods
for text compression. We would like to emphasize that, unlike Cox's work, which develop its VM using a instruction 
set like syntax and semantics, we use, as inspiration, VMs for the~$\lambda$-calculus, like the 
SECD and Krivine machines~\cite{Krivine07,Landin64}. 

One important issue regarding RE parsing is how to deal with the so-called problematic 
RE\footnote{We say that a RE $e$ is problematic if there's exists $e_1$ s.t. $e = e_1^\star$ and 
	$e_1$ accepts the empty string.}\cite{Frisch2004}. In order to avoid the well-known issues with 
problematic RE, we use a transformation proposed by Medeiros et. al.~\cite{Medeiros14} which turns a
problematic RE into an equivalent non-problematic one. We proved that this algorithm indeed produces
equivalent REs using Coq proof assistant.

\section{Objectives}\label{section:objectives}

The main objective of this dissertation is to develop a VM-based RE parsing algorithm and formally verify its relevant correctness properties (completeness and soundness with standard RE semantics\footnote{We say that the VM semantics is sound with respect to standard RE semantics if, and only if, every string accepted by the VM is also accepted by the RE semantics. In the other hand, we say that a VM semantics is complete if, and only if, all strings accepted by the RE semantics are also accepted by the VM.}.) %and guarantee that it follows the greedy disambiguation strategy.

\section{Contributions}\label{section:contributions}
Our contributions are:

\begin{itemize}
	\item We present a small-step semantics for RE inspired by  Thompson's NFA\footnote{Non-deterministic finite automata.}
	construction~\cite{Thompson1968}. The main novelty of this presentation is
	the use of data-type derivatives, a well-known concept in functional programming community, to represent
	the context in which the current RE being evaluated occur. We show informal proofs\footnote{By ``informal proofs'' we
		mean proofs that are not mechanized in a proof-assistant.} that our semantics is sound and complete with respect to RE inductive semantics.
	 % \item We presented a big step semantics for a VM for RE parsing which produces bit-codes as parsing evidence.
	 \item We describe a prototype implementation of our semantics in Haskell and use QuickCheck~\cite{Claessen2000} to test our semantics
	 against a simple implementation of RE parsing, presented in~\cite{Fischer2010}, which we prove correct in the Appendix~\ref{appendix:accept}. 
	 \item We show how our proposed semantics can produce bit codes that denote parse trees~\cite{Lasse2011} and test that
	 such generated codes correspond to valid parsing evidence using QuickCheck.
	 Our test cases cover both accepted and rejected strings for randomly generated REs. We are aware that using automated testing is not sufficient to ensure correctness, but it can expose bugs before using more formal approaches,
	 like formalizing our algorithm in a proof assistant. Such semantic prototyping step is crucial since it can avoid proof attempts that are
	 doomed to fail due to incorrect definitions.
	 	\item We develop a certified implementation of an algorithm that converts a problematic RE into a non-problematic one.
	 \item We present a big-step operational semantics, which is simpler to understand and easier to formalize than the small-step one. Plus, the big-step operational semantics uses the above mentioned algorithm to deal correctly with problematic RE (unlike our previous small-step semantics) and also produces bit-codes as parsing evidence.

	\item We prove that the bit-codes produced by our VM are valid parsing evidence.
	\item We describe an implementation of our semantics in Haskell and formalized it using Coq proof assistant. %and used QuickCheck~\cite{Claessen2000} to test our semantics 	against a simple implementation of RE parsing, presented in~\cite{Fischer2010}, which we prove correct in the Appendix~\ref{appendix:accept}.	Our test cases cover both accepted and rejected strings for randomly generated REs.
	%\item We showed how our proposed semantics can produce bit codes that denote parse trees~\cite{Lasse2011}. %and test that such generated codes correspond to valid parsing evidence using QuickCheck.
	\item We extract from our formalization a certified algorithm in Haskell and used it	to build a RE parsing tool. We compare the its performance against a well known Haskell libraries for RE parsing. 
	\begin{comment}
  \item The definition of the VM, its operational semantics and its correctness properties.
  \item Implement the VM-based algorithm and certify that it follows the greedy disambiguation strategy. The algorithm should be included in a tool for RE-based text search in the style of the well-known GNU-grep~\cite{Grep}.
  \item Compare the efficiency of th
  e developed tool with other approaches for verified parsing present in
        the literature.
        \end{comment}
\end{itemize}

\section{Proposal structure}\label{section:proposal}

[To be edited...] \begin{comment} This work is organized as follows. Chapter~\ref{chapter:background} reviews some important concepts from
formal language theory (including REs) and presents overviews about the Coq proof assistant, the Haskell programming language and the property-based testing tool QuickCheck. Some preliminary results are
presented in Chapter~\ref{chapter:results}. In Chapter~\ref{chapter:schedule},
we present an estimation of the scheduling for the remaining activities for concluding this work, including
the possible conferences and journals we intend to submit the results of this research. Finally, in Chapter \ref{chapter:conclusion}, we present the conclusions for this work.

All implementation codes of this work can be found at \begin{center}\url{https://github.com/thalesad/Implementations}\end{center} and the \LaTeX\ code of this proposal can be found at \begin{center} \url{https://github.com/thalesad/Dissertation} \end{center}.
\end{comment}



%-----------------------------------------------------------------------------------------------------------

\chapter{Background}\label{chapter:background}

This chapter is concerned with concepts that are fundamental to this work. We start by given a succinct
review of formal language theory, as found in classic textbooks~\cite{Hopcroft2000}. Section \ref{section:coq} presents a sucint introduction to Coq proof assistant, while Section \ref{section:res} approaches REs. Section~\ref{section:haskell} gives some basic notions of Haskell programming language, followed by an overview about QuickCheck in Section \ref{section:quickcheck}. Next, we present an introduction to
formal semantics, specially the operational approach~\cite{Pierce2000}, and give some examples of an operational
semantics for a simple expression language. Then, we define the RE parsing problem, show how it can be
formulated using operational semantics~\cite{Rathnayake2011} and discuss the ambiguity problem and the greedy disambiguation strategy.

A reader familiar with these topics can safely skip this chapter.

\section{Formal Language Theory}\label{section:formallanguage}

The whole formal language theory is centered in the notion of an alphabet, which consists of a
non-empty finite set of symbols. Following common practice, we use the meta-variable $\Sigma$ to
denote an arbitrary alphabet. A string over $\Sigma$ is a finite sequence of symbols from $\Sigma$.
We let $\lambda$ denote the empty string and if $x$ is a string over some alphabet, notation $\size{x}$
denote the length of $x$. We let $x^n$ denote the string formed by $n$ repetitions of $x$.
When $n = 0$, $x^0 = \lambda$. A language over an alphabet $\Sigma$ is a set of strings over $\Sigma$.

Below we present examples of such concepts.

\begin{Example}
  Consider the alphabet $\Sigma = \{0,1\}$. The following are examples of strings over $\Sigma$:
  $\lambda, 0, 1, 00, 111, 0101$. Note that $\lambda$ is a valid string for any
  alphabet and,
  $\size{\lambda} = 0$, $\size{0} = 1$, $\size{0101} = 4$ and $0^3 = 000$.

  Example of languages over $\Sigma = \{0,1\}$ are $\{0,11,\lambda\}$ and $\{0^n1^n\,\mid\,n \geq 0\}$.
\end{Example}

Since languages are sets of strings, we can generate new languages by applying standard set operations,
like intersection, union, complement, and so on~\cite{Hopcroft2000}. In addition to standard set operations,
we can build new languages using some operations over strings. Given two languages $L_1$ and $L_2$, we define the
concatenation, $L_1L_2$, as:

\begin{equation*}
L_1L_2 =\{xy\,\mid\,x \in L_1 \land y \in L_2\}
\end{equation*}

Using concatenation, we can define the iterated concatenation as:

\begin{equation*}
\begin{array}{lcl}
  L^0       & = & \{\lambda\}\\
  L^{n + 1} & = & L^n L
\end{array}
\end{equation*}

Finally, the Kleene closure operator of a language $L$, $L^\star$, can be defined as:

\begin{equation*}
L^{\star} = \bigcup_{n\in \mathbb{N}}L^{n}
\end{equation*}

Given an alphabet $\Sigma$, $\Sigma^\star$ denote the set of all possible strings
formed using symbols from $\Sigma$.

Another pervasive notion in formal language theory is the so-called 
Deterministic finite sate automata (DFAs).
\begin{Definition}
    A deterministic finite automata (DFA) $M$ is a 5-tuple $M=(S,\Sigma,\delta,i,F)$, where:
    \begin{itemize}
        \item $S$: non empty set of states.
        \item $\Sigma$: input alphabet.
        \item $\delta : S \times \Sigma \to S$: transition function.
        \item $i \in S$: initial state.
        \item $F \subseteq S$: set of final states.
    \end{itemize}
\end{Definition}

In order to define the set of strings accepted by a DFA, we need to extend its transition function to
operate on strings and not only on symbols of its input alphabet as follows:
\begin{equation*}
  \begin{array}{lcl}
    \widehat{\delta}(s,\lambda) & = & s \\
    \widehat{\delta}(s,ay)      & = & \widehat{\delta}(\delta(s,a),y)\\
  \end{array}
\end{equation*}
with $s \in S$, $a \in \Sigma$ and $y \in \Sigma^\star$. Using this extended transition function we can
define the language accepted by a DFA $M$ as:
\begin{equation*}
  L(M) = \{w \in \Sigma^\star\,\mid\, \widehat{\delta}(i,w) \in F\}
\end{equation*}

\begin{Example}
  Consider the following language \begin{equation*}L =\{w \in\{0,1\}^\star\,\mid\,w \text{ starts with a 0 and ends with a 1}\}\end{equation*}
  A DFA that accepts $L$ is presented below:
  \begin{figure}[h]
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.0cm,
                        semithick, initial text = $ $]

       \node[initial left, state]               (A)                    {$A$};
       \node[state]                             (B) [right of=A]       {$B$};
       \node[accepting, state]                  (C) [right of=B]       {$C$};
       \node[state]                             (D) [below=1cm of A]         {$D$};

       \path (A) edge              node {$0$}       (B)
                 edge              node {$1$}       (D)
             (B) edge [loop above] node {$0$}       (B)
                 edge [bend left]  node {$1$}       (C)
             (C) edge [loop above] node {$1$}       (C)
                 edge [bend left]  node {$0$}       (B)
             (D) edge [loop left]  node {$0,\,1$}   (D) ;
    \end{tikzpicture}
    \centering
  \end{figure}

  From the previous state diagram, the state set $S$ and the final states $F$ are obvious. The following table
  shows the transition function for this DFA.

  \begin{table}[h]
    \begin{tabular}{ccc}
      $\delta$ & $0$ & $1$ \\ \hline
      $A$      & $B$ & $D$ \\
      $B$      & $B$ & $C$ \\
      $C$      & $B$ & $C$ \\
      $D$      & $D$ & $D$ \\
    \end{tabular}
    \centering
  \end{table}
\end{Example}

\section{A tour of Coq proof assistant}\label{section:coq}

Coq is a proof assistant based on the calculus of inductive
constructions (CIC)~\cite{Bertot2010}, a higher-order typed
$\lambda$-calculus extended with inductive definitions. Theorem
proving in Coq follows the ideas of the so-called
``BHK-correspondence''\footnote{Abbreviation of Brower, Heyting,
	Kolmogorov, de Bruijn and Martin-L\"of Correspondence. This is also
	known as the Curry-Howard ``isomorphism''.}, where types represent
logical formulas, $\lambda$-terms represent proofs, and the task of
checking if a piece of text is a proof of a given formula corresponds
to type-checking (i.e. checking if the term that represents the proof
has the type corresponding to the given formula)~\cite{Sorensen2006}.

Writing a proof term whose type is that of a logical formula can be
however a hard task, even for simple propositions.  In order to make
this task easier, Coq provides \emph{tactics}, which are commands that
can be used to help the user in the construction of proof terms.

In this section we provide a brief overview of Coq. We start with the
small example, that uses basic
features of Coq --- types, functions and proof definitions.  In this
example, we use an inductive type that represents natural numbers in
Peano notation. The \coq{nat} type definition includes an
annotation, that indicates that it belongs to the \coq{Set}
sort\footnote{Coq's type language classifies new inductive (and
	co-inductive) definitions by using sorts. \coq{Set} is the sort
	of computational values (programs) and \coq{Prop} is the sort of
	logical formulas and proofs.}. Type \coq{nat} is formed by two
data constructors: \coq{O}, that represents the number $0$, and \coq{S},
the successor function.
\begin{minted}{coq}
Inductive nat : Set :=
| O : nat
| S : nat -> nat.

Fixpoint plus (n m : nat) : nat :=
match n with
| O => m
| S n' => S (plus n' m)
end.

Theorem plus0r : forall n, plus n 0 = n.
Proof.
intros n. induction n.
reflexivity.
simpl. rewrite -> IHn. reflexivity.
Qed.
\end{minted}   

Command \coq{Fixpoint} allows the definition of functions by
structural recursion. The definition of \coq{plus}, for summing two
values of type \coq{nat}, is straightforward. It should be noted
that all functions defined in Coq must be total.

Besides declaring inductive types and functions, Coq allows us to
define and prove theorems. In our example, we show a simple theorem
about \coq{plus}, that states that
\coq{plus n 0 = n}, for an arbitrary value \coq{n} of type
\coq{nat}. Command \coq{Theorem} allows the statement of a
formula that we want to prove and starts the \emph{interactive proof mode}, 
in which tactics can be used to produce the proof term that
is the proof of such formula. In the example, various tactics are
used to prove the desired result.  The first tactic, \coq{intros}, is
used to move premises and universally quantified variables from the
goal to the hypothesis. Tactic \coq{induction} is used to start an inductive
proof over an inductively defined object (in our example,
the natural number \coq{n}), generating a case for each constructor and
an induction hypothesis for each recursive branch in constructors.
Tactic \coq{reflexivity} proves trivial equalities up to conversion and
\coq{rewrite} is used to replace terms using some equality. 

For each inductively defined data type,
Coq generates automatically an induction principle~\cite[Chapter
14]{Bertot2010}. For natural numbers, the following Coq term, called
\coq{nat_ind}, is created: 
\begin{minted}{coq}
nat_ind
: forall P : nat -> Prop,
P O -> (forall n : nat, P n -> P (S n)) ->
forall n : nat, P n
\end{minted}
It expects a property (\coq{P}) over natural numbers (a value of type
\coq{nat -> Prop}), a proof that \coq{P} holds for zero (a value of
type \coq{P 0}) and a proof that if \coq{P} holds for an arbitrary
natural \coq{n}, then it holds for \coq{S n} (i.e. a value of type
\coq{forall n:nat, P n -> P (S n)}). Besides \coq{nat_ind}, generated by
the use of tactic \coq{induction}, the term below 
uses the constructor of the equality
type \coq{eq_refl}, created by tactic \coq{reflexivity}, and term
\coq{eq_ind_r}, inserted by the use of tactic \coq{rewrite}. Term
\coq{eq_ind_r} allows concluding \coq{P y} based on the assumptions
that \coq{P x} and \coq{x = y} are provable.

\begin{minted}{coq}
Definition plus_0_r_term :=
fun n : nat =>
nat_ind
(fun n0 : nat => plus n0 O = n0) (eq_refl O)
(fun (n' : nat) (IHn' : plus n' O = n') =>
eq_ind_r (fun n0 : nat => S n0 = S n')
(eq_refl (S n')) IHn') n
: forall n : nat, plus n O = n
\end{minted}  

Instead of using tactics, one could instead write CIC terms directly
to prove theorems.  This can be however a complex task, even for
simple theorems like \coq{plus_0_r}, because it generally requires
detailed knowledge of the CIC type system.

An interesting feature of Coq is the possibility of defining inductive
types that mix computational and logical parts. Such types are usually
called \emph{strong specifications}, since they allow the definition
of functions that compute values together with a proof that this value
has some desired property. As an example, consider type \coq{sig}
below, also called ``subset type'', that is defined in Coq's standard
library as:
\begin{minted}{coq}
Inductive sig (A : Set)(P : A -> Prop) : Set :=
| exist : forall x : A, P x -> sig A P.
\end{minted}

Type \coq{sig} is usually expressed in Coq by using the following
syntax: $\{x : A \,\vert\,P\:x\}$.
Constructor \coq{exist} has two
parameters. Parameter \coq{x : A} represents the
computational part. The other parameter, of type \coq{P x}, denotes
the ``certificate'' that \coq{x} has the property specified by
predicate \coq{P}. As an example, consider:
\begin{minted}{coq}
forall n : nat, n <> 0 -> {m | n = S m}
\end{minted}
This type can be used to specify a function that returns the
predecessor of a natural number \coq{n}, together with a proof that
the returned value really is the predecessor of \coq{n}. The
definition of a function of type \coq{sig} requires the specification
of a logical certificate. As occurs in the case of theorems, tactics
can be used in the definition of such functions. For example, a
definition of a function that returns the predecessor of a given
natural number, if it is different from zero, can be given as follows:

\begin{minted}{coq}
Definition predcert : forall n : nat, n <> 0 -> {m | n = S m}.
intros n H.
destruct n.
destruct H. reflexivity.
exists n. reflexivity.
Defined.
\end{minted}

Tactic \coq{destruct} is used to start a proof by case analysis on
structure of a value.

Another example of a type that can be used to provide strong
specifications in Coq is \coq{sumor}, that is defined in the
standard library as follows:
\begin{minted}{coq}
Inductive sumor(A : Set) (B : Prop) : Set :=
| inleft : A -> sumor A B
| inright : B -> sumor A B.
\end{minted}

Coq standard library also provides syntactic sugar (or, in Coq's
terminology, notations) for using this type: ``\coq{sumor A B}'' can
be written as \coq{A + {B}}.
This type can be used as the type of a function that returns either a
value of type \coq{A} or a proof that some property specified by
\coq{B} holds.
As an example, we can specify the type of a function that returns a
predecessor of a natural number or a proof that the given number is
equal to zero as follows, using type \coq{sumor}:
\begin{minted}{coq}
{p | n = S p} + {n = 0}
\end{minted}  
A common problem when using rich specifications for functions is the need
of writing proof terms inside its definition body. A possible solution for
this is to use the \coq{refine} tactic, which allows one to specify a term with
missing parts (knowns as ``holes'') to be filled latter using tactics.

The next code piece uses the \coq{refine} tactic to build the computational part
of a certified predecessor function. We use holes to mark positions where proofs are
expected. Such proof obligations are later filled by tactic \coq{reflexivity} which finishes
\coq{predcert} definition.

\begin{minted}{coq}
Definition predcert : forall n : nat, {p | n = S p} + {n = 0}.
refine (fun n =>
match n with
| O => inright _
| S n' => inleft _ (exist _ n' _)
end) ; reflexivity.
Defined.
\end{minted}

The same function can be defined in a more suscint way using notations introduced
in~\cite{Chlipala13}.

\begin{minted}{coq}
Definition predcert : forall n : nat, {p | n = S p} + {n = 0}.
refine (fun n =>
match n with
| O => !!
| S n' => [|| n' ||]
end) ; reflexivity.
Defined.
\end{minted}
The utility of notations is to hide the writing of constructors and holes in
function definitions.

Another useful type for specifications is \coq{maybe}, which allows a proof
obligation-free failure for some predicate~\cite{Chlipala13}.
\begin{minted}{coq}
Inductive maybe (A : Set) (P : A -> Prop) : Set :=
| Unknown : maybe P
| Found : forall x : A, P x -> maybe P.
\end{minted}
Using \coq{maybe}, we can define a certified predecessor function as:
\begin{minted}{coq}
Definition predcert : forall n : nat, {{m | n = S m}}.
refine (fun n =>
match n return {{m | n = S m}} with
| O => ??
| S n' => [ n' ]
end); trivial.
Defined.
\end{minted}
The previous definition uses some notations: first, type \coq{maybe P} is
denoted by \coq{{{x | P}}}. Constructor \coq{Unknown} is represented by \coq{??}
and \coq{Found n} by \coq{[ n ]}. In our development, we use these
specification types to define several certified functions. More details about
these will be given in Section \ref{section:formalization}.

A detailed discussion on using Coq is out of the scope of this paper. Good introductions
to Coq proof assistant are avaliable elsewhere~\cite{Bertot2010,Chlipala13}.

\section{Regular expressions}\label{section:res}

REs are an algebraic and widely used formalism for specifying languages in computer science.
In this section we will look at the formal syntax and semantics for REs.

\begin{Definition}[RE syntax]
	\label{REexpressionsyntax}
  Let $\Sigma$ be an alphabet. The set of REs over $\Sigma$ is described by the following grammar:
  \begin{equation*}
    \begin{array}{lcl}
      e & \to  & \emptyset \\
        & \mid & \epsilon  \\
        & \mid & a         \\
        & \mid & e \: e    \\
        & \mid & e + e     \\
        & \mid & e^\star
    \end{array}
  \end{equation*}
  where $\epsilon$ represents an empty RE; $a\in\Sigma$; the meta-variable $e$ denotes an arbitrary RE; ``$e\:e$" means the concatenation of two REs; ``$e + e$" represents the choice operator between two REs and ``$e^\star$" is the Kleene closure of a RE $e$.
\end{Definition}

A RE describes a set of strings. This is captured by the following definition:

\begin{Definition}[RE semantics]
  Let $\Sigma$ be an alphabet. We define the semantics of a RE over $\Sigma$
  using the following function, $\sembrackets{\_} : RE \to
  \mathcal{P}(\Sigma^\star)$, in which $\mathcal{P}(x)$ denotes the powerset of a set $x$:
  \begin{equation*}
    \begin{array}{lcl}
      \sembrackets{\emptyset} & = & \emptyset \\
      \sembrackets{\epsilon}  & = & \{\lambda\} \\
      \sembrackets{a}         & = & \{a\} \\
      \sembrackets{e\:e'}     & = & \sembrackets{e}\:\sembrackets{e'} \\
      \sembrackets{e + e'}    & = & \sembrackets{e}\cup\sembrackets{e'} \\
      \sembrackets{e^\star}   & = & (\sembrackets{e})^\star \\
    \end{array}
  \end{equation*}
\end{Definition}

After a precise characterization of RE, we can now use it to define the class of RLs.

\begin{Definition}[Regular language]
  A language $L \subseteq \Sigma^\star$ is a RL if there is an RE 
  $e$ such that $L = \sembrackets{e}$.
\end{Definition}

In order to clarify the previous definitions, we present some examples of REs and describe their meaning.

\begin{Example}
  Consider $\Sigma =\{0,1\}$.
  \begin{itemize}
    \item The RE $e = 0^\star 10^\star$ denotes the following language
      \begin{equation*}L =\{w \in\{0,1\}^\star\,\mid\,w \text{ has just one occurrence of }1\}\end{equation*}
    \item The RE $e =(1 + \epsilon)0$ denotes the language $L = \{10, 0\}$.
    \item The RE $e = \emptyset^\star$ denotes the language $L = \{\epsilon\}$.
    \item The RE $e = 0(0 + 1)^\star 1$ denotes the language
      \begin{equation*}L =\{w \in\{0,1\}^\star\,\mid\,w \text{ starts with a 0 and ends with a 1}\}\end{equation*}
  \end{itemize}
\end{Example}

\begin{comment}

Another way of defining the semantics of REs is as an inductively defined relation between strings and REs.
The inductive semantics for REs is presented in the next definition.

\begin{Definition}[Inductive semantics for REs]
	\label{InductiveSemanticsForREs}
  Let $\Sigma$ be an alphabet, $s \in \Sigma^\star$ and $e$ a RE over $\Sigma$.
  We define that $s$ is in $e$'s language to mean that $s \in e$ can be proved
  using the following rules.
\begin{equation*}
  \begin{array}{ccc}
    \infer[_{Eps}]{\epsilon \in \epsilon}{}
    &
    \infer[_{Chr}]{a \in a}{a \in \Sigma}
    &
    \infer[_{Cat}]{ss' \in ee'}
                  {s \in e & s' \in e'} \\ \\
    \infer[_{Left}]{s \in e + e'}
                   {s \in e} &
    \infer[_{Right}]{s \in e + e'}
                    {s \in e'} &
    \infer[_{StarBase}]{\lambda \in e^\star}{} \\ \\
    \multicolumn{3}{c}{
      \infer[_{StarRec}]
            {ss' \in e^\star}
            {s \in e & s' \in e^\star}
    }
  \end{array}
\end{equation*}
\end{Definition}


Rule $Eps$, specifies that only the empty string is accepted by the RE $\epsilon$, while rule $Chr$ says that a single symbol string is accepted by the RE formed by it. 
The rules for concatenation and choice are straightforward and can be read as:

\begin{itemize}
	\item Rule $Cat$: if a string $s$ is accepted by a RE $e$ and a string $s'$ is accepted by a RE $e'$, then the string $ss'$ is accepted by the RE $ee'$.
	\item Rule $Left$: if a string $s$ is accepted by a RE $e$, then $s$ is also accepted by the RE $e + e'$.
	\item Rule $Right$: if a string $s$ is accepted by a RE $e'$, then $s$ is also accepted by the RE $e + e'$.
\end{itemize}

 For Kleene star,
we need two rules: the first specifies that the empty string is in the language of RE $e^\star$ and
rule $StarRec$ says that the string $ss'$ is in the language denoted by $e^\star$ if
$s \in e$ and $s' \in e^\star$.
\end{comment}

Meta-variable $e$ will denote an arbitrary RE and $a$ an
arbitrary alphabet symbol. As usual, all meta-variables can appear primed or subscripted.
In our Coq formalization, we represent alphabet symbols using type \coq{ascii}. We let concatenation
of RE, strings and lists by juxtaposition. Notation $|s|$
denotes the size of an string $s$. Given a RE, we let its \size~ be defined by the following 
function:


\[
\begin{array}{lcl}
\size(\emptyset) & = & 0 \\
\size(\epsilon)  & = & 1 \\
\size(a)         & = & 2 \\
\size(e_1 + e_2) & = & 1 + \size(e_1) + \size(e_2)\\
\size(e_1\,e_2)  & = & 1 + \size(e_1) + \size(e_2)\\
\size(e ^\star)  & = & 1 + \size(e)
\end{array}
\]

Given a pair $(e,s)$, formed by a RE expression $e$ and a string $s$, we define its complexity as
$(\size(e),|s|)$. Many proofs are made by well-formed induction over the complexity of that pair.

Following common practice \cite{Lopes2016,Ribeiro2017,Rathnayake2011}, we adopt an inductive
characterization of RE membership semantics. We let judgment $s \in \sembrackets{e}$ denote
that string $s$ is in the language denoted by RE $e$.

\begin{figure}[h]
	\[
	\begin{array}{cc}
	\infer[_{\{\Eps\}}]
	{\epsilon \in \sembrackets{\epsilon}}{} &
	\infer[_{\{\Chr\}}]
	{a \in \sembrackets{a}}{a \in \Sigma} \\ \\
	\infer[_{\{\Left\}}]
	{s \in \sembrackets{e + e'}}{s \in \sembrackets{e}} &
	\infer[_{\{\Right\}}]
	{s' \in \sembrackets{e + e'}}{s' \in \sembrackets{e'}} \\ \\
	\infer[_{\{\StarBase\}}]
	{\epsilon \in \sembrackets{e^\star}}{} &
	\infer[_{\{\StarRec\}}]
	{ss' \in \sembrackets{e^\star}}
	{s \in \sembrackets{e} & s' \in \sembrackets{e^\star}} \\ \\
	\multicolumn{2}{c}{
		\infer[_{\{\Catt\}}]
		{ss' \in \sembrackets{ee'}}
		{s \in \sembrackets{e} & s' \in \sembrackets{e'}}
	} \\
	\end{array}
	\]
	\centering
	\caption{RE inductive semantics.}
	\label{figure:resemantics}
\end{figure}

Rule $\Eps$ states that the empty string (denoted by the $\epsilon$)
is in the language of RE $\epsilon$.

For any single character $a$, the singleton string \coq{a} is in the language 
of RE $a$. Given membership proofs for REs $e$ and $e'$, $s \in \sembrackets{e}$ 
and $s' \in\sembrackets{e'}$, rule $\Catt$ can be used to build a proof
for the concatenation of these REs.  Rule \Left~(\Right) creates a membership proof
for $e + e'$ from a proof for $e$ ($e'$). Semantics for Kleene star
is built using the following well known equivalence of REs: $e^\star
= \epsilon + e\,e^\star$. 

We say that two REs are equivalent, written $e\approx e'$, if the following holds:

\[
\forall s. s\in \Sigma^\star \to s \in\sembrackets{e} \leftrightarrow s\in\sembrackets{e'} 
\]

\begin{comment}Rule $Eps$, together with other rules, specifies that only the empty string is accepted by RE $\lambda$, while
rule $Chr$ says that a single symbol string is accepted by the RE formed by it.
The rules for concatenation and choice are straightforward. For Kleene star,
we need two rules: the first specifies that the empty string is in the language of RE $e^\star$ and
rule $StarRec$ says that the string $ss'$ is in the language denoted by $e^\star$ if
$s \in e$ and $s' \in e^\star$.
\end{comment}

Next, we present a simple example of the inductive RE semantics.

\begin{Example}
    The string $aab$ is in the language of RE $(aa + b)^\star$, as the following derivation shows:
    \begin{equation*}
        \infer[_{StarRec}]{aab \in (aa + b)^\star}
              {
                  \infer[_{Left}]{aa \in aa + b}
                        {
                            \infer[_{Cat}]{aa \in aa}
                                  {
                                      \infer[_{Chr}]{a \in a}{a \in \Sigma} &
                                      \infer[_{Chr}]{a \in a}{a \in \Sigma}
                                  }
                        }
                  &
                  \infer[_{StarRec}]{b \in (aa + b)^\star}
                        {
                            \infer[_{Right}]{b \in aa + b}
                                  {
                                      \infer[_{Chr}]{b \in b}{b \in \Sigma}
                                  }
                            &
                            \infer[_{StarBase}]{\lambda \in (aa + b)^\star}{}
                        }
              }
    \end{equation*}
\end{Example}

As one would expect, the inductive and functional semantics of REs are equivalent, as shown in the next theorem.

\begin{Theorem}
  For all RE $e$ and strings $s \in \Sigma^\star$, $s \in \sembrackets{e}$ if, and only if, $s \in e$.
\end{Theorem}
\begin{proof}
  Let $e$ and $s$ be an arbitrary RE and string, respectively.
  \begin{itemize}
    \item[$(\to)$]: Suppose that $s \in\sembrackets{e}$. We proceed by induction on the structure of $e$.
    \begin{itemize}
      \item Case $e = \emptyset$. We have:
      \begin{align*}
          s \in \sembrackets{\emptyset}  & \leftrightarrow \\
          s \in \emptyset                & \leftrightarrow \\
          \bot \\
      \end{align*}
      which makes the conclusion hold by contradiction.
      \item Case $e = \lambda$. We have
      \begin{align*}
        s \in\sembrackets{\lambda} & \leftrightarrow \\
        s \in \lambda \\
      \end{align*}
      Since $e = \lambda$ and $s \in \sembrackets{\lambda}$, we have that $s = \lambda$ and
      the conclusion holds by rule $Eps$.
      \item Case $e = a$, $a \in \Sigma$. We have:
      \begin{align*}
        s \in\sembrackets{a} & \leftrightarrow \\
        s \in a
      \end{align*}
      Since $e = a$ and $s \in a$, we have that $s = a$ and the conclusion follows by rule $Chr$.
      \item Case $e = e_1\:e_2$. By the definition of the functional semantics, if $s \in \sembrackets{e_1\:e_2}$,
            then exists $s_1,s_2 \in \Sigma^\star$, such that $s_1 \in\sembrackets{e_1}$, $s_2 \in\sembrackets{e_2}$ and
            $s = s_1\:s_2$. By the induction hypothesis, we have that $s_1 \in e_1$ and $s_2 \in e_2$ and the conclusion
            follows by using rule $Cat$.
      \item Case $e = e_1 + e_2$. By the definition of the functional semantics, if $s \in \sembrackets{e_1 + e_2}$, then
            $s \in\sembrackets{e_1}$ or $s\in\sembrackets{e_2}$. Consider the cases:
            \begin{itemize}
              \item Case $s \in\sembrackets{e_1}$: The conclusion follows by the induction hypothesis and rule $Left$.
              \item Case $s \in\sembrackets{e_2}$: The conclusion follows by the induction hypothesis and rule $Right$.
            \end{itemize}
      \item Case $e = (e_1)^\star$. Here we proceed by strong induction on the structure of $s$. Consider the following cases:
            \begin{itemize}
              \item $s = \lambda$: In this case the conclusion follows by rule $StarBase$.
              \item $s \neq \lambda$: Since $s \in(\sembrackets{(e_1)})^\star$, by the definition of
                    the Kleene closure, we have that there exists $s_1, s_2 \in \Sigma^\star$ such that $s_1 \in\sembrackets{e_1}$,
                    $s_2 \in(\sembrackets{e_1})^\star$ and $s = s_1\:s_2$. The conclusion follows by the induction hypothesis and the rule $StarRec$.
            \end{itemize}
    \end{itemize}
    \item[$(\leftarrow)$]: Suppose that $s \in e$. We proceed by induction on the derivation of $s \in e$ by doing case analysis on the last
      rule employed to deduce $s \in e$.
    \begin{itemize}
      \item Case $Eps$: We have that $s = \lambda$ and $e = \lambda$. The conclusion follows by the definition of the functional semantics.
      \item Case $Chr$: We have that $s = a = e$. The conclusion follows by the definition of the functional semantics.
      \item Case $Cat$: Since the last rule used to deduce $s \in e$ was $Cat$, we have that must exists $s_1, s_2\in\Sigma^\star$, $e_1, e_2$ such that
            $e = e_1\:e_2$, $s = s_1\:s_2$, $s_1 \in e_1$ and $s_2 \in e_2$. By the induction hypothesis, we have that $s_1 \in\sembrackets{e_1}$ and
            $s_2 \in \sembrackets{e_2}$. The conclusion follows by the definition of the functional semantics.
      \item Case $Left$: Since the last rule used to deduce $s \in e$ was $Left$, we have that must exists $e_1, e_2$ such that $e = e_1 + e_2$ and
            $s \in e_1$. The conclusion follows by the definition of functional semantics and the induction hypothesis.
      \item Case $Right$: Since the last rule used to deduce $s \in e$ was $Right$, we have that must exists $e_1, e_2$ such that $e = e_1 + e_2$ and
            $s \in e_2$. The conclusion follows by the definition of functional semantics and the induction hypothesis.
      \item Case $StarBase$: Since the last rule used to deduce $s \in e$ was $StarBase$, we have that $s = \lambda$ and that exists $e_1$ such that
            $e = e_1^\star$. The conclusion follows by the definition of functional semantics and the Kleene closure operator.
      \item Case $StarRec$: Since the last rule used to deduce $s \in e$ was $StarRec$, we have that must exists $s_1,s_2 \in \Sigma^*$, $e_1$ such
            that $e = e_1^\star$, $s = s_1\:s_2$, $s_1 \in e_1$ and $s_2 \in (e_1)^\star$. By the induction hypothesis, we have that
            $s_1 \in\sembrackets{e_1}$ and $s_2\in\sembrackets{(e_1)^\star}$ and the conclusion follows from the definition of the functional semantics.
    \end{itemize}
  \end{itemize}
\end{proof}


Using the semantics for RE, we can define formally when two REs are equivalent as follows.

\begin{Definition}
   Let $e$ and $e'$ be two RE over $\Sigma$. We say $e$ is equivalent to $e'$, written $e \approx e'$, if the following holds:
   \[
       \forall w. w \in \Sigma^\star \to w \in \sembrackets{e} \leftrightarrow w \in \sembrackets{e'} 
   \]
\end{Definition}

\section{An Overview about Haskell}\label{section:haskell}

Here we present some basics concepts about Haskell. According to \cite{Haskell98}, Haskell is a general purpose, purely functional programming language incorporating. It provides high-order functions, non-strict semantics, static polymorphic typing, user-defined algebraic datatypes, pattern-matching, list comprehensions, a module system, a monadic I/O system and a rich set of primitive datatypes, including lists, arrays, arbitrary and fixed precision integers and floating-point numbers.

Math operations in Haskell are very similar to other programming languages, either they are functional or not. For instance, a very simple function in Haskell that doubles a value of a number can be implemented as:

\begin{minted}{haskell}
doubleMe x = x + x 
\end{minted}

If one wants to make a a function that multiplies a number by 2 but only if that number is smaller than or equal to 100, an alternative of implementation is:

\begin{minted}{haskell}
doubleSmallNumber x = if x > 100  
	then x  
	else x*2
\end{minted}


One of the main features of Haskell is its ease to create and work with structured data, specially lists. The language has some native functions that allows one to work with that lind of structure. Some of the basics of them are show in next example.
\begin{Example}
	Basic functions over lists in Haskell.
	\begin{minted}{Haskell}
length L --takes a list L and returns its length.
null L -- checks if a list L is empty. If it is, 
	--it returns True, otherwise it returns False.
reverse L -- reverses a list
take N L -- It extracts N elements from the beginning of the list L.
drop N -- drops N elements from the beginning of a list.
minimum L -- takes a list L of elemnents that can be 
	--ordered and returns the smallest element.
maximum L -- takes a list L of elemnents that can be 
    --ordered and returns the biggest element.
sum L -- takes a list L of numbers and returns their sum.
product L -- takes a list L of numbers and returns their product.
N `elem` L --  takes an element N and a list L and tells us
    -- if N is an element of L. It's usually called as an infix function
     --because it's easier to read that way.
repeat N -- takes an element N and produces an infinite list of 
   -- just that element.
	\end{minted}
	
\end{Example}

Haskell allows working with lists using the concept of list comprehension, something similar to formal math set notations. The next function, which uses the list comprehension resource, allows one to extract all uppercase letters from a sentence:

\begin{minted}{Haskell}
	removeNonUppercase st = [ c | c <- st, c `elem` ['A'..'Z']]}
\end{minted}

It is possible to create tuples with two or more elements, no matter if those elements come from a list or not. Function \haskell{zip}\footnote{\haskell{zip} is a default function in Haskell standard library that works combining two lists}, which receives two lists as parameters - in this case, one of numbers and other of strings:

\begin{minted}{haskell}
zip [1 .. 5] ["one", "two", "three", "four", "five"]
\end{minted}

 produces as result:
 
 \begin{minted}{haskell}
 [(1,"one"),(2,"two"),(3,"three"),(4,"four"),(5,"five")].
 \end{minted}

A math trivial function to generate factorial numbers could be write as:

\begin{minted}{haskell}
factorial :: Integer -> Integer  
factorial n = product [1..n]
\end{minted}

In the previous function, the word \haskell{factorial} is the name of the function. The sequence \haskell{Integer -> Integer} means that the first argument (from the left to the right) when calling \haskell{factorial} must be an element of type \haskell{Integer} and the second specifies the type of the element that the function \haskell{factorial} returns - another Integer, in this case.

A little more advanced function in Haskell is presented below. It calculates the body mass index (BMI):

\begin{minted}{haskell}
	bmiTell :: (RealFloat a) => a -> a -> String  
	bmiTell weight height  
	| bmi <= skinny = "You're underweight, you emo, you!"  
	| bmi <= normal = "You're supposedly normal. I bet you're ugly!"  
	| bmi <= fat    = "You're fat! Lose some weight, fatty!"  
	| otherwise     = "You're a whale, congratulations!"  
	where bmi = weight / height ^ 2  
	skinny = 18.5  
	normal = 25.0  
	fat = 30.0 
	
\end{minted}

The first element in the function above is its name (\haskell{bmiTell}). The command \linebreak ``\haskell{(RealFloat a)}'' says that the generic type of the following parameters \haskell{a} must be both of the type \haskell{RealFloat} (\haskell{a -> a}) . The last element (\haskell{String}) is the type of return of the function \haskell{bmiTell}. In the second line, \haskell{weight} and \haskell{height} are the local variables of type \haskell{RealFloat} (\haskell{weight} is the left \haskell{a} and \haskell{height} is the right \haskell{a} in \haskell{a -> a})
 and are used inside the clause \haskell{where}, which contains other variables: \haskell{bmi}, \haskell{skinny}, \haskell{normal} and \haskell{fat}. Although these four variables's types were not declared, Haskell can infer their type - in this case, all are also \haskell{RealFloat}. The "$\mid$" in the beginning of each context inside the function is known in Haskell as \textit{clause} and it works in a similar way as the structure \textit{switch/case} in many imperative programming languages.
 
 Another practical example of Haskell programming can be seen in the next function. It implements the Quick-sort sorting algorithm:
 
 \begin{minted}{haskell}
 quicksort :: (Ord a) => [a] -> [a]  
 quicksort [] = []  
 quicksort (x:xs) =   
 let smallerSorted = quicksort [a | a <- xs, a <= x]  
 biggerSorted = quicksort [a | a <- xs, a > x]  
 in  smallerSorted ++ [x] ++ biggerSorted
 \end{minted}
 
 The function's header says that it is necessary one parameter for the function to work - an array which elements are of type \haskell{Ord} (denoted by the left \haskell{[a]} in \haskell{[a] -> [a]}) and that the return type of the function is also a list of elements of type \haskell{Ord} (denoted by the right \haskell{[a]} in \haskell{[a] -> [a]}). The other lines of the function say that the correct order of an empty list is the empty list itself (base case) and the rest of the function works for non-empty lists until the base case is reached and there are no more recursive calls.
 
The reader interested in deeper features and resources of Haskell can find help in a large variety of material available worldwide, such as \cite{Haskell98} and \cite{Lipovaca2011}, in which the Haskell examples showed along this section are based.

\section{An Overview about QuickCheck}\label{section:quickcheck}

The first version of our semantics, which we will show in \ref{section:semantics}, was submitted to a property-based testing tool called QuickCheck~\cite{Claessen2000}, a library that allows the testing of properties expressed as Haskell functions. Such verification is done by generating random values of the desired type, instantiating the relevant property with them, and checking it directly by evaluating it to a boolean. This process continues until a counterexample is found or a specified number of cases are tested with success. The library provides generators for several standard library data types and combinators to build new generators for user-defined types.

As an example of a custom generator, consider the task of generating a random alpha-numeric character. To implement such generator, \ensuremath{\F{genChar}}, we use QuickCheck function \ensuremath{\F{suchThat}} which generates a random value which satisfies a predicate passed as argument (in example, we use \ensuremath{\F{isAlphaNum}}, which is true whenever we pass an alpha-numeric character to it), using an random generator taken as input.

\begin{hscode}\SaveRestoreHook
	\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
	\>[B]{}\F{genChar}\mathbin{::}\D{Gen}\;\D{Char}{}\<[E]%
	\\
	\>[B]{}\F{genChar}\mathrel{=}\F{suchThat}\;(\F{arbitrary}\mathbin{::}\D{Gen}\;\D{Char})\;\F{isAlphaNum}{}\<[E]%
	\ColumnHook
\end{hscode}\resethooks


In its simplest form, a property is a boolean function. As an example, the following function
states that reversing a list twice produces the same input list.
\begin{hscode}\SaveRestoreHook
	\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
	\>[B]{}\F{reverseInv}\mathbin{:}[\mskip1.5mu \D{Int}\mskip1.5mu]\to \D{Bool}{}\<[E]%
	\\
	\>[B]{}\F{reverseInv}\;\V{xs}\mathrel{=}\F{reverse}\;(\F{reverse}\;\V{xs})\equiv \V{xs}{}\<[E]%
	\ColumnHook
\end{hscode}\resethooks

We can understand this property as been implicitly quantified universally over the argument \ensuremath{\V{xs}}.
Using the function \ensuremath{\F{quickCheck}} we can test this property over randomly generated lists:
\begin{tabbing}\tt
	~~~quickCheck~reverseInv\\
	\tt ~~~\char43{}\char43{}\char43{}~OK\char44{}~passed~100~tests\char46{}
\end{tabbing}

Test execution is aborted whenever a counter example is found for the tested property. For example,
consider the following wrong property about the list reverse algorithm:

\begin{hscode}\SaveRestoreHook
	\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
	\>[B]{}\F{wrong}\mathbin{::}[\mskip1.5mu \D{Int}\mskip1.5mu]\to \D{Bool}{}\<[E]%
	\\
	\>[B]{}\F{wrong}\;\V{xs}\mathrel{=}\F{reverse}\;(\F{reverse}\;\V{xs})\equiv \F{reverse}\;\V{xs}{}\<[E]%
	\ColumnHook
\end{hscode}\resethooks

When we execute such property, a counter-example is found and printed as a result of the test.

\begin{tabbing}\tt
	~~~quickCheck~wrong\\
	\tt ~~~\char42{}\char42{}\char42{}~Failed\char33{}~Falsifiable~\char40{}6~tests~and~4~shrinks\char41{}\char46{}\\
	\tt ~~~\char91{}0\char44{}1\char93{}
\end{tabbing}

\section{Formal Semantics}\label{section:formalsemantics}

After defining the syntax of some formal system (e.g. a programming language), the next step in its specification is to describe its semantics~\cite{Pierce2000}. There are three basic approaches to formalize semantics:

\begin{enumerate}
	\item \textit{Operational semantics} specifies the behavior of a programming language by defining a simple \textit{abstract machine}
              for it. This machine is ``abstract" in the sense that it uses the terms of the language as its machine code, rather than some low-level microprocessor instruction set. For simple languages, a state of the machine is just a term, and the machine’s behavior is defined by a \textit{transition function} that, for each state, either gives the next state by performing a step of
              simplification on the term or declares that the machine has halted. The \textit{meaning} of a term t can be taken to be the final  state that the machine reaches when started with \texttt{t} as its initial state. Intuitively, the operational semantics for
              a formal system can be seen as the mathematical specification of its interpreter.
	\item \textit{Denotational semantics} takes a more abstract view of meaning: instead of just a sequence of machine states, the meaning
              of a term is taken to be some mathematical object, such as a number or a function. Giving denotational semantics for a language
              consists of finding a collection of \textit{semantic domains} and then defining an \textit{interpretation function} mapping terms
              into elements of these domains, i.e., the denotational semantics for a programming language is a mathematical specification of
              its compiler.
	\item \textit{Axiomatic semantics} instead of specify how the program should behave when executed, the axiomatic semantics
              tries to answer the following question: ``what can we prove about this program?''. The axiomatic approach is concerned with
              logics for proving properties about some formalism which is already specified using another approach, like operational or
              denotational semantics.
\end{enumerate}

Since our main interest is defining VM for RE parsing, we will focus on operational semantics which is
a convenient tool for specifying abstract machines of any sort. We finish this section with an example semantics for
a small language which consists solely of addition and natural numbers. While such language is certainly a toy example,
it is sufficient to illustrate the main concepts used in operational semantics specifications.

\subsection{Operational semantics for a simple language}\label{subsection:simplelanguage}

The language we will use is commonly referred in the literature as Hutton's razor~\cite{Hutton98} (HR) and serves as a minimal
example to illustrate ideas in formal semantics and compilation. The HR abstract syntax is defined as follows.

\begin{Definition}
   Let $n$ be a arbitrary numeric literal and $v$ a variable. The abstract syntax of terms of HR is defined by the following
   context-free language.
   \[
      \begin{array}{lcl}
         e & \to  & n \\
           & \mid & v \\
           & \mid & e + e \\
      \end{array}
   \]
 \end{Definition}

Following common practice, meta-variables like $n$, $e$ and $v$ can appear primed or subscripted. Next, we present some
examples of terms of the HR language. 

\begin{Example}
   The following are valid terms of the HR language. 
   \begin{itemize}
     \item 42, denotes an integer constant.
     \item $v_1$, is a variable.
     \item $(v_1 + v_2) + 42$, denotes a term that sums two
      variables and an integer constant.
   \end{itemize}
\end{Example}

Since terms of the HR language have variables, we need to define how these should be evaluated. A possible
approach is to evaluate expressions with respect to an \emph{environment}, which will be a total function
between variable names and integer values. We let the meta-variable $\sigma$ denote an arbitrary environment
and notation $\sigma(v)$ denotes the integer $n$ such that $(v,n) \in \sigma$. Sometimes we write an environment
as a finite mapping between variables and its corresponding integer values like $[v_1 \mapsto 1, v_2 \mapsto 2]$.
In such situation, variables not explicitly listed are mapped to 0.

In operational semantics, we can use two styles to present the meaning of a formal system: the small-step and
big-step style. The next sections we present semantics for HR using these styles.

\subsubsection{Small-step semantics for HR}\label{subsubsection:small-step}

Informally, a small-step operational semantics defines a method to evaluate an expression one-step at time. When considering the HR language, its small step operational semantics will be defined as
a binary relation between pairs of expressions and an environment as shown in the next definition.

\begin{Definition}
   The small-step semantics for HR is the binary relation between pairs of environments and
   expressions defined by the rules below. We let the notation
   $\langle \sigma, e \rangle \to \langle \sigma, e' \rangle$ denote the pair
   ($\langle \sigma, e \rangle , \langle \sigma, e' \rangle$) and symbol $\oplus$ denotes
   integer constant addition.
   \[
      \begin{array}{c}
        \fbox{$\langle \sigma, e \rangle \to \langle \sigma, e' \rangle$} \\  \\
         \begin{array}{cc}
           \infer[_{\{VAR\}}]
                 {\langle \sigma, v \rangle \to \langle \sigma, \sigma(v)\rangle}{} &
           \infer[_{\{ADD1\}}]
                 {\langle \sigma, e_1 + e_2\rangle \to \langle \sigma, e'_1 + e_2 \rangle}
                 {\langle \sigma, e_1\rangle \to \langle \sigma, e'_1\rangle} \\ \\
           \infer[_{\{ADD2\}}]
                 {\langle \sigma, n + e_2 \rangle \to \langle \sigma , n + e'_2\rangle}
                 {\langle \sigma, e_2 \rangle \to \langle \sigma, e'_2 \rangle} &
           \infer[_{\{ADD3\}}]
                 {\langle \sigma, n_1 + n_2 \rangle \to \langle \sigma, n_3 \rangle}
                 {n_3 = n_1 \oplus n_2}
         \end{array}
       \end{array}
   \]
\end{Definition}

The meaning of the previous rules are immediate. Rule $_{\{VAR\}}$ specifies that a variable evaluates
to its value in the environment $\sigma$. On the other hand, rule $_{\{ADD1\}}$ specifies that the sum
of two expressions $e_1$ and $e_2$ evaluates to $e'_1 + e_2$, where $e_1$ steps to $e'_1$ and rule
$_{\{ADD2\}}$ starts the evaluation of an expression $e_2$ only when the first operand of a sum is
completely evaluated. Finally, rule $_{\{ADD3\}}$ specifies that an expression formed by the addition of two
integer constants should evaluate to their sum. 

The result of evaluating a program using an operational semantics is usually called a value. In the HR language,
values are just integer constants. Since a small-step semantics produces only a single pass in the
program execution, we need to apply it repeatedly until we reach a value. Following standard practice,
we denote the repeated application of the small-step semantics by its reflexive transitive closure, often
named multi-step semantics, which is formally defined next.

\begin{Definition}
   The multi-step semantics for HR is the binary relation between pairs of environments and
   expressions defined as the reflexive-transitive closure of HR small-step semantics, as follows:
   \[
       \begin{array}{c}
          \fbox{$\langle \sigma,e\rangle \to^\star \langle \sigma, e'\rangle$} \\ \\
          \begin{array}{cc}
             \infer[_{\{Refl\}}]
                   {\langle \sigma, e\rangle \to^\star \langle \sigma, e\rangle}{} &
             \infer[_{\{Step\}}]
                   {\langle \sigma, e \rangle \to^\star \langle \sigma, e' \rangle}
                   {\langle \sigma, e \rangle \to \langle \sigma, e_1 \rangle &
                    \langle \sigma, e_1 \rangle \to^\star \langle \sigma, e' \rangle}
          \end{array}
       \end{array}
   \]
\end{Definition}

Again, the meaning of the multi-step semantics is immediate. Rule $Refl$ states that the relation is reflexive and rule $Step$ ensures its transitivity. Next, we present an example of these semantics.

\begin{Example}
   Let $\sigma = [v_1\mapsto 3, v_2\mapsto 5]$ and $e = (v_1 + v_2) + 42$. Below we present part of the evaluation of
   $e$ using $\sigma$.
   \[
      \infer[_{\{Step\}}]
            {\langle \sigma, (v_1 + v_2) + 42 \rangle \to^\star \langle \sigma, 50 \rangle}
            {
               \infer[_{\{ADD1\}}]
                     {\langle \sigma, (v_1 + v_2) + 42 \rangle \to \langle \sigma, (3 + v_2) + 42 \rangle}
                     {
                        \infer[_{\{ADD1\}}]
                              {\langle \sigma, v_1 + v_2 \rangle \to \langle \sigma, 3 + v_2 \rangle}
                              {
                                 \infer[_{\{ADD1\}}]
                                       {\langle \sigma, v_1 \rangle \to \langle \sigma, 3 \rangle}
                                       {
                                         \infer[_{\{VAR\}}]
                                               {\langle \sigma, v_1 \rangle \to \langle \sigma, 3 \rangle}{}
                                       }
                              } 
                     }
               &
               \infer[_{\{Step\}}]
                     {\langle \sigma, (3 + v_2) + 42 \rangle \to^\star \langle \sigma, 50 \rangle}{\vdots}
            }
   \]
\end{Example}

The semantics of HR has some important properties: convergence, i.e. every expression can be evaluated
until it reaches a value and determinism, i.e. the small-step semantics for HR is a function. Below we
state theorems about the semantics and provide its proof sketches.

\begin{Theorem}[Determinism of HR small-step semantics]
   For every $\sigma$ and expressions $e,\,e'$ and $e''$; if
   $\langle \sigma, e\rangle \to \langle \sigma, e'\rangle$ and
   $\langle \sigma, e\rangle \to \langle \sigma, e''\rangle$ then
   e' = e''.
\end{Theorem}
\begin{proof}
   By induction on the structure of the derivation of $\langle \sigma, e\rangle \to \langle \sigma, e'\rangle$ and
   case analysis on the last rule used to conclude that $\langle \sigma, e\rangle \to \langle \sigma, e''\rangle$.
\end{proof}


\subsubsection{Big-step semantics for HR}\label{subsubsection:big-step}

Intuitively, a big-step semantics defines a method to evaluate an expression until it reaches its final value.
The big-step semantics for HR consists of a binary relation between triples formed by an environment, an
expression and an integer constant. We let the notation $\langle \sigma, e \rangle\:\Downarrow n$ denotes the
triple $(\sigma, e, n)$. The next definition specifies the rules for HR big-step semantics.
\[
   \begin{array}{c}
      \fbox{$\langle \sigma, e \rangle\:\Downarrow n$} \\ \\ \\
      \begin{array}{cc}
         \infer[_{\{NUM\}}]
               {\langle \sigma, n \rangle\:\Downarrow n}{} &
         \infer[_{\{VAR\}}]
               {\langle \sigma, v \rangle\:\Downarrow \sigma(v)}{} \\ \\ \\
         \multicolumn{2}{c}{
         \infer[_{ADD}]
               {\langle \sigma, e + e' \rangle\:\Downarrow n \oplus n'}
               {\langle \sigma, e \rangle\:\Downarrow n &
                \langle \sigma, e' \rangle\:\Downarrow n'}
         }
      \end{array}
   \end{array}
\]

Rules $NUM$ and $VAR$ specifies how to evaluate numbers and variables, respectively and rule
$ADD$ say that the result of a sum expression is the addition of its corresponding numeric values.
We illustrate the semantics using the following example.

\begin{Example}
   Let $\sigma = [v_1\mapsto 3, v_2\mapsto 5]$ and $e = (v_1 + v_2) + 42$. The evaluation of $e$ using $\sigma$
   by the big-step semantics is as follows:
   \[
      \infer[_{\{ADD\}}]
            {\langle \sigma, (v_1 + v_2) + 42 \rangle \Downarrow 50}
            {
              \infer[_{\{ADD\}}]
                    {\langle \sigma, v_1 + v_2 \rangle \Downarrow 8}
                    {
                       \infer[_{\{VAR\}}]
                             {\langle \sigma, v_1 \rangle \Downarrow 3}{} &
                       \infer[_{\{VAR\}}]
                             {\langle \sigma, v_2 \rangle \Downarrow 5}{}
                    }
              &
              \infer[_{\{NUM\}}]
                    {\langle \sigma, 42 \rangle \Downarrow 42}{}
            }
   \]
\end{Example}

But a final question needs to be answered: How the big-step semantics relates with the small-step?
The answer is given by the following theorem.

\begin{Theorem}
   For every $\sigma$, $e$ and $n$, we have that $\langle \sigma, e \rangle \Downarrow n$ if, and only if,
   $\langle \sigma, e \rangle \to^\star \langle \sigma, n \rangle$.
\end{Theorem}
\begin{proof} $\:$\\
   $(\to)$ : By induction on the derivation of $\langle \sigma, e \rangle \Downarrow n$. \\
   $(\leftarrow)$ : By induction on the derivation of $\langle \sigma, e \rangle \to^\star \langle \sigma, n \rangle$.
\end{proof}

\subsection{Operational Semantics for Regular Expressions}\label{subsection:reoperationalsemantics}

Rathnayake and Thielecke \cite{Rathnayake2011} use operational semantics to formalize a VM-based interpreter for REs. The big-step semantics for their machine is the same as shown in Figure \ref{figure:resemantics} (Inductive semantics for REs), differing only in notation details: instead of the symbols $\in$ and $s$ (for instance, $s \in e$), their big-step semantics uses $\downarrow$ and $w$ (e.g. $e \downarrow w$).

The matching of a string $w$ to a RE $e$ is represented by $e \downarrow w$, regarding it as a big-step operation semantics for a language with non-deterministic branching $e_1 \mid e_2$ and a non-deterministic loop $e^*$.

The big-step operational semantics for RE matching in the previous definition has no details about how one should attempt to match a given input string $w$. So, the authors defined a small-step semantics, called the \textit{EKW machine}, that makes the matching process more explicit. The machine is named after its components: $E$ for expression, $K$ for continuation and $W$ for word to be matched.

\begin{Definition}
	A configuration of the $EKW machine$ is of the form $\langle e \, ; \, k \, ;\, w \rangle$ where $e$ is a RE, $k$ is a list of REs, and $w$ is a string. The transitions of the EKW machine are given in the next example. The accepting configuration is $\langle \epsilon \,; [ ] \,;\, \varepsilon \rangle$.
\end{Definition}

Here, $e$ is the RE the machine is currently focusing on. What remains to the right of the current expression is represented by $k$, the current continuation. The combination of $e$ and $k$ together is attempting to match $w$, the current input string.

Note that many of the rules are fairly standard, specifically the pushing and popping of the continuation stack. The machine is non-deterministic. The paired rules with the same current expressions $e^*$ or $(e_1 \mid e_2)$ give rise to branching in order to search for matches, where it is sufficient that one of the branches succeeds.

\begin{Theorem}[Partial correctness]
	$e \downarrow w$ if and only if there is a run
	\[\langle e \,;\, [ ]\, ;\, w \rangle \rightarrow \, ... \, \rightarrow \langle \, \epsilon \,;\, [ ]\, ;\, \varepsilon\rangle\]
\end{Theorem}

\begin{Definition}
	The EKW machine transition steps are
	\[
	\begin{array}{rcl}
	\langle e_1 \mid e_2 \,;\, k \,;\, w \rangle & \rightarrow & \langle e_1\, ;\, k\, ;\, w \rangle
	\\
	\langle e_1 \mid e_2 \,;\, k \,;\, w \rangle & \rightarrow & \langle e_2\, ;\, k\, ;\, w \rangle
	\\
	\langle e_1 e_2 \,;\, k \,;\, w \rangle & \rightarrow & \langle e_1\, ;\, e_2\,::\,k\, ;\, w \rangle
	\\
	\langle e^* \,;\, k \,;\, w \rangle & \rightarrow & \langle e\, ;\,e^*\,::\, k\, ;\, w \rangle
	\\
	\langle e^* \,;\, k \,;\, w \rangle & \rightarrow & \langle \epsilon\, ; k\, ;\, w \rangle
	\\
	\langle a \,;\, k \,;\, aw \rangle & \rightarrow & \langle \epsilon\, ; k\, ;\, w \rangle
	\\
	\langle \epsilon \,;\, e\,::\,k \,;\, w \rangle & \rightarrow & \langle e\, ; k\, ;\, w \rangle
	\end{array}
	\]	
\end{Definition}

The authors do not mention if their proposed semantics follows any disambiguation policy.

While the previous theorem ensures that all matching strings are correctly accepted, there is no guarantee that the machine accepts all strings that it should on every run. The next example will present this situation.% : $\langle a^{**} \,;\, [ ] \,;\, a\rangle$:

\begin{Example}
	Consider the RE $a^{**}$ and the string $a$. A possible looping execution for the EKW machine is presented below.
	\[
	\begin{array}{ccc}
	\langle a^{**} \,;\,[]\,;\,a\rangle & \rightarrow & \langle a^*\,;\,[a^{**}]\,;a\rangle\\
	& \rightarrow & \langle \epsilon\,;\,[a^{**}]\,;a\rangle\\
	& \rightarrow & \langle a^{**} \,;\,[]\,;\,a\rangle\\
	& \rightarrow & ...
	\end{array}
	\]
\end{Example}


To solve this problem, the authors propose the PW$\pi$ machine, refining the EKW machine by the RE as a data structure in a heap $\pi$, which serves as the program run by the machine. That way, the machine can distinguish between different positions in the syntax tree, avoiding infinite loop.


\section{Regular Expression Parsing}\label{subsection:reparsing}

We follow the definition of RE parsing as presented by Frisch et. al~\cite{Frisch2004}, where RE are interpreted
as types which describes evidence that some string is in the underlying RE language, i.e. to represent RE membership
as a type inhabitation problem\footnote{The type inhabitation problem can be defined as follows: Given a type
$\tau$ and a typing context $\Gamma$, determine if there's an expression $e$ such that $\Gamma \vdash e : \tau$, where
$\Gamma \vdash e : \tau$ is the underlying proof that $e$ has type $\tau$ under the assumptions in $\Gamma$~\cite{Hindley2008}.}.
The next definition specifies the set of term evidence for a given RE.

\begin{Definition}
  The set of evidence values of a RE $e$, $\mathcal{T}(e)$, is defined as:
  \[
      \begin{array}{lcl}
         \mathcal{T}(\lambda)   & = & \{\bullet\} \\
         \mathcal{T}(a)         & = & \{a\} \\
         \mathcal{T}(e + e')    & = & \{\text{inl}\,v\,\mid\, v \in \mathcal{T}(e)\} \cup
                                      \{\text{inr}\,v\,\mid\, v \in \mathcal{T}(e')\} \\
         \mathcal{T}(e\:e')     & = & \{\langle v, v' \rangle \,\mid \, v \in \mathcal{T}(e),\, v' \in \mathcal{T}(e')\} \\
         \mathcal{T}(e^\star)   & = & \{[v_0,...,v_n]\,\mid\,v_i \in \mathcal{T}(e)\} \\
      \end{array}
  \]
\end{Definition}

Note that the previous definition doesn't have a equation for the empty RE ($\emptyset$), since it does not
have any evidence of string membership. Since, we use lists to denote evidence for the Kleene star operator,
we need to set up notations to work with lists. We let the symbol $\text{inl }\bullet$ denote the empty list and 
$\text{inr }\langle v, l \rangle$ denote the list formed by head $v$ and tail $l$. Following the syntax sugar used in
Haskell, we let notation $[v_0,...,v_n]$ denote $\text{inr }\langle v_0,\langle...,\langle \text{inr }\langle v_n, \text{inl } \bullet\rangle^n$,
where $\rangle^n$ is the repeated concatenation, as usual. Using this notation for lists has the benefit of matching
the meaning of the Kleene star operator, which is described by the following well known equivalence:
$e^\star \approx e e^\star + \lambda$.

Next we give examples of these definitions.

\begin{Example}
   Consider the following RE $e = 0(0 + 1)^\star$, which denotes the language of string that begin with 0.
   Under the type interpretation, this RE denotes the following set of membership evidence:
   \begin{align*}
      \mathcal{T}(0(0+1)^\star) & = \{\langle 0 , v \rangle\, \mid\, v \in \mathcal{T}((0 + 1)^\star) \}\\
                                & = \{\langle 0 , [v_0,...,v_n] \rangle\, \mid\, v_i \in\mathcal{T}(0 + 1)\} \\
                                & = \{\langle 0 , [v_0,...,v_n] \rangle\, \mid\, v_i \in \{\text{inl }v\,\mid\,v \in \mathcal{T}(0)\} \cup
                                                                                           \{\text{inr }v\,\mid\,v \in \mathcal{T}(1)\}\} \\
                                & = \{\langle 0 , [v_0,...,v_n] \rangle\, \mid\, v_i \in \{\text{inl }0, \text{inr }1\}\} \\
   \end{align*}
   Observe that this set of evidence is formed by pairs whose first component is the symbol 0 and the second are
   lists formed by arbitrary combinations of $\text{inl }0$ and $\text{inr }1$. Tags inl and inr adds information
   about the structure; it means that $0$ is the left and $1$ the right operand in choice.
\end{Example}

\begin{Example}
   Consider the RE $e = 0^\star$, which under RE typing interpretation, denotes the set of lists of evidences for RE 0.
   The element of $\mathcal{T}(e)$ with 3 repetitions can be written as:
   \[
       [0,0,0] = \text{inl }\langle 0 ,\, \text{inl } \langle 0,\, \text{inl } \langle 0,\, \text{inr }\bullet \rangle \rangle \rangle
   \]
\end{Example}

The reader must have noticed that the set of evidences for a given RE $e$ corresponds to the set of parse trees formed by the
concatenation of symbols in the evidence leafs, considering the $\bullet$ as the empty string. The operation of building the
parsed string from a given evidence for a RE e is called \emph{flattening} as defined next.

\begin{Definition}
   The flattening of a parse tree $v$, written $\Vert v \Vert$, is defined as:
   \[
       \begin{array}{lcl}
         \Vert \bullet \Vert               & = & \lambda \\
         \Vert a       \Vert               & = & a \\
         \Vert\langle v , v' \rangle \Vert & = & \Vert v \Vert\:\Vert v' \Vert \\
         \Vert\text{inl }v\Vert            & = & \Vert v \Vert \\
         \Vert\text{inr }v\Vert            & = & \Vert v \Vert \\
       \end{array}
   \]
\end{Definition}

\begin{Example}\label{example:ambiguoustrees}
   Consider the RE $e = (01 + 0)(1 + \lambda)$, which denotes the following language
   $\{011, 01, 0\}$ and the following set of evidences:
   \begin{align*}
      \mathcal{T}(e) & = \mathcal{T}((01 + 0)(1 + \lambda)) \\
                     & = \mathcal{T}(01 + 0) \times \mathcal{T}(1 + \lambda) \\
                     & = \{\text{inl }\langle 0, 1 \rangle, \text{inr }0\} \times \{\text{inl }1, \text{inr }\bullet\} \\
                     & = \{\langle \text{inl }\langle 0, 1 \rangle, \text{inl }1 \rangle,
                           \langle \text{inl }\langle 0, 1 \rangle, \text{inr }\bullet \rangle,
                           \langle \text{inr }0, \text{inl }1 \rangle,
                           \langle \text{inr }0, \text{inr }\bullet \rangle\}
   \end{align*}
   Drawn as trees, such evidences will be like:

   \begin{table}[h]
      \begin{tabular}{cccc}
         \begin{tikzpicture}[tlabel/.style={font=\footnotesize}]
             \node{$\langle,\rangle$}
                  child{
                     node{inl}
                       child{node {$\langle,\rangle$}
                          child{node{0}}
                          child{node{1}}
                       }
                  }
                  child{
                     node{inl}
                       child{node {1}}
                  } ;
         \end{tikzpicture}
         &
         \begin{tikzpicture}[tlabel/.style={font=\footnotesize}]
            \node{$\langle,\rangle$}
                  child{
                     node{inl}
                       child{node {$\langle,\rangle$}
                          child{node{0}}
                          child{node{1}}
                       }
                  }
                  child{
                     node{inr}
                       child{node {$\bullet$}}
                  } ; 
         \end{tikzpicture}
         &
         \begin{tikzpicture}[tlabel/.style={font=\footnotesize}]
            \node{$\langle,\rangle$}
               child{
                 node{inr}
                   child{node{0}}
               }
               child{
                 node{inl}
                   child{node{1}}
               }; 
         \end{tikzpicture}
         &
         \begin{tikzpicture}[tlabel/.style={font=\footnotesize}]
            \node{$\langle,\rangle$}
               child{
                 node{inr}
                   child{node{0}}
               }
               child{
                 node{inl}
                   child{node{$\bullet$}}
               }; 
         \end{tikzpicture}
      \end{tabular}
      \centering
   \end{table}
   and the result of applying the flattening function on each of these
   these trees will produce the following strings, respectively:
   $\{011,01,01,0\}$.
\end{Example}

Using these previous definitions, we can formally state the RE parsing problem:

\begin{Definition}[Regular expression parsing]
   Let $e$ be an arbitrary RE over $\Sigma$ and $s \in \Sigma^\star$. The problem of
   parse $s$ under $e$ is to construct an evidence $v \in \mathcal{T}(e)$ such that
   $\Vert v \Vert = s$.
\end{Definition} 


\subsection{Ambiguity in Regular Expressions}\label{subsection:ambiguity}

Observe that, in Example \ref{example:ambiguoustrees}, there are two different evidences
whose flattening produce the same string:
$\langle \text{inl }\langle 0, 1 \rangle, \text{inl }1 \rangle,\langle \text{inl }\langle 0, 1 \rangle, \text{inr }\bullet \rangle$.

\begin{Definition}
   A RE $e$ is ambiguous when two or more parse trees in $\mathcal{T}(e)$ flattens to the same string, i.e.:
   \[
    \exists v_1, v_2 \in \mathcal{T}(e). v_1 \neq v_2 \land \Vert v_1 \Vert = \Vert v_2 \Vert
   \]
\end{Definition}

In order to make the result of parsing problem deterministic, we need to define a
method to choose the ``best'' parse tree from the set of evidences for a RE and string.
This allows for predictability on parsing results and helps a user to understand
why some matchings are chosen instead of others, for ambiguous expressions.
In the next subsection, we present the greedy disambiguation policy.

\subsection{Greedy Disambiguation Policy}\label{subsubsection:greedypolicy}

Intuitively, the greedy policy specifies that we must choose the ``left-most'' possibility
whenever more than one is possible. For this, we define a ordering relation on evidence~\cite{Frisch2004}.

\begin{Definition}[Greedy order on evidence~\cite{Frisch2004}]
   The binary relation $\lessdot$ is inductively defined on the structure of evidence as follows:
   \[
       \begin{array}{rclll}
          \langle v_1, v_2 \rangle & \lessdot & \langle v'_1, v'_2 \rangle & \text{if} & v_1 \lessdot v'_1 \lor (v_1 = v'_1 \land v_2 \lessdot v'_2)\\
           \text{inl }v & \lessdot & \text{inl }v'              & \text{if} & v \lessdot v' \\
           \text{inr }v & \lessdot & \text{inr }v'               & \text{if} & v \lessdot v' \\
           \text{inl }v & \lessdot & \text{inr }v'               &           & \\
       \end{array}
   \]
\end{Definition}

\begin{Example}
  Let's revisit the parse trees for the ambiguous RE presented in example~\ref{example:ambiguoustrees}:

   \begin{table}[h]
      \begin{tabular}{cc}
         \begin{tikzpicture}[tlabel/.style={font=\footnotesize}]
            \node{$\langle,\rangle$}
                  child{
                     node{inl}
                       child{node {$\langle,\rangle$}
                          child{node{0}}
                          child{node{1}}
                       }
                  }
                  child{
                     node{inr}
                       child{node {$\bullet$}}
                  } ; 
         \end{tikzpicture}
         &
         \begin{tikzpicture}[tlabel/.style={font=\footnotesize}]
            \node{$\langle,\rangle$}
               child{
                 node{inr}
                   child{node{0}}
               }
               child{
                 node{inl}
                   child{node{1}}
               }; 
         \end{tikzpicture}
      \end{tabular}
      \centering
   \end{table}
   Under the greedy ordering, the first tree is considered smaller then the second.
\end{Example}

Having an order defined between evidence, we can formulate de disambiguation strategy
as picking the minimum element with respect to this ordering. The ordering is not total if one compares elements from different REs. However, if only elements of the same RE are compared, then the order is strict and total~\cite{Frisch2004}. In order to ensure that a minimum element exist, we need to restrict the set $\mathcal{T}(e)$ to the so-called \emph{non-problematic}. According to~\cite{Frisch2004}, the reason for this, is that some REs no least element exist for the greedy ordering, and therefore we cannot pick out the minimum element.

\begin{Example}\label{example:nonminimum}
   Let $e = (\lambda + 1)^\star$. The following are all evidence int $\mathcal{T}(e)$
   that flattens to $11$.
   \[
       \begin{array}{lcl}
         v_0 & = & [\text{inr }1, \text{inr 1}] \\
         v_1 & = & [\text{inl }\bullet, \text{inr }1, \text{inr 1}] \\
         v_2 & = & [\text{inl }\bullet,\text{inl }\bullet, \text{inr }1, \text{inr 1}] \\
             & \vdots & \\
       \end{array}
   \]
   so minimum element exist because of the infinite descending chain $v_0 \: \gtrdot\: v_1 \: \gtrdot \: ...$
\end{Example}

\begin{Definition}[Non-problematic evidence~\cite{Frisch2004}]
   Given a RE $e$, the set of its non-problematic evidence, $\mathcal{T}^{\textbf{np}}(e)$, is defined as:

  \[
      \begin{array}{lcl}
         \mathcal{T}^{\textbf{np}}(\lambda)   & = & \{\bullet\} \\
         \mathcal{T}^{\textbf{np}}(a)         & = & \{a\} \\
         \mathcal{T}^{\textbf{np}}(e + e')    & = & \{\text{inl}\,v\,\mid\, v \in \mathcal{T}^{\textbf{np}}(e)\} \cup
                                      \{\text{inr}\,v\,\mid\, v \in \mathcal{T}^{\textbf{np}}(e')\} \\
         \mathcal{T}^{\textbf{np}}(e\:e')     & = & \{\langle v, v' \rangle \,\mid \, v \in \mathcal{T}^{\textbf{np}}(e),\, v' \in \mathcal{T}^{\textbf{np}}(e')\} \\
         \mathcal{T}^{\textbf{np}}(e^\star)   & = & \{[v_0,...,v_n]\,\mid\,v_i \in \mathcal{T}^{\textbf{np}}(e)\} -
                 \{v \,\mid\,v \in \mathcal{T}^{\textbf{np}}(e), \Vert v \Vert = \lambda\} \\
      \end{array}
  \]
\end{Definition}

Note that the only difference between $\mathcal{T}(e)$ and $\mathcal{T}^{\textbf{np}}(e)$ is that the latter
doesn't allow ``empty'' elements in lists --- elements that flatten to the empty string. This corresponds to
what programmers expect in looping structures: they should not do unnecessary iterations between doing actual
work. Limiting ourselves to non-problematic evidence, avoids the problem presented in Example~\ref{example:nonminimum}.

Following~\cite{Frisch2004}, we refer to RE whose type interpretation do not contain any problematic values as
\emph{non-problematic REs}:

\begin{Definition}[Non-problematic RE~\cite{Frisch2004}]
   A RE $e$ is non-problematic if it does not contain any sub-term of the form $e'^\star$, where
   $\lambda \in \sembrackets{e'}$.
\end{Definition}

With the refined notion of evidence, we can now formulate the definition of parsing to
ensure that minimum elements do exist:

\begin{Definition}[Parsing]
   Given a RE $e$ over $\Sigma$ and a string $w \in \Sigma^*$, to parse $w$ under $e$ is to
   produce the evidence $v$ such that:
   \[
       v \in \mathcal{T}^{\textbf{np}}(e) \land \forall v' \in \mathcal{T}^{\textbf{np}}(e). t \lessdot t' \lor t = t'
   \]
\end{Definition}

\begin{comment}
\subsection{POSIX Disambiguation Policy}\label{subsubsection:posixpolicy}

The POSIX strategy prefers the \emph{longest, left most parse}, prioritizing the length of the string that a particular
subexpression matches. Roughly, it will try to shift as much of the parse tree to the left in the expression~\cite{Ausaf16}.
\end{comment}

Next, we review Thompson NFA construction which is similar to the proposed small-step semantics
for RE parsing developed in Section~\ref{chapter:smallstepsemantics}.

\section{Thompson NFA construction}
\label{subsection:thompsonnfraconstruction}

The Thompson NFA construction is a classical algorithm for building an equivalent
NFA with $\epsilon$-transitions by induction over the structure of an input RE.
We follow a presentation given in~\cite{Aho1986} where $N(e)$ denotes the NFA
equivalent to RE $e$. The construction proceeds as follows. If $e = \epsilon$,
we can build the following NFA equivalent to $e$.
\begin{center}
	\begin{tikzpicture}[auto, node distance=24mm, initial text=, >=latex]
	\node[state, initial, fill=white]   (q_1) [] {};
	\node[state, accepting, fill=white] (q_2) [right of=q_1] {};
	
	\path[->] (q_1) edge [] node {$\epsilon$}  (q_2);
	\end{tikzpicture}
\end{center}
If $e = a$, for $a \in \Sigma$, we can make a NFA with a single transition consuming
$a$:
\begin{center}
	\begin{tikzpicture}[auto, node distance=24mm, initial text=, >=latex]
	\node[state, initial, fill=white]   (q_1) [] {};
	\node[state, accepting, fill=white] (q_2) [right of=q_1] {};
	
	\path[->] (q_1) edge [] node {$a$}  (q_2);
	\end{tikzpicture}
\end{center}
When $e = e_1 + e_2$, we let $N(e_1)$ be the NFA for $e_1$ and $N(e_2)$ the
NFA for $e_2$. The NFA for $e_1 + e_2$ is built by adding a new initial and accepting
state which can be combined with $N(e_1)$ and $N(e_2)$ using $\epsilon$-transitions as
shown in the next picture.
\begin{center}
	\begin{tikzpicture}[auto, node distance=17mm, initial text=, >=latex]
	\node[state, initial]  (s_i)   []                   {};
	\node[state]        (a_1)   [above right of=s_i] {};
	\node[draw=none,fill=none]            (namea) [right of=a_1] {$N(e_1)$};
	\node[state]         (a_2)   [right of=namea]     {};
	
	\node[state]        (b_1)   [below right of=s_i] {};
	\node[draw=none]            (nameb) [right of=b_1]           {$N(e_2)$};
	\node[state]         (b_2)   [right of=nameb]     {};
	
	\node[state, accepting] (s_a)   [below right of=a_2] {};
	
	\path[->] (s_i) edge [below right] node {$\epsilon$} (a_1)
	edge [above right] node {$\epsilon$} (b_1)
	(a_2) edge [below left]  node {$\epsilon$} (s_a)
	(b_2) edge []            node {$\epsilon$} (s_a);
	\begin{scope}[on background layer]
	\node[ellipse, draw=black, aspect=5, minimum width=45mm, minimum height=20mm, right of=b_1] {};
	\node[ellipse, draw=black, aspect=5, minimum width=45mm, minimum height=20mm, right of=a_1] {};
	\end{scope}
	\end{tikzpicture}
\end{center}
The NFA for the concatenation $e = e_1e_2$ is built from the NFAs $N(e_1)$ and $N(e_2)$. The accepting
state of $N(e_1e_2)$ will be the accepting state from $N(e_2)$ and the starting state of $N(e_1)$ will be
the initial state of $N(e_1)$.

\begin{center}
	\begin{tikzpicture}[auto,  node distance=15mm, initial text=, >=latex]
	\node[state, initial]   (a_1)   []               {};
	\node[draw=none,fill=none]             (namea) [right of=a_1]   {$N(e_1)$};
	\node[state] (a_2)   [right of=namea] {};
	
	\node[draw=none]             (nameb) [right of=a_2]   {$N(e_2)$};
	\node[state, accepting]  (b_2)   [right of=nameb] {};
	
	\begin{scope}[on background layer]
	\node[ellipse, draw=black, aspect=5, minimum width=40mm, minimum height=20mm, right of=a_1] {};
	\node[ellipse, draw=black, aspect=5, minimum width=40mm, minimum height=20mm, right of=a_2] {};
	\end{scope}
	\end{tikzpicture}
\end{center}
Finally, for the Kleene star operator, we built a NFA for the RE $e$, add a new
starting and accepting states and the necessary $\epsilon$ transitions, as shown below.
\begin{center}
	\begin{tikzpicture}[auto, node distance=15mm, initial text=, >=latex]
	\node[state, initial]  (s_i)   []               {};
	\node[state]        (a_1)   [right of=s_i]   {};
	\node[draw=none]            (namea) [right of=a_1]   {$N(e_1)$};
	\node[state]         (a_2)   [right of=namea] {};
	
	\node[state, accepting] (s_a)   [right of=a_2]   {};
	
	\path[->] (s_i) edge []                     node {$\epsilon$} (a_1)
	edge [bend right=40, below] node {$\epsilon$} (s_a)
	(a_2) edge []                     node {$\epsilon$} (s_a)
	edge [bend right=90, above] node {$\epsilon$} (a_1);
	\begin{scope}[on background layer]
	\node[ellipse, draw=black, aspect=5, minimum width=50mm, minimum height=20mm, right of=a_1] {};
	\end{scope}
	\end{tikzpicture}
\end{center}

\begin{Example}
	In order to show a step-by-step automata construction following Thompson's algorithm, we take as
	example the RE $((ab) + c)^* $ over the alphabet $\Sigma = \{a,b,c\}$.
	
	The first step is to construct an automata ($S_1$) that accepts the symbol $a$.
	
	\begin{center}
		\begin{tikzpicture}[->,>=stealth',initial text =$S_1:$,shorten >=1pt,auto,node distance=24mm,scale = 1,transform shape]
		
		\node[state,initial] (1) {$1$};
		\node[state,accepting] (2) [right of=1] {$2$};
		
		\path (1) edge              node {$a$} (2);
		
		\end{tikzpicture}
	\end{center}
	
	Then, we construct another automata ($S_2$) that accepts the symbol $b$:
	
	\begin{center}
		\begin{tikzpicture}[->,>=stealth',initial text =$S_2:$,shorten >=1pt,auto,node distance=24mm,scale = 1,transform shape]
		
		\node[state,initial] (3) {$3$};
		\node[state,accepting] (4) [right of=3] {$4$};
		
		\path (3) edge              node {$b$} (4);
		
		\end{tikzpicture}
	\end{center}
	
	The concatenation $ab$ is accepted by automata $S_3$:
	
	\begin{center}
		\begin{tikzpicture}[->,>=stealth',initial text =$S_3:$,shorten >=1pt,auto,node distance=24mm,scale = 1,transform shape]
		
		\node[state,initial] (1) {$1$};
		\node[state] (2) [right of=1] {$2$};
		\node[state,accepting] (4) [right of=2] {$4$};
		
		\path (1) edge              node {$a$} (2)
		(2) edge              node {$b$} (4);
		
		\end{tikzpicture}
	\end{center}
	
	Now we build automata $S_4$, which recognizes the symbol $c$:
	
	\begin{center}
		\begin{tikzpicture}[->,>=stealth',initial text =$S_4:$,shorten >=1pt,auto,node distance=24mm,scale = 1,transform shape]
		
		\node[state,initial] (5) {$5$};
		\node[state,accepting] (6) [right of=5] {$6$};
		
		\path (5) edge              node {$c$} (6);
		
		\end{tikzpicture}
	\end{center}
	
	The automata $S_5$ accepts the RE $(ab) + c$:
	
	\begin{center}
		\begin{tikzpicture}[->,>=stealth',initial text =$S_5:$,shorten >=1pt,auto,node distance=15mm,scale = 1,transform shape]
		
		\node[state,initial] (7) {$7$};
		\node[state] (1) [above right of=7] {$1$};
		\node[state] (5) [below right of=7] {$5$};
		\node[state] (2) [below right of=1] {$2$};
		\node[state] (4) [above right of=2] {$4$};
		\node[state,accepting] (8) [below right of=4] {$8$};
		\node[state] (6) [right of=5] {$6$};
		
		\path (7) edge              node {$\epsilon$} (1)
		(7) edge              node {$\epsilon$} (5)
		(1) edge              node {$a$} (2)
		(2) edge              node {$b$} (4)
		(4) edge              node {$\epsilon$} (8)
		(5) edge              node {$c$} (6)
		(6) edge              node {$\epsilon$} (8);
		
		
		\end{tikzpicture}
	\end{center}
	
	Finally, we have the NFA $S_6$, that accepts $((ab) + c)^*$:
	
	\begin{center}
		\begin{tikzpicture}[->,>=stealth',initial text =$S_6:$,shorten >=1pt,auto,node distance=13mm,scale = 1,transform shape]
		
		\node[state,initial] (9) {$9$};
		\node[state] (7) [right of=9] {$7$};
		\node[state] (1) [above right of=7] {$1$};
		\node[state] (5) [below right of=7] {$5$};
		\node[state] (2) [below right of=1] {$2$};
		\node[state] (4) [above right of=2] {$4$};
		\node[state] (8) [below right of=4] {$8$};
		\node[state] (6) [right of=5] {$6$};
		
		\node[state,accepting] (10) [right of=8] {$10$};
		
		\path (7) edge              node {$\epsilon$} (1)
		(7) edge              node {$\epsilon$} (5)
		(1) edge              node {$a$} (2)
		(2) edge              node {$b$} (4)
		(4) edge              node {$\epsilon$} (8)
		(5) edge              node {$c$} (6)
		(6) edge              node {$\epsilon$} (8)
		(9) edge [bend right=60]             node [above] {$\epsilon$} (10)
		(8) edge  [bend right=100]            node {$\epsilon$} (7)
		(9) edge node {$\epsilon$} (7)
		(8) edge node {$\epsilon$} (10);
		
		\end{tikzpicture}
	\end{center}
\end{Example}

Originally, Thompson formulate its construction as a IBM 7094 program~\cite{Thompson1968}. Next we reformulate it as a small-step
operational semantics using contexts, modeled as data-type derivatives for RE, which is the subject of
the next section.

\subsection{Data-type derivatives}

The usage of evaluation contexts is standard in reduction semantics~\cite{Felleisen2009}.
Contexts for evaluating a RE during the parse of a string $s$ can be defined by the following
context-free syntax:
\[E[\,] \to E[\,]+ e\,\mid\,e + E[\,]\,\mid\,E[\,]\,e\,\mid\,e\,E[\,]\,\mid\,\star\]

The semantics of a $E[\,]$ context is a RE with a hole that needs to be ``filled'' to form a
RE. We have two cases for union and concatenation denoting that the hole could be the left
or the right component of such operators. Since the Kleene star has only a recursive occurrence,
it is denoted just as a ``mark'' in context syntax.

Having defined our semantics (Figure~\ref{figure:smallstep}), we have noticed that our RE context syntax is exactly the data type
for \emph{one-hole contexts}, known as derivative of an algebraic data type.
Derivatives where introduced by McBride and its coworkers~\cite{McBride08} as a generalization
of Huet's zippers for a large class of algebraic data types~\cite{AbbottAGM03}. RE contexts are
implemented by the following Haskell data-type:
\begin{hscode}\SaveRestoreHook
	\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
	\>[B]{}\mathkw{data}\;\D{Hole}\mathrel{=}\C{InChoiceL}\;\D{Regex}\mid \C{InChoiceR}\;\D{Regex}{}\<[E]%
	\\
	\>[B]{}\hsindent{3}{}\<[3]%
	\>[3]{}\mid \C{InCatL}\;\D{Regex}\mid \C{InCatR}\;\D{Regex}\mid \C{InStar}{}\<[E]%
	\ColumnHook
\end{hscode}\resethooks
Constructor \ensuremath{\C{InChoiceL}} store the right component of a union RE (similarly for \ensuremath{\C{InChoiceR}}). We need
to store contexts for union because such information is used to allow backtracking in case of failure.
Constructors \ensuremath{\C{InCatL}} and \ensuremath{\C{InCatR}} store the right (left) component of a concatenation and they are
used to store the next subexpresssions that need to be evaluated during input string parsing.
Finally, \ensuremath{\C{InStar}} marks that we are currently processing an expression with a Kleene star operator.

\chapter{Small-step operational semantics}\label{chapter:smallstepsemantics}


In this chapter we present the definition of a small-step operational semantics for RE parsing which is
equivalent to executing the Thompson's construction NFA over the input string. Observe that
the inductive semantics for RE (Figure~\ref{figure:resemantics}) can be understood as a big-step
operational semantics for RE, since it ignores many details on how should we proceed to match
an input~\cite{Rathnayake2011}.

The semantics is defined as a binary relation between \emph{configurations}, which are 5-uples
$\conf{d,e,c,b,s}$ where:
\begin{itemize}
	\item $d$ is a direction, which specifies if the semantics is starting (denoted by $B$) or
	finishing ($F$) the processing of the current expression $e$.
	\item $e$ is the current expression being evaluated;
	\item $c$ is a context in which $e$ occurs. Contexts are just a list of
	\ensuremath{\D{Hole}} type in our implementation.
	\item $b$ is a bit-code for the current parsing result, in reverse order.
	\item $s$ is the input string currently being processed.
\end{itemize}
Notation $\conf{d,e,c,b,s}\to\conf{d',e',c',b',s'}$ denotes that from
configuration $\conf{d,e,c,b,s}$ we can give a step leading to a new state
$\conf{d',e',c',b',s'}$ using the rules specified in Figure~\ref{figure:smallstep}.

\begin{figure*}[h]
	\[
	\begin{array}{cc}
	\infer[_{(Eps)}]{\conf{B,\epsilon,c,b,s} \to \conf{F,\epsilon,c,b,s}}{}
	&
	\infer[_{(Chr)}]{\conf{B,a,c,b,a:s} \to \conf{F,a,c,b,s}}{}
	
	\\ \\
	\infer[_{(Left_B)}]{\conf{B,e+e',c,b,s}\to\conf{B,e,c',b',s}}
	{\begin{array}{c}
		b' = \ensuremath{\C{0_b}} : b\\
		c' = E[\,]+e' : c \\
		\end{array}}
	&
	\infer[_{(Right_B)}]{\conf{B,e+e',c,b,s}\to\conf{B,e',c',b',s}}
	{\begin{array}{c}
		b' = \ensuremath{\C{1_b}} : b\\
		c' = e + E[\,] : c \\
		\end{array}}
	
	\\ \\
	\infer[_{(Cat_B)}]{\conf{B,ee',c,b,s}\to\conf{B,e,c',b,s}}
	{c' = E[\,]e' : c}
	&
	\infer[_{(Star_1)}]{\conf{B,e^\star,c,b,s}\to\conf{B,e,\star : c, \ensuremath{\C{0_b}} : b, s}}{}
	
	\\ \\
	\infer[_{(Star_2)}]{\conf{B,e^\star,c,b,s}\to\conf{F,e^\star, c, \ensuremath{\C{1_b}} : b, s}}{}
	&
	\infer[_{(Cat_{EL})}]{\conf{F,e,E[\,]e':c,b,s}\to\conf{B,e',c',b,s}}{c'=eE[\,]:c}
	
	\\ \\
	\infer[_{(Cat_{ER})}]{\conf{F,e',eE[\,]:c,b,s}\to\conf{F,ee',c,b,s}}{}
	&
	\infer[_{(Left_E)}]{\conf{F,e,c,b,s}\to\conf{F,e+e',c',\ensuremath{\C{0_b}}:b,s}}{c = E[\,]+e' : c'}
	
	\\ \\
	
	\infer[_{(Right_E)}]{\conf{F,e,c,b,s}\to\conf{F,e+e',c',\ensuremath{\C{1_b}}:b,s}}{c = e + E[\,] : c'}
	&
	\infer[_{(Star_{E1})}]{\conf{F,e,\star:c,b,s}\to\conf{B,e,\star:c,\ensuremath{\C{0_b}}:b,s}}{}
	
	\\ \\
	
	\multicolumn{2}{c}{
		
		\begin{array}{c}
			\infer[_{(Star_{E2})}]{\conf{F,e,\star:c,b,s}\to\conf{F,e^\star,c,\ensuremath{\C{1_b}}:b,s}}{}
		\end{array}
		
	}
	\end{array}
	\]
	\centering
	\caption{Small-step semantics for RE parsing.}
	\label{figure:smallstep}
\end{figure*}
The rules of the semantics can be divided in two groups: starting rules and finishing rules.
Starting rules deal with configurations with a begin ($B$) direction and denote that we are
beginning the parsing for its RE $e$. Finishing rules use the context to decide how the parsing
for some expression should end. Intuitively, starting rules correspond to transitions entering a
sub-automata of Thompson NFA and finishing rules to transitions exiting a sub-automata.

The meaning of each starting rule is as follows. Rule $\{Eps\}$ specifies that we can mark a state as
finished if it consists of a starting configuration with RE $\epsilon$. We can finish any configuration
for RE \ensuremath{\C{Chr}\;\V{a}} if it is starting with current string with a leading $a$. Whenever we have a starting configuration
with a choice RE, $e_1 + e_2$, we can non-deterministically choose if input string $s$ can be processed by
$e_1$ (rule $Left_B$) or $e_2$ (rule $Right_B$). For beginning configurations with concatenation, we parse
input string using each of its components sequentially. Finally, for starting configurations with a Kleene
star operator, $e^\star$, we can either start the processing of $e$ or finish the processing for $e^\star$.
In all recursive cases for RE, we insert context information in the third component of the resulting
configuration in order to decide how the machine should step after finishing the execution of the RE
currently on focus.

Rule $(Cat_{EL})$ applies to any configuration which is finishing with a left concatenation context ($E[\,]e'$).
In such situation, rule specifies that a computation should continue with $e'$ and push the context $e\,E[\,]$.
We end the computation for a concatenation, whenever we find a context $e\,E[\,]$ in the context component
(rule $(Cat_{ER})$). Finishing a computation for choice consists in just popping its correspondent context,
as done by rules $(Left_E)$ and $(Right_E)$. For the Kleene star operator, we can either finish the computation
by popping the contexts and adding the corresponding \ensuremath{\C{1_b}} to end its matching list or restart with RE $e$ for
another matching over the input string.

The proposed semantics is inspired by Thompson's NFA construction
(as shown in Section \ref{subsection:thompsonnfraconstruction}).
First, the rule $Eps$ can be understood as executing the transition highlighted
in red in the following schematic automata.

\begin{center}
	\begin{tikzpicture}[auto, node distance=24mm, initial text=, >=latex]
	\node[state, initial, fill=white]   (q_1) [] {};
	\node[state, accepting, fill=white] (q_2) [right of=q_1] {};
	
	\path[->](q_1) edge [red] node {$\epsilon$}  (q_2);
	\end{tikzpicture}
\end{center}

The $Chr$ rule corresponds to the following transition (represented in red) in the next automata.

\begin{center}
	\begin{tikzpicture}[auto, node distance=24mm, initial text=, >=latex]
	\node[state, initial, fill=white]   (q_1) [] {};
	\node[state, accepting, fill=white] (q_2) [right of=q_1] {};
	
	\path[->] (q_1) edge [red] node {$a$}  (q_2);
	\end{tikzpicture}
\end{center}

Rule $Cat_B$ corresponds to start the processing of the input string in the automata $N(e_1)$;
while rule $Cat_{EL}$ deals with exiting the automata  $N(e_1)$ followed by processing the remaining
string in $N(e_2)$. Rule $Cat_{ER}$ deals with ending the processing in the automata below.

\begin{center}
	\begin{tikzpicture}[auto,  node distance=15mm, initial text=, >=latex]
	\node[state, initial]   (a_1)   []               {};
	\node[draw=none,fill=none]             (namea) [right of=a_1] {$N(e_1)$};
	\node[state] (a_2)   [right of=namea] {};
	
	\node[draw=none]             (nameb) [right of=a_2]  {$N(e_2)$};
	\node[state, accepting]  (b_2)   [right of=nameb] {};
	
	\begin{scope}[on background layer]
	\node[ellipse, draw=red, aspect=5, minimum width=40mm, minimum height=20mm, right of=a_1] {};
	\node[ellipse, draw=red, aspect=5, minimum width=40mm, minimum height=20mm, right of=a_2] {};
	\end{scope}
	\end{tikzpicture}
\end{center}

If we consider a RE $e = e_1 + e_2$ and lets $N(e_1)$ and $N(e_2)$ be two NFAs for
$e_1$ and $e_2$, respectively, we have the following correspondence between transtions and
semantics rules in the next NFA:

\begin{itemize}
	\item Red transition for rule $Left_B$;
	\item Green for $Right_B$;
	\item Blue for $Left_E$; and
	\item Black for $Right_E$.
\end{itemize}

\begin{center}
	\begin{tikzpicture}[auto, node distance=17mm, initial text=, >=latex]
	\node[state, initial]  (s_i)   []                   {};
	\node[state]        (a_1)   [above right of=s_i] {};
	\node[draw=none,fill=none]            (namea) [right of=a_1] {$N(e_1)$};
	\node[state]         (a_2)   [right of=namea]     {};
	
	\node[state]        (b_1)   [below right of=s_i] {};
	\node[draw=none]            (nameb) [right of=b_1]           {$N(e_2)$};
	\node[state]         (b_2)   [right of=nameb]     {};
	
	\node[state, accepting] (s_a)   [below right of=a_2] {};
	
	\path[->] (s_i) edge [below right, red] node {$\epsilon$} (a_1)
	edge [above right, green] node {$\epsilon$} (b_1)
	(a_2) edge [below left, blue]  node {$\epsilon$} (s_a)
	(b_2) edge []            node {$\epsilon$} (s_a);
	\begin{scope}[on background layer]
	\node[ellipse, draw=black, aspect=5, minimum width=45mm, minimum height=20mm, right of=b_1] {};
	\node[ellipse, draw=black, aspect=5, minimum width=45mm, minimum height=20mm, right of=a_1] {};
	\end{scope}
	\end{tikzpicture}
\end{center}

Finally, we present Kleene star rules in next automata according to Thompson's NFA construction.
The colors are red for $Star_1$ rule, green for $Star_2$, blue for  $Star_{E1}$ and black for $Star_{E2}$.

\begin{center}
	\begin{tikzpicture}[auto, node distance=15mm, initial text=, >=latex]
	\node[state, initial]  (s_i)   []               {};
	\node[state]        (a_1)   [right of=s_i]   {};
	\node[draw=none]            (namea) [right of=a_1]   {$N(e_1)$};
	\node[state]         (a_2)   [right of=namea] {};
	
	\node[state, accepting] (s_a)   [right of=a_2]   {};
	
	\path[->] (s_i) edge [red]                     node {$\epsilon$} (a_1)
	edge [bend right=40, below, green] node {$\epsilon$} (s_a)
	(a_2) edge [black]                     node {$\epsilon$} (s_a)
	edge [bend right=90, above, blue] node {$\epsilon$} (a_1);
	\begin{scope}[on background layer]
	\node[ellipse, draw=black, aspect=5, minimum width=50mm, minimum height=20mm, right of=a_1] {};
	\end{scope}
	\end{tikzpicture}
\end{center}

The starting state of the semantics is given by the configuration
$\conf{B,e,[],[],s}$ and accepting configurations are $\conf{F,e',[],bs,[]}$, for some RE $e'$ and code $bs$.
Following common practice, we let $\to^\star$ denote the reflexive, transitive closure of the small-step
semantics defined in Figure~\ref{figure:smallstep}.
We say that a string $s$ is accepted by RE $e$ if $\conf{B,e,[],[],s}\to^\star\conf{F,e',[],bs,[]}$.
The next theorem asserts that our semantics is sound and complete with respect to RE
inductive semantics (Figure~\ref{figure:resemantics}).

\begin{Theorem}
	For all strings $s$ and non-problematic REs $e$, $s\in\sembrackets{e}$ if, and only if, $\conf{B,e,[],[],s}\to^\star\conf{F,e',[],b,[]}$ and
	$\conf{F,e',[],b,[]}$ is an accepting configuration.
\end{Theorem}
\begin{proof}
	$(\to)$: We proceed by induction on the derivation of $s\in\sembrackets{e}$.
	\begin{enumerate}
		\item Case rule $Eps$: Then, $e = \epsilon$, $s = \epsilon$ and the conclusion is immediate.
		\item Case rule $Chr$: Then, $e = a$, $s = a$ and the conclusion follows.
		\item Case rule $Left$: Then, $e = e_1 + e_2$ and $s \in\sembrackets{e_1}$. By the induction hypothesis,
		we have $\conf{B,e_1,ctx,b,s}\to^\star\conf{E,e',ctx',b',[]}$ and the conclusion follows.
		\item Case rule $Right$: Then, $e = e_1 + e_2$ and $s \in\sembrackets{e_2}$. By the induction hypothesis,
		we have $\conf{B,e_2,ctx,b,s}\to^\star\conf{E,e',ctx',b',[]}$ and the conclusion follows.
		\item Case rule $Cat$: Then, $e = e_1\:e_2$, $s_1 \in\sembrackets{e_1}$, $s_2\in\sembrackets{e_2}$ and
		$s = s_1\,s_2$. By the induction hypothesis on $s_1 \in\sembrackets{e_1}$ we have that
		$\conf{B,e_1,ctx,b,s}\to^\star\conf{E,e',E[\,]\,e_2 : ctx,b',[]}$ and by
		induction hypothesis on $s_2\in\sembrackets{e_2}$, we have
		$\conf{B,e_2,e_1\,E[\,]:ctx,b,s}\to^\star\conf{E,e',ctx,b',[]}$ and the conclusion follows.
		\item Case rule $StarBase$: Then, $e = e_1^\star$ and $s = \epsilon$. The conclusion is immediate.
		\item Case rule $StarRec$: Then, $e = e_1^\star$, $s = s_1s_2$, $s_1 \in \sembrackets{e_1}$ and
		$s_2\in\sembrackets{e_1^\star}$. By the induction hypothesis on  $s_1 \in \sembrackets{e_1}$, we
		have $\conf{B,e_1,\star : ctx,b,s_1}\to^\star\conf{E,e', \star: ctx,b',[]}$, the induction
		hypothesis on $s_2\in\sembrackets{e_1^\star}$ give us $\conf{B,e_1^\star,\star : ctx,b,s_2}\to^\star\conf{E,e', \star: ctx,b',[]}$
		and conclusion follows.
	\end{enumerate}
	$(\leftarrow)$: We proceed by induction on $e$.
	\begin{enumerate}
		\item Case $e = \epsilon$. Then, we have
		$\conf{B,\epsilon,ctx,b,s}\to^\star\conf{E,e',ctx',b',[]}$ and $s = \epsilon$. Conclusion follows by rule $Eps$.
		\item Case $e = a$. Then
		$\conf{B,a,ctx,b,s}\to^\star\conf{E,e',ctx',b',[]}$ and $s = a$. Conclusion follows by rule $Chr$.
		\item Case $e = e_1 + e_2$. Now, we consider the following cases.
		\begin{enumerate}
			\item $s$ is accepted by $e_1$. Then, we have the following derivation:
			\[\conf{B,e_1 + e_2,ctx,b,s}\to\conf{B,e_1,E[\,] + e_2 : ctx,b,s} \to^\star\conf{E,e',ctx',b',[]}\]
			By induction hypothesis on $e_1$ and the derivation $\conf{B,e_1,E[\,] + e_2 : ctx,b,s} \to^\star\conf{E,e',ctx',b',[]}$
			we have $s \in\sembrackets{e_1}$ and the conclusion follows by rule $Left$.
			\item $s$ is accepted by $e_2$. Then, we have the following derivation:
			\[\conf{B,e_2,ctx,b,s}\to\conf{B,e_1,e_1 + E[\,] : ctx,b,s} \to^\star\conf{E,e',ctx',b',[]}\]
			By induction hypothesis on $e_2$ and the derivation $\conf{B,e_1,e_1 + E[\,] : ctx,b,s} \to^\star\conf{E,e',ctx',b',[]}$,
			we have $s \in\sembrackets{e_2}$ and conclusion follows by rule $Right$.
		\end{enumerate}
	\end{enumerate}
\end{proof}

\section{Small-step implementation details}\label{section:smallstepimplementation}

We chose Haskell to implement the first version of our VM-based algorithm due to Haskell's easiness to quickly prototype an interpreter for our small-step semantics. Thus, it could be easier and faster to discover eventual obvious errors in our semantics formulation (i.e., a '`cheap verification''), mainly because of the use of QuickCheck (Section \ref{section:quickcheck}). After having a stable version of our implementation, it is safer to move towards to the formal verification in Coq.

In order to implement the small-step semantics of Figure~\ref{figure:smallstep}, we need to represent configurations.
We use type \ensuremath{\D{Conf}} to denote configurations and directions are represented by type \ensuremath{\D{Dir}}, where \ensuremath{\C{Begin}} denote the
starting and \ensuremath{\C{End}} the finishing direction.

\begin{hscode}\SaveRestoreHook
	\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
	\>[B]{}\mathkw{data}\;\D{Dir}\mathrel{=}\C{Begin}\mid \C{End}{}\<[E]%
	\\
	\>[B]{}\mathkw{type}\;\D{Conf}\mathrel{=}(\D{Dir},\D{Regex},[\mskip1.5mu \D{Hole}\mskip1.5mu],\D{Code},\D{String}){}\<[E]%
	\ColumnHook
\end{hscode}\resethooks

Function \ensuremath{\F{finish}} tests if a configuration is an accepting one.

\begin{hscode}\SaveRestoreHook
	\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
	\>[B]{}\F{finish}\mathbin{::}\D{Conf}\to \D{Bool}{}\<[E]%
	\\
	\>[B]{}\F{finish}\;(\C{End},\anonymous ,[\mskip1.5mu \mskip1.5mu],\anonymous ,[\mskip1.5mu \mskip1.5mu])\mathrel{=}\C{True}{}\<[E]%
	\\
	\>[B]{}\F{finish}\;\anonymous \mathrel{=}\C{False}{}\<[E]%
	\ColumnHook
\end{hscode}\resethooks


The small-step semantics is implemented by function \ensuremath{\F{next}}, which returns a list of configurations that can
be reached from a given input configuration. We will begin by explaining the equations that code the set of
starting rules from the small-step semantics. The first alternative

\begin{hscode}\SaveRestoreHook
	\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
	\>[B]{}\F{next}\mathbin{::}\D{Conf}\to [\mskip1.5mu \D{Conf}\mskip1.5mu]{}\<[E]%
	\\
	\>[B]{}\F{next}\;(\C{Begin},\C{\epsilon},\V{ctx},\V{bs},\V{s})\mathrel{=}[\mskip1.5mu (\C{End},\C{\epsilon},\V{ctx},\V{bs},\V{s})\mskip1.5mu]{}\<[E]%
	\ColumnHook
\end{hscode}\resethooks
implements rule $(Eps)$, which finishes a starting \ensuremath{\D{Conf}} with an \ensuremath{\C{\epsilon}}. Rule $(Chr)$ is implemented by
the following equation
\begin{hscode}\SaveRestoreHook
	\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{15}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
	\>[B]{}\F{next}\;(\C{Begin},\C{Chr}\;\V{c},\V{ctx},\V{bs},\V{a}\mathbin{:}\V{s}){}\<[E]%
	\\
	\>[B]{}\hsindent{3}{}\<[3]%
	\>[3]{}\mid \V{a}\equiv \V{c}\mathrel{=}{}\<[15]%
	\>[15]{}[\mskip1.5mu (\C{End},\C{Chr}\;\V{c},\V{ctx},\V{bs},\V{s})\mskip1.5mu]{}\<[E]%
	\\
	\>[B]{}\hsindent{3}{}\<[3]%
	\>[3]{}\mid \F{otherwise}\mathrel{=}[\mskip1.5mu \mskip1.5mu]{}\<[E]%
	\ColumnHook
\end{hscode}\resethooks
which consumes input character \ensuremath{\V{a}} if it matches RE \ensuremath{\C{Chr}\;\V{c}}, otherwise it fails by returning an empty list.
For a choice expression, we can use two distinct rules: one for parsing the input using its left component and
another rule for the right. Since both union and Kleene star introduce non-determinism in RE parsing, we can
easily model this using the list monad, by return a list of possible resulting configurations.
\begin{hscode}\SaveRestoreHook
	\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
	\>[B]{}\F{next}\;(\C{Begin},\V{e}\C{\:+\:}\V{e'},\V{ctx},\V{bs},\V{s}){}\<[E]%
	\\
	\>[B]{}\hsindent{3}{}\<[3]%
	\>[3]{}\mathrel{=}[\mskip1.5mu (\C{Begin},\V{e},\C{InChoiceL}\;\V{e'}\mathbin{:}\V{ctx},\C{0_b}\mathbin{:}\V{bs},\V{s}){}\<[E]%
	\\
	\>[3]{}\hsindent{2}{}\<[5]%
	\>[5]{},(\C{Begin},\V{e'},\C{InChoiceR}\;\V{e}\mathbin{:}\V{ctx},\C{1_b}\mathbin{:}\V{bs},\V{s})\mskip1.5mu]{}\<[E]%
	\ColumnHook
\end{hscode}\resethooks
Concatenation just sequences the computation of each of its composing RE.
\begin{hscode}\SaveRestoreHook
	\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
	\>[B]{}\F{next}\;(\C{Begin},\V{e}\C{\:\bullet\:}\V{e'},\V{ctx},\V{bs},\V{s}){}\<[E]%
	\\
	\>[B]{}\hsindent{3}{}\<[3]%
	\>[3]{}\mathrel{=}[\mskip1.5mu (\C{Begin},\V{e},\C{InCatL}\;\V{e'}\mathbin{:}\V{ctx},\V{bs},\V{s})\mskip1.5mu]{}\<[E]%
	\ColumnHook
\end{hscode}\resethooks
For a starting configuration with Kleene star operator, \ensuremath{\C{Star}\;\V{e}}, we can proceed in two ways: by beginning the
parsing of RE \ensuremath{\V{e}} or by finishing the computation for \ensuremath{\C{Star}\;\V{e}} over the input.
\begin{hscode}\SaveRestoreHook
	\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
	\>[B]{}\F{next}\;(\C{Begin},\C{Star}\;\V{e},\V{ctx},\V{bs},\V{s}){}\<[E]%
	\\
	\>[B]{}\hsindent{3}{}\<[3]%
	\>[3]{}\mathrel{=}[\mskip1.5mu (\C{Begin},\V{e},\C{InStar}\mathbin{:}\V{ctx},\C{0_b}\mathbin{:}\V{bs},\V{s}){}\<[E]%
	\\
	\>[3]{}\hsindent{2}{}\<[5]%
	\>[5]{},(\C{End},(\C{Star}\;\V{e}),\V{ctx},\C{1_b}\mathbin{:}\V{bs},\V{s})\mskip1.5mu]{}\<[E]%
	\ColumnHook
\end{hscode}\resethooks
The remaining equations of \ensuremath{\F{next}} deal with operational semantics finishing rules. The equation below implements
rule $(Cat_{EL})$ which specifies that an ended computation for the left component of a concatenation should continue
with its right component.
\begin{hscode}\SaveRestoreHook
	\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
	\>[B]{}\F{next}\;(\C{End},\V{e},\C{InCatL}\;\V{e'}\mathbin{:}\V{ctx},\V{bs},\V{s}){}\<[E]%
	\\
	\>[B]{}\hsindent{3}{}\<[3]%
	\>[3]{}\mathrel{=}[\mskip1.5mu (\C{Begin},\V{e'},\C{InCatR}\;\V{e}\mathbin{:}\V{ctx},\V{bs},\V{s})\mskip1.5mu]{}\<[E]%
	\ColumnHook
\end{hscode}\resethooks
Whenever we are in a finishing configuration with a right concatenation context, (\ensuremath{\C{InCatR}\;\V{e}}), we end the parsing of
the input for the whole concatenation RE.
\begin{hscode}\SaveRestoreHook
	\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
	\>[B]{}\F{next}\;(\C{End},\V{e'},\C{InCatR}\;\V{e}\mathbin{:}\V{ctx},\V{bs},\V{s}){}\<[E]%
	\\
	\>[B]{}\hsindent{3}{}\<[3]%
	\>[3]{}\mathrel{=}[\mskip1.5mu (\C{End},\V{e}\C{\:\bullet\:}\V{e'},\V{ctx},\V{bs},\V{s})\mskip1.5mu]{}\<[E]%
	\ColumnHook
\end{hscode}\resethooks
Next equations implement the rules that finish configurations for the union, by committing to its first successful branch.
\begin{hscode}\SaveRestoreHook
	\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
	\>[B]{}\F{next}\;(\C{End},\V{e},\C{InChoiceL}\;\V{e'}\mathbin{:}\V{ctx},\V{bs},\V{s}){}\<[E]%
	\\
	\>[B]{}\hsindent{3}{}\<[3]%
	\>[3]{}\mathrel{=}[\mskip1.5mu (\C{End},\V{e}\C{\:+\:}\V{e'},\V{ctx},\C{0_b}\mathbin{:}\V{bs},\V{s})\mskip1.5mu]{}\<[E]%
	\\
	\>[B]{}\F{next}\;(\C{End},\V{e'},\C{InChoiceR}\;\V{e}\mathbin{:}\V{ctx},\V{bs},\V{s}){}\<[E]%
	\\
	\>[B]{}\hsindent{3}{}\<[3]%
	\>[3]{}\mathrel{=}[\mskip1.5mu (\C{End},\V{e}\C{\:+\:}\V{e'},\V{ctx},\C{1_b}\mathbin{:}\V{bs},\V{s})\mskip1.5mu]{}\<[E]%
	\ColumnHook
\end{hscode}\resethooks
Equations for Kleene star implement rules $(Star_{E1})$ and $(Star_{E2})$ which allows ending or add one more match
for an RE $e$.
\begin{hscode}\SaveRestoreHook
	\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
	\>[B]{}\F{next}\;(\C{End},\V{e},\C{InStar}\mathbin{:}\V{ctx},\V{bs},\V{s}){}\<[E]%
	\\
	\>[B]{}\hsindent{3}{}\<[3]%
	\>[3]{}\mathrel{=}[\mskip1.5mu (\C{Begin},\V{e},\C{InStar}\mathbin{:}\V{ctx},\C{0_b}\mathbin{:}\V{bs},\V{s}){}\<[E]%
	\\
	\>[3]{}\hsindent{2}{}\<[5]%
	\>[5]{},(\C{End},(\C{Star}\;\V{e}),\V{ctx},\C{1_b}\mathbin{:}\V{bs},\V{s})\mskip1.5mu]{}\<[E]%
	\ColumnHook
\end{hscode}\resethooks
Finally, stuck states on the semantics are properly handled by the following equation which turns them all
into a failure (empty list).
\begin{hscode}\SaveRestoreHook
	\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
	\>[B]{}\F{next}\;\anonymous \mathrel{=}[\mskip1.5mu \mskip1.5mu]{}\<[E]%
	\ColumnHook
\end{hscode}\resethooks
The reflexive-transitive closure of the semantics is implemented by function \ensuremath{\F{steps}}, which computes the
trace of all states needed to determine if a string can be parsed by the RE $e$.
\begin{hscode}\SaveRestoreHook
	\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
	\>[B]{}\F{steps}\mathbin{::}[\mskip1.5mu \D{Conf}\mskip1.5mu]\to [\mskip1.5mu \D{Conf}\mskip1.5mu]{}\<[E]%
	\\
	\>[B]{}\F{steps}\;[\mskip1.5mu \mskip1.5mu]\mathrel{=}[\mskip1.5mu \mskip1.5mu]{}\<[E]%
	\\
	\>[B]{}\F{steps}\;\V{cs}\mathrel{=}\F{steps}\;[\mskip1.5mu \V{c'}\mid \V{c}\leftarrow \V{cs},\V{c'}\leftarrow \F{next}\;\V{c}\mskip1.5mu]\plus \V{cs}{}\<[E]%
	\ColumnHook
\end{hscode}\resethooks
Finally, the function for parsing a string using an input RE is implemented as follow
s:
\begin{hscode}\SaveRestoreHook
	\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{15}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{17}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
	\>[B]{}\F{vmAccept}\mathbin{::}\D{String}\to \D{Regex}\to (\D{Bool},\D{Code}){}\<[E]%
	\\
	\>[B]{}\F{vmAccept}\;\V{s}\;\V{e}\mathrel{=}\mathkw{let}\;\V{r}\mathrel{=}[\mskip1.5mu \V{c}\mid \V{c}\leftarrow \F{steps}\;\F{init_{cfg}},\F{finish}\;\V{c}\mskip1.5mu]{}\<[E]%
	\\
	\>[B]{}\hsindent{15}{}\<[15]%
	\>[15]{}\mathkw{in}\;\mathkw{if}\;\F{null}\;\V{r}\;\mathkw{then}\;(\C{False},[\mskip1.5mu \mskip1.5mu])\;\mathkw{else}\;(\C{True},\F{bitcode}\;(\F{head}\;\V{r})){}\<[E]%
	\\
	\>[B]{}\hsindent{15}{}\<[15]%
	\>[15]{}\mathkw{where}{}\<[E]%
	\\
	\>[15]{}\hsindent{2}{}\<[17]%
	\>[17]{}\F{init_{cfg}}\mathrel{=}[\mskip1.5mu (\C{Begin},\V{e},[\mskip1.5mu \mskip1.5mu],[\mskip1.5mu \mskip1.5mu],\V{s})\mskip1.5mu]{}\<[E]%
	\\
	\>[15]{}\hsindent{2}{}\<[17]%
	\>[17]{}\F{bitcode}\;(\anonymous ,\anonymous ,\anonymous ,\V{bs},\anonymous )\mathrel{=}\F{reverse}\;\V{bs}{}\<[E]%
	\ColumnHook
\end{hscode}\resethooks
Function \ensuremath{\F{vmAccept}} returns a pair formed by a boolean and the bit-code produced during the parsing of
an input string and RE. Observe that we need to reverse the bit-codes, since they are built in reverse
order.

\section{Testsuit}\label{section:smallsteptestsuit}

In order to test the correctness of our semantics, we needed to
build generators for REs and for strings. We develop functions to randomly generate strings accepted
and rejected for a RE, using the QuickCheck library.

Generation of random RE is done by function \ensuremath{\F{sizedRegex}}, which takes a depth limit to restrict
the size of the generated RE. Whenever the input depth limit is less or equal to 1, we can
only build a \ensuremath{\C{\epsilon}} or a single character RE. The definition of \ensuremath{\F{sizedRegex}} uses
QuickCheck function \ensuremath{\F{frequency}}, which receives a list of pairs formed by a weight and
a random generator and produces, as result, a generator which uses such frequency distribution.
In \ensuremath{\F{sizedRegex}} implementation we give a higher weight to generate characters and equal distributions
to build concatenation, union or star.

\begin{hscode}\SaveRestoreHook
	\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{10}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{23}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
	\>[B]{}\F{sizedRegex}\mathbin{::}\D{Int}\to \D{Gen}\;\D{Regex}{}\<[E]%
	\\
	\>[B]{}\F{sizedRegex}\;\V{n}{}\<[E]%
	\\
	\>[B]{}\hsindent{3}{}\<[3]%
	\>[3]{}\mid \V{n}\leq \C{1}\mathrel{=}\F{frequency}\;[\mskip1.5mu (\C{10},\F{return}\;\C{\epsilon}),(\C{90},\C{Chr}\F{\,\langle\$\rangle\,}\F{genChar})\mskip1.5mu]{}\<[E]%
	\\
	\>[B]{}\hsindent{3}{}\<[3]%
	\>[3]{}\mid \F{otherwise}\mathrel{=}\F{frequency}\;[\mskip1.5mu (\C{10},\F{return}\;\C{\epsilon}),(\C{30},\C{Chr}\F{\,\langle\$\rangle\,}\F{genChar}){}\<[E]%
	\\
	\>[3]{}\hsindent{7}{}\<[10]%
	\>[10]{},(\C{20},(\C{\:\bullet\:})\F{\,\langle\$\rangle\,}\F{sizedRegex}\;\V{n2}\F{\,\langle\star\rangle\,}\F{sizedRegex}\;\V{n2}){}\<[E]%
	\\
	\>[3]{}\hsindent{7}{}\<[10]%
	\>[10]{},(\C{20},(\C{\:+\:})\F{\,\langle\$\rangle\,}\F{sizedRegex}\;\V{n2}\F{\,\langle\star\rangle\,}\F{sizedRegex}\;\V{n2}){}\<[E]%
	\\
	\>[3]{}\hsindent{7}{}\<[10]%
	\>[10]{},(\C{20},\C{Star}{}\<[23]%
	\>[23]{}\F{\,\langle\$\rangle\,}\F{suchThat}\;(\F{sizedRegex}\;\V{n2})\;(\F{not}\mathbin{\circ}\F{nullable}))\mskip1.5mu]{}\<[E]%
	\\
	\>[3]{}\hsindent{7}{}\<[10]%
	\>[10]{}\mathkw{where}\;\V{n2}\mathrel{=}\F{div}\;\V{n}\;\C{2}{}\<[E]%
	\ColumnHook
\end{hscode}\resethooks

For simplicity and brevity, we only generate REs that do not contain sub-REs of the form $e^\star$,
where $e$ is nullable\footnote{A RE $e$ is \emph{nullable} if $\epsilon \in \sembrackets{e}$.}.
All results can be extended to problematic\footnote{We say that a RE $e$ is problematic if there's $e'$
	such that $e = e'^\star$ and $\epsilon \in\sembrackets{e'}$.} REs in the style of Frisch et. al~\cite{Frisch2004}.


Given an RE $e$, we can generate a random string $s$ such that $s \in\sembrackets{e}$
using the next definition. We generate strings by choosing randomly between branches of
a union or by repeating $n$ times a string $s$ which is accepted by $e$, whenever we
have $e^\star$ (function \ensuremath{\F{randomMatches}}).

\begin{hscode}\SaveRestoreHook
	\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{7}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{29}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{38}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
	\>[B]{}\F{randomMatch}\mathbin{::}\D{Regex}\to \D{Gen}\;\D{String}{}\<[E]%
	\\
	\>[B]{}\F{randomMatch}\;\C{\epsilon}\mathrel{=}\F{return}\;\text{\tt \char34 \char34}{}\<[E]%
	\\
	\>[B]{}\F{randomMatch}\;(\C{Chr}\;\V{c})\mathrel{=}\F{return}\;[\mskip1.5mu \V{c}\mskip1.5mu]{}\<[E]%
	\\
	\>[B]{}\F{randomMatch}\;(\V{e}\C{\:\bullet\:}\V{e'})\mathrel{=}\F{liftM2}\;(\plus )\;(\F{randomMatch}\;\V{e})\;{}\<[E]%
	\\
	\>[B]{}\hsindent{38}{}\<[38]%
	\>[38]{}(\F{randomMatch}\;\V{e'}){}\<[E]%
	\\
	\>[B]{}\F{randomMatch}\;(\V{e}\C{\:+\:}\V{e'})\mathrel{=}\F{oneof}\;[\mskip1.5mu \F{randomMatch}\;\V{e},\F{randomMatch}\;\V{e'}\mskip1.5mu]{}\<[E]%
	\\
	\>[B]{}\F{randomMatch}\;(\C{Star}\;\V{e})\mathrel{=}\mathkw{do}{}\<[E]%
	\\
	\>[B]{}\hsindent{7}{}\<[7]%
	\>[7]{}\V{n}\leftarrow \F{choose}\;(\C{0},\C{3})\mathbin{::}\D{Gen}\;\D{Int}{}\<[E]%
	\\
	\>[B]{}\hsindent{7}{}\<[7]%
	\>[7]{}\F{randomMatches}\;\V{n}\;\V{e}{}\<[E]%
	\\[\blanklineskip]%
	\>[B]{}\F{randomMatches}\mathbin{::}\D{Int}\to \D{Regex}\to \D{Gen}\;\D{String}{}\<[E]%
	\\
	\>[B]{}\F{randomMatches}\;\V{m}\;\V{e'}{}\<[E]%
	\\
	\>[B]{}\hsindent{3}{}\<[3]%
	\>[3]{}\mid \V{m}\leq \C{0}\mathrel{=}\F{return}\;[\mskip1.5mu \mskip1.5mu]{}\<[E]%
	\\
	\>[B]{}\hsindent{3}{}\<[3]%
	\>[3]{}\mid \F{otherwise}\mathrel{=}\F{liftM2}\;(\plus )\;(\F{randomMatch}\;\V{e'})\;{}\<[E]%
	\\
	\>[3]{}\hsindent{26}{}\<[29]%
	\>[29]{}(\F{randomMatches}\;(\V{m}\mathbin{-}\C{1})\;\V{e'}){}\<[E]%
	\ColumnHook
\end{hscode}\resethooks
The algorithm for generating random strings that aren't accepted by a RE is similarly defined.

\subsection{Properties considered}\label{subsection:smallsteppropertiesconsidered}


In order to verify if the defined semantics is correct, we need to check the following properties:
\begin{enumerate}
	\item Our semantics accepts only and all the strings in the language described by the input RE: we
	test this property by generating random strings that should be accepted and strings that must
	be rejected by a random RE.
	\item Our semantics generates valid parsing evidence: the bit-codes produced as result have the
	following properties: 1) the bit-codes can be parsed into a valid parse tree $t$ for the random
	produced RE $e$, i.e. $\vdash t : e$ holds ; 2) \ensuremath{\F{flat}\;\V{t}\mathrel{=}\V{s}} and 3) \ensuremath{\F{code}\;\V{e}\;\V{t}\mathrel{=}\V{bs}}.
\end{enumerate}
Note that we need a correct implementation of RE parsing to verify the first property. We use the \ensuremath{\F{accept}}
function from~\cite{Fischer2010} for this and compare its result with \ensuremath{\F{vmAccept}}'s. The second property demands
that the bit-codes produced can be decoded into valid parsing evidence. The verification of produced bit-codes
is done by function \ensuremath{\F{validCode}} shown below.


\begin{hscode}\SaveRestoreHook
	\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{22}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{29}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
	\>[B]{}\F{validCode}\mathbin{::}\D{String}\to \D{Code}\to \D{Regex}\to \D{Bool}{}\<[E]%
	\\
	\>[B]{}\F{validCode}\;\anonymous \;[\mskip1.5mu \mskip1.5mu]\;\anonymous \mathrel{=}\C{True}{}\<[E]%
	\\
	\>[B]{}\F{validCode}\;\V{s}\;\V{bs}\;\V{e}\mathrel{=}\mathkw{case}\;\F{decode}\;\V{e}\;\V{bs}\;\mathkw{of}{}\<[E]%
	\\
	\>[B]{}\hsindent{22}{}\<[22]%
	\>[22]{}\C{Just}\;\V{t}\to \F{and}\;[\mskip1.5mu \F{tc}\;\V{t}\;\V{e},\F{flat}\;\V{t}\equiv \V{s},\F{code}\;\V{t}\;\V{e}\equiv \V{bs}\mskip1.5mu]{}\<[E]%
	\\
	\>[B]{}\hsindent{22}{}\<[22]%
	\>[22]{}\anonymous {}\<[29]%
	\>[29]{}\to \C{False}{}\<[E]%
	\ColumnHook
\end{hscode}\resethooks

Finally, function \ensuremath{\F{vmCorrect}} combines both properties mentioned above into a function that
is called to test the semantics implementation.

\begin{hscode}\SaveRestoreHook
	\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
	\>[B]{}\F{vmCorrect}\mathbin{::}\D{Regex}\to \D{String}\to \D{Property}{}\<[E]%
	\\
	\>[B]{}\F{vmCorrect}\;\V{e}\;\V{s}{}\<[E]%
	\\
	\>[B]{}\hsindent{3}{}\<[3]%
	\>[3]{}\mathrel{=}\mathkw{let}\;(\V{r},\V{bs})\mathrel{=}\F{vmAccept}\;\V{s}\;\V{e}{}\<[E]%
	\\
	\>[3]{}\hsindent{2}{}\<[5]%
	\>[5]{}\mathkw{in}\;(\F{accept}\;\V{e}\;\V{s}\F{\,\equiv\,}\V{r})\F{\,\land\,}\F{validCode}\;\V{s}\;\V{bs}\;\V{e}{}\<[E]%
	\ColumnHook
\end{hscode}\resethooks

In addition to coding / decoding of parse trees, we need a function which checks if a tree is indeed a
parsing evidence for some RE $e$. Function \ensuremath{\F{tc}} takes, as arguments, a parse tree $t$ and a RE $e$ and
verifies if $t$ is an evidence for $e$.

\begin{hscode}\SaveRestoreHook
	\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
	\>[B]{}\F{tc}\mathbin{::}\D{Tree}\to \D{Regex}\to \D{Bool}{}\<[E]%
	\\
	\>[B]{}\F{tc}\;\C{()}\;\C{\epsilon}\mathrel{=}\C{True}{}\<[E]%
	\\
	\>[B]{}\F{tc}\;(\C{Chr}\;\V{c})\;(\C{Chr}\;\V{c'})\mathrel{=}\V{c}\equiv \V{c'}{}\<[E]%
	\\
	\>[B]{}\F{tc}\;(\V{t}\C{\:\bullet\:}\V{t'})\;(\V{e}\C{\:\bullet\:}\V{e'})\mathrel{=}\F{tc}\;\V{t}\;\V{e}\mathrel{\wedge}\F{tc}\;\V{t'}\;\V{e'}{}\<[E]%
	\\
	\>[B]{}\F{tc}\;(\C{InL}\;\V{t})\;(\V{e}\C{\:+\:}\anonymous )\mathrel{=}\F{tc}\;\V{t}\;\V{e}{}\<[E]%
	\\
	\>[B]{}\F{tc}\;(\C{InR}\;\V{t'})\;(\anonymous \C{\:+\:}\V{e'})\mathrel{=}\F{tc}\;\V{t'}\;\V{e'}{}\<[E]%
	\\
	\>[B]{}\F{tc}\;(\C{List}\;\V{ts})\;(\C{Star}\;\V{e})\mathrel{=}\F{all}\;(\F{flip}\;\F{tc}\;\V{e})\;\V{ts}{}\<[E]%
	\ColumnHook
\end{hscode}\resethooks

Function \ensuremath{\F{tc}} is a implementation of parsing tree typing relation, as specified by the following
result.
\begin{Theorem}
	For all tree $t$ and RE $e$, $\vdash t : e$ if, and only if, \ensuremath{\F{tc}\;\V{t}\;\V{e}\mathrel{=}\C{True}}.
\end{Theorem}

\begin{proof}
	$(\to)$: We proceed by induction on the derivation of $\vdash t : e$.
	\begin{enumerate}
		\item Case rule $T1$: Then, $e = \epsilon$ and \ensuremath{\V{t}\mathrel{=}\C{()}} and conclusion follows.
		\item Case rule $T2$: Then, $e = a$ and \ensuremath{\V{t}\mathrel{=}\C{Chr}\;\V{a}} and conclusion follows.
		\item Case rule $T3$: Then, $e = e_1 + e_2$ and \ensuremath{\V{t}\mathrel{=}\C{InL}\;\V{tl}}, where $\vdash tl : e_1$.
		By induction hypothesis, we have that \ensuremath{\F{tc}\;\V{tl}\;\V{e1}\mathrel{=}\C{True}} and conclusion follows.
		\item Case rule $T4$: Then, $e = e_1 + e_2$ and \ensuremath{\V{t}\mathrel{=}\C{InR}\;\V{tr}}, where $\vdash tr : e_2$.
		By induction hypothesis, we have that \ensuremath{\F{tc}\;\V{tr}\;\V{e2}\mathrel{=}\C{True}} and conclusion follows.
		\item Case rule $T5$: Then, $e = e_1\:e_2$ and \ensuremath{\V{t}\mathrel{=}\V{tl}\C{\:\bullet\:}\V{tr}}. Conclusion is immediate
		from the induction hypothesis.
		\item Case rule $T6$: Then, $e = e_1^\star$ and \ensuremath{\V{t}\mathrel{=}\C{List}\;\V{ts}} and conclusion follows
		from the induction hypothesis on each element of \ensuremath{\V{ts}}.
	\end{enumerate}
	$(\leftarrow)$: We proceed by induction on $e$.
	\begin{enumerate}
		\item Case $e = \epsilon$: Then, \ensuremath{\V{t}\mathrel{=}\C{()}} and the conclusions follows by rule $T1$.
		\item Case $e = a$: Then, \ensuremath{\V{t}\mathrel{=}\C{Chr}\;\V{a}} and the conclusions follows by rule $T2$.
		\item Case $e = e_1 + e_2$: Now, we consider the following subcases:
		\begin{enumerate}
			\item Case \ensuremath{\V{t}\mathrel{=}\C{InL}\;\V{tl}}: By induction hypothesis, we have that \ensuremath{\F{tc}\;\V{tl}\;\V{e1}\mathrel{=}\C{True}} and conclusion follows.
			\item Case \ensuremath{\V{t}\mathrel{=}\C{InR}\;\V{tr}}: By induction hypothesis, we have that \ensuremath{\F{tc}\;\V{tr}\;\V{e2}\mathrel{=}\C{True}} and conclusion follows.
		\end{enumerate}
		\item Case $e = e_1\,e_2$: Then, \ensuremath{\V{t}\mathrel{=}\V{tl}\C{\:\bullet\:}\V{tr}} and conclusion follows by the induction
		hypothesis and the rule $T5$.
		\item Case $e = e_1^\star$: Then, \ensuremath{\V{t}\mathrel{=}\C{List}\;\V{ts}} and conclusion follows by induction hypothesis
		on each element of \ensuremath{\V{ts}} and rule $T6$.
	\end{enumerate}
\end{proof}

\subsection{Code coverage results}

After running thousands of well-succeeded tests, we gain a high degree of confidence in the correctness
of our semantics, however, it is important to measure how much of our code is covered by the test suite.
We use the Haskell Program Coverage tool (HPC)~\cite{Gill2007} to generate statistics about the execution of our tests.
Code coverage results are presented in Figure~\ref{figure:coverage}.

\begin{figure}[h!]
	\includegraphics[width=\linewidth]{coverage-results.png}
	\caption{Code coverage results}
	\label{figure:coverage}
\end{figure}

Our test suite give us almost 100\% of code coverage, which provides a strong evidence that our semantics
is indeed correct. All top level definitions and function alternatives are actually executed by the test cases
and just two expressions are marked as non-executed by HPC.

\chapter{Big-step operational semantics}\label{chapter:bigstepsemantics}

The small-step semantics presented in Chapter \ref{chapter:smallstepsemantics} was our first attempt to develop a VM-based algorithm for the RE parsing problem. Despite its high coverage results when submitted to QuickCheck, that semantics has some issues. As we stated previously, it does not work with problematic REs. We also had some issues when trying to formalize that semantics in Coq. 

To solve the first problem, we adopted a function which converts a problematic RE into an equivalent non-problematic one, as proposed by Medeiros et al. \cite{Medeiros14}. In order to formalize our small-step operational semantics in Coq, we now propose a big-step one for it, which is easier to understand and to formalize in Coq and behaves the same way as the small-step one. In fact, we consider the previous small-step semantics as an intermediate step to achieve the big-step semantics presented in this chapter.

\section{Dealing with problematic REs}\label{section:problematic}

A known problem in RE parsing is how to deal with the so-called problematic REs. A naive approach for 
parsing problematic REs can make the algorithm loop~\cite{Frisch2004}. Medeiros et al. \cite{Medeiros14}
present a function which converts a problematic RE into a equivalent non-problematic one.

The conversion function relies on two auxiliar definitions: one for testing if a RE accepts the empty string and 
other to test if a RE is equivalent to $\epsilon$. We name such functions as \nullable~ and \emptyy, 
respectively.

\[
\begin{array}{lcl}
\nullable(\emptyset) & = & \bot \\ 
\nullable(\epsilon)  & = & \top \\
\nullable(a)         & = & \bot \\ 
\nullable(e_1 + e_2) & = & \nullable(e_1)\lor\nullable(e_2)\\
\nullable(e_1\:e_2)  & = & \nullable(e_1)\land\nullable(e_2)\\
\nullable(e ^\star)  & = & \top \\
\\
\emptyy(\emptyset)    & = & \bot \\ 
\emptyy(\epsilon)     & = & \top \\
\emptyy(a)            & = & \bot \\
\emptyy(e_1 + e_2)    & = & \emptyy(e_1) \land \emptyy(e_2)\\
\emptyy(e_1\:e_2)     & = & \emptyy(e_1) \land \emptyy(e_2)\\
\emptyy(e ^\star)     & = & \emptyy(e)\\        
\end{array}
\]

Functions \nullable~ and \emptyy~ obeys the following correctness properties.


\begin{Lemma}
	\nullable($e$) = $\top$ if, and only if, $\epsilon\in\sembrackets{e}$.
\end{Lemma}
\begin{proof}$\,$\\
	$(\to)$ Induction over the structure of $e$. \\$(\leftarrow)$ Induction over the derivation of $\epsilon \in\sembrackets{e}$.
\end{proof}

\begin{Lemma}
	If \emptyy($e$) = $\top$ then $e \approx \epsilon$.
\end{Lemma}
\begin{proof}
	Induction over the structure of $e$.
\end{proof}


Given these two predicates, Medeiros et.al. define two mutually recursive functions,
named \fin~ and \fout. The function \fout~ recurses over the structure of an input RE
searching for a problematic sub-expression and \fin~ rewrites the Kleene star subexpression
so that it became non-problematic and preserves the language of the original 
RE~\cite{Medeiros14}. The definition of functions \fin~ and \fout~ are presented next.

\[
\begin{array}{lcl}
\fout(e) & = & e,\,\texttt{if } e = \epsilon, e = \emptyset \texttt{ or } e = a\\
\fout(e_1 + e_2) & = & \fout(e_1) + \fout(e_2) \\
\fout(e_1\:e_2) & = & \fout(e_1)\:\fout(e_2)\\
\fout(e^\star) & = & \left\{
\begin{array}{ll}
\fout(e)^\star & \texttt{if } \neg\,\nullable(e)\\
\epsilon       & \texttt{if } \emptyy(e)\\
\fin(e)^\star  & \texttt{otherwise}
\end{array}
\right.
\end{array}
\]

\[
\begin{array}{lcl}
\fin(e_1\:e_2) & = & \fin(e_1+e_2)\\
\fin(e_1 + e_2) & = & \left\{
\begin{array}{ll}
\fin(e_2)  & \texttt{if }\emptyy(e_1) \land \nullable(e_2)\\
\fout(e_2) & \texttt{if }\emptyy(e_1) \land \neg \nullable(e_2)\\
\fin(e_1)  & \texttt{if }\nullable(e_1) \land \emptyy(e_2)\\
\fout(e_1) & \texttt{if }\neg \nullable(e_1) \land \emptyy(e_2)\\
\fout(e_1) + \fin(e_2) & \texttt{if }\neg \nullable(e_1) \land \neg\emptyy(e_2)\\
\fin(e_1) + \fout(e_2) & \texttt{if }\neg \emptyy(e_1) \land \neg \nullable(e_2)\\
\fin(e_1) + \fin(e_2) & \texttt{otherwise}
\end{array}
\right. \\
\fin(e^\star) & = & \left\{
\begin{array}{ll}
\fin(e) & \texttt{if }\nullable(e)\\
\fout(e) & \texttt{otherwise}\\
\end{array}
\right.
\end{array}
\]

The result of applying \fout~ on a RE is producing an equivalent non-problematic one. This fact is expressed
by the following theorem. 

\begin{Theorem}
	If $\fout(e) = e'$ then $e \approx e'$ and $e'$ is a non-problematic RE.
\end{Theorem}
\begin{proof}
	Well-founded induction on the complexity of $(e,s)$, where $s$ is an arbitrary string, using
	several lemmas about RE equivalence and lemmas 1 and 2.
\end{proof}

This result is proved (informally\footnote{By ``informally'', we mean that the
	result is not mechanized in a proof assistant.}) by Medeiros et. al.~\cite{Medeiros14}. In order to formalize this result
in Coq, we needed to prove several theorems about RE equivalence. We postpone the discussion on some details
of our formalization to Section \ref{section:formalization}.

\section{Big-step semantics for RE parsing}\label{section:semantics}

In this section we present the definition of a big operational semantics for a VM for RE parsing.
The state of our VM is a pair formed by the current RE and the string being parsed. Each machine 
transition may produce, as a side effect, a bit-coded parse tree and the remaining string to be 
parsed. We denote our semantics by a judgement of the form $\langle e, s \rangle \leadsto (bs,s_p,s_r)$,
where $e$ is current RE, $s$ is the input string, $bs$ is the produced bit-coded tree, $s_p$ is
the parsed prefix of the input string and $s_r$ is the yet to be parsed string.

\begin{figure}[h]
	\[
	\begin{array}{ccc}
	\infer[_{\{EpsVM\}}]{\langle \epsilon, s\rangle \leadsto ([\,],\epsilon,s)}{} & &
	\infer[_{\{ChrVM\}}]{\langle a, as\rangle\leadsto ([\,],a,s)}{} \\ \\
	\infer[_{\{LeftVM\}}]{\langle e_1 + e_2,s\rangle\leadsto (0_b\,b,s_p,s_r)}{\langle e_1, s\rangle \leadsto (b,s_p,s_r)} & &
	\infer[_{\{RightVM\}}]{\langle e_1 + e_2,s\rangle\leadsto (1_b\,b,s_p,s_r)}{
		\langle e_2, s\rangle \leadsto (b,s_p,s_r)} \\ \\
	\infer[_{\{CatVM\}}]{\langle e_1\:e_2,s\rangle\leadsto (b_1\,b_2,s_{p1},s_{p2},s_r)}
	{\begin{array}{c}
		\langle e_1, s\rangle \leadsto (b_1,s_{p1},s_1) \\
		\langle e_2, s_1\rangle \leadsto (b_2,s_{p2},s_r)
		\end{array}} & & 
	\infer[_{\{NilVM\}}]{\langle e^\star, s\rangle\leadsto
		(1_b,\epsilon,s)}{\langle e, s \rangle
		\not\leadsto}\\ \\
	
	\multicolumn{3}{c}{
		\infer[_{\{ConsVM\}}]{\langle e^\star, s\rangle\leadsto (b_1\,b_2,s_{p1}\,s_{p2},s_r)}
		{\langle e, s \rangle \leadsto (b_1,s_{p1},s_1) & s_{p1} \neq \epsilon &
			\langle e^\star, s_1 \rangle \leadsto (b_2, s_{p2},s_r)}
	}
	\end{array}
	\]
	\caption{Operational semantics for RE parsing.}
	\label{figure:bigstepsemantics}
\end{figure}

The meaning of each semantics rules is as follows. Rule $EpsVM$ specifies that parsing $s$ using RE $\epsilon$ produces 
an empty list of bits and does not consume any symbol from $s$. Rule $ChrVM$ consumes the first symbol of the input string
if it matches the input RE.  Rules $LeftVM$ and 
$RightVM$ specifies how the semantics executes an RE $e + e'$, by trying to
parse the input using either the left or right subexpression. Note that, as a result, we append a bit $0_b$ when we successfully 
parse the input using the left choice operand and the bit $1_b$ for a parsing using the right operand. Rule $CatVM$ defines 
how a concatenation $e_1\,e_2$ is executed by the semantics: first, the input is parsed using the RE $e_1$ and the remaining 
string is used as input to execute $e_2$. The bit-coded tree for the $e_1\,e_2$ is just the concatenation of the produced 
codes for $e_1$ and $e_2$. Rules $NilVM$ and $ConsVM$ deal with unproblematic Kleene star REs. The rule $NilVM$ is only applicable when
is not possible to parse the input using the RE $e$ in $e^\star$. Rule $ConsVM$ can be used whenever we can parse the
input using $e$ and the parsed prefix is not an empty string. The remaining string ($s_1$) of $e$'s parsing is used as
input for the next iteration of RE $e^\star$ parsing. 

Evidently, the proposed semantics is sound and complete w.r.t. standard RE
semantics and only produces valid parsing evidence.

\begin{Theorem}[Soundness]
	If $\langle e, s \rangle \leadsto (bs,s_p,s_r)$ then $s = s_p\,s_r$ and $s_p\in\sembrackets{e}$.
\end{Theorem}
\begin{proof}
	Well-founded induction on the complexity of $(e,s)$.
\end{proof}

\begin{Theorem}[Completeness]
	If $s_p\in\sembrackets{e}$ then for all $s_r$ we have that exists $bs$, s.t. $\langle e, s_p\,s_r \rangle \leadsto (bs,s_p,s_r)$.
\end{Theorem}
\begin{proof}
	Well-founded induction on the complexity of $(e,s)$.
\end{proof}

\begin{Theorem}[Parsing result soundness]
	If $\langle e, s \rangle \leadsto (bs,s_p,s_r)$ then: 1) $bs \rhd e$; 2) $\flatten(\decodee(bs : e)) = s_p$; and 3)
	$\code(\decodee(bs : e) : e) = bs$. 
\end{Theorem}
\begin{proof}
	Well-founded induction on the complexity of $(e,s)$ using Theorem 3.
\end{proof}

\section{Coq formalization}\label{section:formalization}

In this section we describe the main design decisions in our formalization.

\paragraph{RE syntax and semantics} Our representation of RE syntax and
semantics is as usual in type theory-based proof assistants. We use an inductive
type to represent RE syntax and an inductive predicate to denote its semantics.

\begin{minted}{coq}
Inductive regex : Set :=
| Empty : regex | Eps : regex | Chr : ascii -> regex
| Cat : regex -> regex -> regex
| Choice : regex -> regex -> regex
| Star   : regex -> regex.
\end{minted}

Type \coq{regex} represents RE syntax and its definition is straightforward.
We use some notations to write \coq{regex} values. We let \coq{#0} denote
\coq{\empty}, \coq{#1} represents \coq{Eps}, while infix operators \coq{:+:} and
\coq{@} denote \coq{Choice} and \coq{Cat}. Finally, \coq{Star e} is
written \coq{(e ^*)}.

RE semantics is represented by type \coq{in_regex} which has a constructor for each
rule of the semantics presented in Figure~\ref{figure:resemantics}.

\begin{minted}{coq}
Inductive in_regex : string -> regex -> Prop :=
| InEps : "" <<- #1
| InChr : forall c, String c "" <<- ($ c)
| InLeft
:  forall s e e'
,  s <<- e
-> s <<- (e :+: e')
| InStarRight              
: forall s s' e s1 
, s <> ""  
-> s <<- e
-> s' <<- (e ^*)
-> s1 = s ++ s'
-> s1 <<- (e ^*)
...  (** some constructors omitted. *)
where "s '<<-' e" := (in_regex s e).
\end{minted}
We use notation \coq{s <<- e} to denote \coq{in_regex s e}.

\paragraph{RE equivalence} Using the previous presented semantics, we can define
RE equivalence by coding its standard definition in Coq as:
\begin{minted}{coq}
Definition regex_equiv (e e' : regex) : Prop :=
forall s, s <<- e <-> s <<- e'.
\end{minted}
We use notation \coq{e1 === e2} to denote \coq{regex_equiv e1 e2}. In our
formalization, we proved that \coq{regex_equiv} is an equivalence relation,
which is necessary to allow the rewriting of such equalities by Coq
tactics.

In order to complete our formalization, we needed several results about RE
equivalence. Most of them are proved by well-founded induction on the complexity
of a pair formed by a RE and a string (defined in Section~\ref{subsection:resyntaxsemantics}).
In order to formalize the needed ordering relation, we take advantage of Coq's
standard library, which provide several combinators to assemble well-founded
relations. As an example, consider the following fact used by Medeiros et. al to
prove the correctness of its \fout~function: $(e_1 + e_2)^\star \approx
(e_1\,e_2)^\star$, which holds if both $e_1$ and $e_2$ accepts the empty string.
In our formalization such equivalence is proved by the following theorem proved
by well-founded induction.

\begin{minted}{coq}
Lemma choice_star_cat_star 
: forall e1 e2, "" <<- e1 -> "" <<- e2 ->
((e1 @ e2) ^*) === ((e1 :+: e2) ^*). 

\end{minted}
Several other lemmas about RE equivalence were proved in order to complete
the formalization of the problematic RE conversion function. We omit them
for brevity.

\paragraph{Converting problematic REs} The first step to certify the algorithm
for converting problematic REs into non-problematic ones is to define the
predicates for testing if an input RE is nullable or if it is equivalent to
$\epsilon$. We define such functions using dependently typed programming, i.e.
its types provide certificates that the result has its desired correctness
property.

The nullability test is represented by function \coq{null}:
\begin{minted}{coq}
Definition null : forall e, {"" <<- e} + {~ "" <<- e}.
refine (fix null e : {"" <<- e} + {~ "" <<- e} :=
match e as e' return e = e' -> 
{"" <<- e'} + {~ "" <<- e'} with
| #1 => fun Heq => Yes
| e1 @ e2 => fun Heq =>
match null e1 , null e2 with
| Yes , Yes  => Yes
| _ , _   => No
end 
| e1 :+: e2 => fun Heq => ...
| e1 ^* => fun Heq => Yes
end (eq_refl e)) ...
(** some cases and tactics omitted *)
\end{minted}
Its type specifies that for any RE $e$ either $e$ accepts the empty string (i.e.
\coq{"" <<- e} holds) or not (\coq{~ "" <<- e}). Since such function contains
proofs terms, we se tactic \coq{refine} to define its computation content
leaving the logical subterms to be filled by tactics. The definition of
\coq{null} employs the convoy-pattern~\cite{Chlipala13}, which consists in
introducing an equivality to allow the refinement of each equation type in
dependently typed pattern-matching.

In order to specify the emptyness test predicate, we use an inductive type
which characterize when a RE is equivalent to $\epsilon$.
\begin{minted}{coq}
Inductive empty_regex : regex -> Prop :=
| Emp_Eps : empty_regex #1
| Emp_Cat : forall e e', empty_regex e ->
empty_regex e' ->
empty_regex (e @ e')
| Emp_Choice : forall e e', empty_regex e ->
empty_regex e' ->
empty_regex (e :+: e')
| Emp_Star : forall e, empty_regex e ->
empty_regex (e ^*).
\end{minted}
The meaning of each constructor of \coq{empty_regex} is as follows:
\coq{Emp_Eps} specifies that the empty RE is equivalent to itself.
For concatenation, choice and Kleene star, we can only say that they are
equivalent to $\epsilon$ if all of its subterms are also equivalent
to the empty RE.

Using the \coq{empty_regex} predicate we can easily prove the following
theorems. The first specifies that if \coq{empty_regex e} holds then
\coq{e} accepts the empty string and the second says that if
\coq{empty_regex e} is provable then \coq{e} is equivalent to the empty
string RE.

\begin{minted}{coq}
Lemma empty_regex_sem : forall e, empty_regex e -> "" <<- e.
Theorem empty_regex_spec : forall e, empty_regex e -> e === #1.
\end{minted}

The emptyness test function follows the same definition pattern as \coq{null}
using the \coq{refine} tactic. We specify its type using \coq{empty_regex}
predicate and we omit its definition for brevity.

Having defined these two predicates, we can implement the function to
convert problematic REs into non-problematic ones. The specification
of when a RE is problematic is given by the following inductive predicate. 
\begin{minted}{coq}
Inductive unproblematic : regex -> Prop :=
| UEmpty : unproblematic #0
| UEps   : unproblematic #1
| UChr   : forall c, unproblematic ($ c)
| UCat   : forall e e', unproblematic e ->
unproblematic e' ->
unproblematic (e @ e')
| UChoice : forall e e', unproblematic e ->
unproblematic e' ->
unproblematic (e :+: e')
| UStar : forall e, ~ ("" <<- e) -> 
unproblematic e -> 
unproblematic (Star e).
\end{minted}
Type \coq{unproblematic} says that empty set, empty string and single
characters REs are unproblematic. Concatenation and choice REs are
unproblematic if both its subexpression are unproblematic. Finally,
a Kleene star is unproblematic if its subexpression is unproblematic and
does not accept the empty string. Finally, we specify the problematic RE
conversion function with the following type:
\begin{minted}{coq}
Definition unprob 
: forall (e : regex), {e' | e === e' /\ unproblematic e'}.
\end{minted}
Function \coq{unprob} type says that from a input RE \coq{e} it returns another
RE \coq{e'} which is unproblematic and equivalent to \coq{e}. Again, we define
\coq{unprob} using \coq{refine} tactic and its definition follows is just the
Coq coding of \fout. As pointed by Medeiros et. al.~\cite{Medeiros14}, most of
the work to produce a unproblematic RE is done by function \fin, which is
applied when the inner RE of a Kleene star accepts the empty string and is not
equivalent to the empty RE. Function \coq{unprob_rec} implements \fin function
and we specify it with the following type:
\begin{minted}{coq}
Definition unprob_rec : forall e, "" <<- e -> ~ empty_regex e -> 
{e' | (e ^*) === (e' ^*) /\ ~ "" <<- e' /\ unproblematic e'}
\end{minted}
\coq{unprob_rec}'s type stabilishs that the return RE \coq{e'} is unproblematic, does not
accepts the empty string and that its Kleene star is equivalent to input REs
Kleene star, i.e.  \coq{(e ^*) === (e' ^*)}.

\section{Parse trees and bit-code representation}

In our formalization, we use the following inductive type to represent parse
trees:

\begin{minted}{coq}
Inductive tree : Set :=
| TUnit  : tree | TChr   : ascii -> tree
| TCat   : tree -> tree -> tree 
| TLeft  : tree -> tree | TRight : tree -> tree 
| TNil   : tree | TCons  : tree -> tree -> tree.
\end{minted}

Constructor \coq{TUnit} denotes a parse tree for the empty string RE,
\coq{TChr} the tree for a single symbol RE and \coq{TCat} the tree for
the concatenation of two REs. \coq{TLeft} and \coq{TRight} denote trees
for the choice operator. Constructors \coq{TCons} and \coq{TNil} can be
used to form a list of trees for a Kleene star RE.

The parse tree typing judgement is
coded as the following inductive predicate, where each constructor
has a correspondent rule in Figure~\ref{figure:parsetreetyping}.

\begin{minted}{coq}
Inductive is_tree_of : tree -> regex -> Prop :=
| ITUnit : TUnit :> #1
| ITChr  : forall c, (TChr c) :> ($ c)
| ITCat  : forall e t e' t',
t :> e   ->
t' :> e' ->
(TCat t t') :> (e @ e')
| ITLeft : forall e t e',
t :> e ->
(TLeft t) :> (e :+: e')
| ITRight : forall e e' t',
t' :> e' ->
(TRight t') :> (e :+: e')
| ITNil : forall e, TNil :> (Star e)
| ITCons : forall e t ts,
t :> e ->
ts :> (Star e) ->
(TCons t ts) :> (Star e)
where "t ':>' e" := (is_tree_of t e).
\end{minted}
Function \flatten~has a direct encoding as a Coq recursive definition and
we omit it for brevity. From \flatten~ and tree typing relation definitions,
theorems \ref{theorem:semanticstypingflatten} and
\ref{theorem:typingflattensemantics} are easily proved.

Bit coding of parse trees is represented by a list of bits, as follows:
\begin{minted}{coq}
Inductive bit : Set := O : bit | I : bit.
Definition code := list bit.
\end{minted}
The typing relation for bit-coded parse trees
(Figure~\ref{figure:typing-bitcodes}) has an immediate definition as an
inductively defined Coq relation.
\begin{minted}{coq}
Inductive is_code_of : code -> regex -> Prop :=
| ICEpsilon : [] :# #1
| ICChar    : forall c, [] :# ($ c)
| ICLeft    : forall bs e e'
, bs :# e ->
(O :: bs) :# (e :+: e')
| ICRight   : forall bs e e'
, bs :# e' ->
(I :: bs) :# (e :+: e')
| ICCat : forall bs bs' e e'
, bs :# e ->
bs' :# e' ->
(bs ++ bs') :# (e @ e')
| ICNil : forall e, (I :: []) :# (e ^*)
| ICCons : forall e bs bss, 
bs :# e -> 
bss :# (e ^*) -> 
(O :: bs ++ bss) :# (e ^*) 
where "bs ':#' e" := (is_code_of bs e).
\end{minted}
As with \flatten, function \code has a immediate Coq definition. The next
results about \code~are proved by a routine inductive proof.
\begin{minted}{coq}
Lemma encode_sound
: forall bs e, bs :# e -> exists t, t :> e /\ encode t = bs.
Lemma encode_complete
: forall t e, t :> e -> (encode t) :# e.   
\end{minted}
Unlike \code, function \decodee~ has a more elaborate recursive definition, as
shown in Section~\ref{subsection:bitcodedparsetrees}, since it recurses over the
input RE while threading the remaining bits to be parsed into a tree. Since it
has a more involved definition, we use dependent types to combine its definition
with its correctness proof. First, we define type \coq{nocode_for} which
denotes proofs that some bit list is not a valid bit-coded tree for some RE.
\begin{minted}{coq}
Inductive nocode_for : code -> regex -> Prop :=
| NCEmpty : forall bs, nocode_for bs #0
| NCChoicenil : forall e e', nocode_for [] (e :+: e')
| NCLBase : forall bs e e', 
nocode_for bs e -> 
nocode_for (O :: bs) (e :+: e')
| NCRBase : forall bs e e', 
nocode_for bs e' -> 
nocode_for (I :: bs) (e :+: e')
| NCStarnil : forall e, nocode_for [] (e ^*)
| NCStar : forall bs bs1 bs2 e,
is_code_of bs1 e ->
nocode_for bs2 (e ^*) ->
bs = O :: bs1 ++ bs2  ->
nocode_for bs (e ^*)
| NCStar1 : forall bs e,
nocode_for bs e ->
nocode_for (O :: bs) (e ^*).
(** some code omitted *)
\end{minted}
Constructor \coq{NCEmpty} specifies that there is no code for the empty set RE,
\coq{#0}. For choice REs, we have several cases to cover. Constructor
\coq{NCChoicenil} specifies that the empty list is not a valid code for any
choice RE. Constructor \coq{NCLBase} (\coq{NCRBase}) specifies that if a list
isn't a valid code for a RE \coq{e} (\coq{e'}) it cannot be used to form a valid
code for \coq{e :+: e'}. In order to build a proof that some bit list isn't a
valid code for a concatenation RE, we just need to prove that it is not a code
for some of its sub-expressions. Finally, for the Kleene star, we have some
cases to cover: first, constructor \coq{NCStarnil} shows that the empty list
cannot be a code for any star RE. For non-empty bit-lists, it is just necessary
to show that some part of the bit list isn't a code either for \coq{e} or
\coq{e ^*}.

Using predicate \coq{nocode_for} we can define a type for invalid bit-codes:
\begin{minted}{coq}
Definition invalid_code bs e :=
nocode_for bs e \/ exists t b1 bs1, bs = (code t) ++ (b1 :: bs1).
\end{minted}
which basically says that a bit list is an invalid code for a RE \coq{e} when
either we can construct a proof of \coq{nocode_for} or we can parse a prefix of it into a
valid tree but it leaves a non-empty bit list as a remaining suffix. Using this
infrastructure, we can define the decode function with the following type:
\begin{minted}{coq}
Definition decode e bs : 
{t | bs = code t /\ is_tree_of t e} + {invalid_code bs e}.
\end{minted}
Note that the previous type denotes the correctness property of a decode
function: either it returns a valid tree for the input RE that can be
converted into the input bit list or a proof that such bit list isn't a valid
code for the input RE.

\section{Formalizing the proposed semantics and its interpreter}

Our semantics definition consists of the Coq representation of the judgement in
Figure~\ref{figure:bigstepsemantics}, which is presented below.
\begin{minted}{coq}
Inductive in_regex_p : string -> regex -> string -> string -> Prop :=
| InEpsP
: forall s, s <$- #1 ; "" ; s
| InChrP
: forall a s,
(String a s) <$- ($ a) ; (String a "") ; s
| InLeftP
: forall s s' e e',
(s ++ s') <$- e ; s ; s' ->
(s ++ s') <$- (e :+: e') ; s ; s'
| InCatP : forall s s' s'' e e',
(s ++ s' ++ s'') <$- e ; s ; (s' ++ s'') ->
(s' ++ s'') <$- e' ; s' ; s'' ->
(s ++ s' ++ s'') <$- (e @ e') ; (s ++ s') ; s''
| InStarRightP : forall s1 s2 s3 e,
s1 <> "" ->
(s1 ++ s2 ++ s3) <$- e ; s1 ; (s2 ++ s3) ->
(s2 ++ s3) <$- (Star e) ; s2 ; s3 ->
(s1 ++ s2 ++ s3) <$- (Star e) ; (s1 ++ s2) ; s3
where "s '<$-' e ';' s1 ';' s2" := (in_regex_p s e s1 s2).
(** some code omitted *)
\end{minted}
In order to ease the task of writing types involving \coq{in_regex_p}, we define
the following notation \coq{s <$- e ; s1 ; s2$} for \coq{in_regex_p s e s1 s2}.
The meaning of \coq{in_regex_p} is the same as the rules of our semantics
in Figure~\ref{figure:bigstepsemantics} and we omit redundat explanations for
brevity.

The soundness and completeness theorems of the proposed semantics are stated below.
Both are proved by induction on the complexity of the pair $(e,s)$.
\begin{minted}{coq}
Theorem in_regex_p_complete : 
forall e s, s <<- e -> forall s', (s ++ s') <$- e ; s ; s'.
Theorem in_regex_p_sound : 
forall e s s1 s', s <$- e ; s1 ; s' -> s = s1 ++ s' /\ s1 <<- e.
\end{minted}
The completeness express that if an string \coq{s} is in the language of RE
\coq{e}, i.e. \coq{s <<- e}, then our semantics can parse the string \coq{s ++
	s'}, for any string \coq{s'}. Soundness theorem says that whenever we have
a derivation of \coq{s <$- e ; s1 ; s'}, then we have that the input string
\coq{s} should be equal to the concatenation of the parsed prefix (\coq{s1}) and
the remainder (\coq{s'}), i.e. \coq{s = s1 ++ s'}, and the parsed prefix should
be in \coq{e}'s language (\coq{s1 <<- e}).

After a proper definition of our semantics, we developed a formalized
interpreter for it. First, we need to define a type to store the
intermediate results of the VM. We call this type \coq{result} and its
definition is shown below.
\begin{minted}{coq}
Record result : Set
:= Result {
	bitcode   : code
	; consumed  : string  
	; remaining : string
}.
\end{minted} 
Type \coq{result} has a obvious meaning: it stores the computed bit-coded parse
tree, the consumed prefix of the input string and its remaining suffix. Using
type \coq{result}, we can define the specification of our interpreter as:
\begin{minted}{coq}
Definition interp
: forall e s,
{{r | exists e', unproblematic e' /\ e === e'  /\
		s = consumed r ++ remaining r /\
		(consumed r ++ remaining r) <$- e' ; consumed r ; remaining r /\
		(bitcode r) :# e'}}. 
\end{minted}
Function \coq{interp} is defined as follows: first it converts the input RE into
an equivalent unproblematic one and then proceed to parse the input string by
well-founded recursion on the complexity of the pair $(e,s)$. In its definition,
we follow the same pattern used before: the computational content is specified
using tactic \coq{refine} marking proof positions using holes that are filled
latter by tactics.

\section{Extracting a certified implementation}

In order to obtain a certified Haskell implementation of our VM-based algorithm,
we use Coq support for extraction, which has several pre-defined
settings for using data-types and functions of Haskell's
Prelude\footnote{Prelude is the name of the Haskell library automatically loaded
in any Haskell module~\cite{Haskell98}.}.


\chapter{Related work}\label{section:relatedwork}
%Starting missing references in October/November 2018 according Rodrigo's task board in Trello

A new technique for constructing a finite deterministic automaton from a RE was presented by Asperti et al in \cite{Asperti10}. It's based on the idea of marking a suitable set of positions inside the RE, intuitively representing the possible points reached after the processing of an initial prefix of the input string. In other words, the points mark the positions inside the RE which have been reached after reading some prefix of the input string, or better positions where the processing of the remaining string has to be started. Each pointed expression for a RE $e$ represents a state of the deterministic automaton associated with $e$; since there is obviously only a finite number of possible labellings, the number of states of the automaton is finite. The authors argued that Pointed REs join the elegance and the symbolic appealingsness if Brzozowski's derivatives with the effectiveness of McNaughton and Yamada's labelling technique, essentially combining the best of the two approaches, allowing a direct, intuitive and easily verifiable construction of the deterministic automaton for $e$. The authors said that pointed expressions can provide a more compact description for RLs than traditional REs; however, despite the many proofs presented in this paper, the authors did not show any evidence about that statement.

Ausaf et al approached the POSIX disambiguation strategy. In \cite{Ausaf16}, they gave their inductive definition of what a POSIX value is and showed that such a value is unique for a given RE and a string being matched. They also proved the correctness of an optimized version of the POSIX matching algorithm. They said their definitions and proofs are much simpler than another existing in literature and can be easily formalized in a proof assistant. One interesting point of this work is that the authors alleged they tried to formalize an original proof provided by another work about POSIX, but they believe that the other approach has ``unfillable gaps" and that these gaps cannot be filled easily.

Br{\"{u}}ggemann-Klein \cite{Bruggemann-Klein1992} showed that the Glushkov automaton can be constructed in a time quadratic in the size of the RE, and that this is worst-case optimal and output sensitive. For deterministic REs, her algorithm has even linear run time. This improves on the cubic methods suggested in the literature. She also showed that, under a technical condition, a RE is strongly unambiguous if and only if it is weakly unambiguous and in the star-normal form, a concept that she denotes by transforming a RE $E$ in linear time into a RE $E^ \bullet$. She also provided a quadratic-time decision algorithm for weak unambiguity, which improves on the biquadratic method introduced by another work in the literature. Although her paper had no formal proofs and was focused in time complexity, the author stated - based in one of her references - that strong unambiguity of REs can be reduced in linear time to unambiguity of $\varepsilon$-NFA's via Thompson's construction, which is the one we based to do this work.

The concept of prioritized transducers to formalize capturing groups in RE matching was introduced by Berglund and Merwe \cite{Berglund2016}. Their main goal was to provide an automata-based theoretical foundation for the basic functionality of modern RE matchers (with a focus on the Java RE standard library). Many RE matching libraries perform matching as a form of parsing by using capturing groups, and thus output what subexpression matched which substring. Their approach permits an analysis of matching semantics of a subset of the REs supported in Java. According to the authors, converting REs to what they called as prioritized transducers is a natural generalization of the Thompson construction for REs to NFA.

A method of obtaining a $\lambda$-free automaton from RE is presented by García et al \cite{Garcia2011}. In their proposal, the number of states of the automata they obtain is bounded above by the size of both the partial derivatives (Antimirov) and of the follow automata (Illie an Yu \cite{Ilie2003}). Their algorithm also runs with the same time complexity of these methods. Although they mentioned Thompson's automaton as one of the first methods to do the task of representing REs as automata, their work did not present any formal proofs about the correctness of their proposed algorithm and their main concern seemed to be the efficiency of their algorithm, not its correctness.

Berry and Sethi \cite{Berry1986} presented a study about two well-known algorithms for constructing a finite automaton from a RE. Their main idea is to allow an elegant algorithm to be refined into an efficient one. The elegant algorithm is based on `derivatives' of REs; the efficient one is based on `marking of' REs. They showed with proofs that it is possible to move from the derivative approach to the marking one without losing the benefits of both approaches. However, intersection and complement (which are additional operators for REs) cannot be handled because the marking and unmarking processes do not preserve the languages generated by REs with these operators.

A formal constructive theory of RLs was presented by Doczkal et al in \cite{Doczkal2013}. They formalized some fundamental results about RLs. For their formalization, they used the Ssreflect extension to Coq, which features an extensive library with support for reasoning about finite structures such as finite types and finite graphs. They established all of their results in about 1400 lines of Coq, half of which are specifications. Most of their formalization deals with translations between different representations of RLs, including REs, DFAs, minimal DFAs and NFAs. They formalized all these (and other) representations and constructed computable conversions between them. Besides other interesting aspects of their work, they proved the decidability of language equivalence for all representations. 

Groz and Maneth \cite{Groz2017} approached the efficiency of testing and matching of deterministic REs. They presented a linear time algorithm for testing whether a RE is deterministic and an efficient algorithm for matching words against deterministic REs. It was shown that an input word of length $n$ can be matched against a deterministic RE of length $m$ in time $O (m + n  \log \log m)$. If the deterministic RE has bounded depth of alternating union and concatenation operators, then matching can be performed in time $O (m + n)$. According to the authors, these results extend to REs containing numerical occurrence indicators. The authors presented the concept of deterministic REs and the differences between weak and strong determinism. Their paper contains some proofs, many of them related to algorithmic running time. However, their approach was focused on performance over deterministic REs, leaving aside the non-deterministic ones.

Radanne and Thiemann \cite{radanne:hal-01788827} pointed that some of the algorithms for RE matching are rather intricate and the natural question that arises is how to test these algorithms. It is not too hard to come up with generators for strings that match a given RE, but on the other hand, the algorithms should reject strings that do not match that RE. So it is equally important to come up with strings that do not match. In other words, a satisfactory solution for testing such matchers would require generating positive as well as negative examples for some language. Thus, the authors presented an algorithm to generate the language of a generalized RE with union, intersection and complement operators. Using this technique, they could generate both positive and negative instance of a RE. They provided two implementations: one in Haskell, which explores different algorithmic improvements, and one in OCaml, which evaluates choices in data structures. Their algorithm lacks of correctness proofs. They said that they would like to implement their algorithm in Agda and prove its correctness and its productivity.

%end of references in October/November 2018

%Starting new references in September 2018

Ilie and Yu \cite{Ilie2003} presented two algorithms for constructing nondeterministic finite automata (NFA) from REs. The first one constructs NFAs with $\varepsilon$-transitions ($\varepsilon$-NFA) which are smaller than all other $\varepsilon$-NFAs obtained by similar constructions. The second one constructs NFAs by removing $\varepsilon$-elimination in the $\varepsilon$-NFAs they just introduced and builds a quotient of the well-known position automaton with respect to the equivalence given by the follow relation, named by the authors as \textit{follow automaton}, which uses optimally the information from the positions of a RE. The authors compared the follow automaton with the best existing constructions in their time (position, partial derivative, and common follow sets automata) and concluded that their follow automaton has interesting properties: it is always a quotient of the position automaton, is very easy to compute and is at least as small as all the other similarly constructed automata in most cases. Among the several problems pointed by the authors that should be investigated further, it should be done a more rigorous comparison between the follow automaton and common follow sets or partial derivative automaton. According to the authors, ``probably the only way to decide which one is better is by testing all of them in real-life applications".


Spivey approached the theme of parser combinators in \cite{Spivey2012}. The main idea is that a parser of phrases of a type $\alpha$ is a function that takes an input string and produces results $(x,rest)$ in which $x$ is a value of type $\alpha$ and $rest$ is the remainder of the input after the phrase with value $x$ has been consumed. The results are often arranged into a list, because this allows a
parser to signal failure with the empty list of results, an unambiguous success with one result, or multiple possibilities with a longer ‘list of successes’ - we will call this approach as type $List$. Producing a list of results naturally leads to backtracking parsers that can be exponentially slow, so it is preferable when possible to replace the type $List$ by a different parser type, known in Haskell as type $Maybe$. Using this parser type reduces the amount of fruitless searching and permits the record of choices made in recognizing a phrase to be discarded as soon as one of the choices succeeds. In an unambiguous grammar, an input string will either fail to be in the language or will have exactly one derivation tree. The author says that a parser \textit{works correctly} if it has type $List$ and it returns $[\,]$ and $[(x, ``\,")]$ or if it has type $Maybe$ and returns $Nothing$ and $Just (x, ``\,")$ in both cases. According to the author, both $List$ and $Maybe$ types work correctly for any grammar that has no left recursion. On the other hand, grammars that are $LL(1)$ can be parsed with no backtracking at all and both types of parsers work correctly. The result reported in Spivey's paper is that it is not decidable whether a $Maybe$-based parser will continue to work correctly for cases in which the grammars `are not quite $LL(1)$'.



The main goal of Medeiros et al's work \cite{Mascarenhas2011} is to present a new formalization of REs via transformation to PEGs and show that their formalization accommodates some of regex \footnote{Regexes add several ad-hoc extensions to REs. They may look like REs, but can have syntactical and semantical extensions that are difficult - or impossible - to express through pure REs.} extensions. They present formalizations of both REs and PEGs in the framework of natural semantics and use these to show the similarities and differences between REs and PEGs. Then, they define a transformation that converts a RE to a PEG and prove its correctness. Finally, they show how this transformation can be adapted to accommodate some regex extensions. One of many interesting points of their work is that they also show how to obtain a well-formed \footnote{A RE $e$ that has a subexpression $e^*_i$ where $e_i$ can match the empty string is not well-formed.} RE that recognizes the same language as non-well-formed REs.
 

%End of references in September 2018.

Ierusalimschy \cite{Ierusalimschy2009} proposed the use of Parsing Expression Grammars (PEGs) as a basis for pattern matching. He argued that pure REs have proven to be a too weak formalism for that task: many interesting patterns either are difficult to describe or cannot be described by REs. He also said that the inherent non-determinism of REs does not fit the need to capture specific parts of a match. Following this proposal, he presented LPEG, a pattern-matching tool based on PEGs for the Lua scripting language. He argued that LPEG unifies the ease of use of pattern-matching tools with the full expressive power of PEGs. He also presented a parsing machine (PM) that allows an implementation of PEGs for pattern matching. The author presented no proofs of the PM's correctness. Besides, there is no guarantee that his LPEG implementation follows his specification.

%\section{A Parsing Machine for PEGS}

However, Medeiros and Ierusalimschy \cite{Medeiros2008} presented a new approach for implementing PEGs, based on a virtual parsing machine (VM). Each PEG has a corresponding program that is executed by the parsing machine, and new programs are dynamically created and composed. They gave an operational semantics of PEGs used for pattern matching, then described their parsing machine and its semantics.
They showed how to transform PEGs to parsing machine programs, and gave a correctness proof of their compiler transformation. This work is more similar to ours, once that we also intend to develop a VM for parsing and prove its correctness. However, the proofs presented by those authors were not verified by a proof assistant. They said that the execution model of their machine cannot handle infinite loops. Furthermore, the ``star" operator for PEGs has a greedy semantics which differs from the conventional RE semantics for this operator.


%\section{Regular ExpressionMatching and Operational Semantics}
In \cite{Rathnayake2011}, Rathnayake and Thielecke formalized a VM implementation for RE matching using operational semantics. Specifically, they derived a series of abstract machines, moving from the abstract definition of matching to realistic machines. First, a continuation is added to the operational semantics to describe what remains to be matched after the current expression. Next, they represented the expression as a data structure using pointers, which enables redundant searches to be eliminated via testing for pointer equality. We show more details about their VM's specification in Subsection \ref{subsection:reoperationalsemantics}. Although their work has some similarities with ours (a VM-based parsing of REs), they presented no proof of the VM correctness, and they did not mention what disambiguation policy was followed by their semantics (assuming that they used some policy). Their focus is performance rather correctness.

%\section{A Play on Regular Expressions}

Fischer, Huch and Wilke \cite{Fischer2010} developed a Haskell program for matching REs. The program is purely functional and it is overloaded over arbitrary semirings, which solves the matching problem and supports other applications like computing leftmost longest matchings or the number of matchings. Their program can also be used for parsing every context-free language by taking advantage of laziness. Their developed program is based on an old technique to turn REs into finite automata, which makes it efficient compared to other similar approaches. One advantage of their implementation over our proposal is that their approach works with context-free languages, not only with REs purely. However, they did not present any correctness proofs of their Haskell code.

%\section{Regular Expression Matching: the Virtual Machine Approach}

Cox \cite{Cox2009} said that viewing RE matching as executing a special machine makes it possible to add new features just by adding and implementing new machine instructions. He presented two different ways to implement a VM that executes a RE that has been compiled into text-machine byte-codes: a recursive and a non-recursive backtracking implementation, both in C programming language. Cox’s work on VM-based RE parsing has some problems. First, it is poorly specified: both the VM semantics and the RE compilation process are described only informally and no correctness guarantees is even mentioned. Second, it does not provide a specific disambiguation strategy for dealing with ambiguous REs: both the ``star" and ``choice" operators for REs can introduce more than one possible way to process a given string.

%\section{Greedy Regular Expression Matching}

Frisch and Cardelli \cite{Frisch2004} studied the theoretical problem of matching a flat sequence against a type (RE): the
result of the process is a structured value of a given type. Their contributions were in noticing that: (1) A disambiguated result of parsing can be presented as a data structure that does not contain ambiguities. (2) There are problematic cases in parsing values of star types that need to be disambiguated. (3) The disambiguation strategy used in XDuce and CDuce (two XML-oriented functional languages) pattern matching can be characterized mathematically by what they call greedy RE matching. (4) There is a linear time algorithm for the greedy matching. Their approach is different since they want to axiomatize abstractly the disambiguation policy, without providing an explicit matching algorithm. They identify three notions of problematic words, REs, and values (which represent the ways to match words), relate these three notions, and propose matching algorithms to deal with the problematic case. The authors did not propose parsing RE techniques. Their goal was to propose solutions to problems related to matching disambiguation, which includes the kleene star case. The authors, however, did not provide any proof about their disambiguation strategies, which seems to be too complex to apply to a VM-based approach.

%\section{Certified Bit-Coded Regular Expression Parsing}

Ribeiro and Bois \cite{Ribeiro2017} described the formalization of a RE parsing algorithm that produces a bit representation
of its parse tree in the dependently typed language Agda. The algorithm computes bit-codes using Brzozowski derivatives and they proved that the produced codes are equivalent to parse trees ensuring soundness and completeness with regard to an inductive RE semantics. They included the certified algorithm in a tool developed by themselves, named verigrep, for RE-based search in the style of GNU grep. While the authors provided formal proofs, their tool showed a poor performance when compared with other approaches to RE parsing. Besides, they
didn't prove that their algorithm follows some disambiguation policy, like POSIX or greedy.

%\section{Bit-coded Regular Expression Parsing}

Nielsen and Henglein \cite{Nielsen2011} showed how to generate a compact \textit{bit-coded} representation of a parse tree for a given RE efficiently, without explicitly constructing the parse tree first, by simplifying the DFA-based parsing algorithm to Dubé and Feeley \cite{Dube2000} to emit the bits of the bit representation without explicitly materializing the parse tree itself. They also showed that Frisch and Cardelli’s greedy RE parsing algorithm \cite{Frisch2004} can be straightforwardly modified to produce bit codings directly. They implemented both solutions as well as a backtracking parser and performed benchmark experiments to gauge their practical performance. They argued that bit codings are interesting in their own right since they are typically not only smaller than the parse tree, but also smaller than the string being parsed and can be combined with other techniques for improved text compression. As others related works, the authors did not present a formal verification of their implementations. Furthermore, their main goal was efficiency, not correctness.

%Starting new references in 2019 January

A recent application of REs was presented by Radanne \cite{Radanne2019}. In many cases, the goal of a RE is not only to match a given text, but also to extract information from it. With that in mind, the author presented a technique to provide type-safe extraction based on the typed interpretation of REs. That technique relies on two-layer REs in which the upper layer allows to compose and transform data in a well-typed way, while the lower one is composed by untyped REs that can leverage features from a preexisting RE matching engine. Results showed that this technique is faster than other two libraries that perform the same task, despite its lack of efficiency when compared with some full RE parsing algorithms. No formalization was provided in that work.


%Ending new references in 2019 January


%-----------------------------------------------------------------------------------------------------------

\chapter{Preliminary Results}\label{chapter:results}

In this chapter, we present some preliminary results. We have developed a primary version for our proposed VM. We implemented its semantics in Haskell and used \textit{QuickCheck} over it for property-based testing.

\textit{QuickCheck} is a combinator library originally written in Haskell, designed to assist in software testing by generating test cases for test suites. The programmer writes assertions about logical properties that a function should fulfill. Then, \textit{QuickCheck} attempts to generate a test case that falsifies these assertions. Once such a test case is found, \textit{QuickCheck} tries to reduce it to a minimal failing subset by removing or simplifying input data that are not needed to make the test fail. In this work, we used \textit{QuickCheck} to generate random REs and random strings that can be parsed by them.

After testing, we noticed that our VM still do not cover all cases. Specially, it is not working properly on REs that use the Kleene star operator followed by a concatenation (for example, cases like $a^*a$). Therefore, we are working on another semantics version for the VM, which is intended to cover all cases.

We present below the primary version of the developed VM, detailing its semantics and giving execution examples.

\section{First version of the VM}

The semantics of our developed VM follows the structure

\[
\langle E \,;\, K \,;\, B \,;\, W \rangle
\]

where:

\begin{itemize}
	\item $E$: The current RE executed by the machine. 
	\item $K$: Continuation stack. It contains the ordered REs to be executed just after the current RE execution.
	\item $B$: Backtracking stack. It stores triples in the format $(E,K,W)$ that  will be used for the machine backtracking. In case of failure, the machine returns to the state composed by the expression $E$, by the continuation stack $K$ and by the remaining word $W$ on the top of the backtracking stack.
	\item $W$: The processed word.
\end{itemize}

The semantics details are shown in next definition.

\begin{Definition}\label{definition:MachineSemantis}
	Semantics for our VM:
	
	\[
		\begin{array}{lrclr}
		(1) & \langle \lambda \,;\, eK\,;\, B\,;\, w\rangle & \rightarrow & \langle e \,;\,K\,;\,B\,;\,w\rangle & \\
		(2) & \langle a \,;\, K \,;\, B\,;\,aw \rangle & \rightarrow & \langle \lambda \,;\, K \,;\, B\,;\, w\rangle & \\
		(3) & \langle a \,;\, K \,;\, (e,K',w'):B \,;\, bw \rangle & \rightarrow & \langle e \,;\, K' \,;\, B \,;\, w' \rangle & \text{(if a $\neq$ b)}\\
		(4) & \langle ee' \,;\, K \,;\, B \,;\, w \rangle & \rightarrow & \langle e \,;\, e':K \,;\, B \,;\, w \rangle & \\
		(5) & \langle e+e' \,;\, K \,;\, B \,;\, w \rangle & \rightarrow & \langle e \,;\, K \,;\, (e',K,w): B \,;\, w \rangle &\\
		(6) & \langle e^* \,;\, K \,;\, B \,;\, w \rangle & \rightarrow & \langle e \,;\, e^*:K \,;\, \lambda:B \,;\, w \rangle & \\
		(7) & \langle e^* \,;\, K \,;\, \lambda : B \,;\, w \rangle & \rightarrow & \langle e \,;\, e^*:K \,;\, \lambda:B \,;\, w \rangle & \\\\
		\end{array}
	\]
	In (1), whenever the current RE is empty ($\lambda$) and there is a word to be matched ($w$), the next part of the RE stored in K ($e$ in $eK$) goes to execution. In (2) we have the match case. In (3), the VM restores itself to the previous valid VM state stored in $B$ in case of failure. In (4), the left hand side of a RE concatenation goes to execution, while its right hand side goes to the top of the continuation stack. In (5), the left hand side of a RE disjunction goes to execution, while its right hand side goes to the backtracking stack together with the current continuation stack and the remaining word to be processed. That will be useful in case of matching failure of the right hand side of the disjunction. In (6), we have the Kleene star case: the top of the RE goes to execution, while the remaining Kleene closure goes to the top of the current continuation stack. We put $\lambda$ on the top of the backtracking stack because the matching can fail with $e$ but succeed with $\lambda$ (that belongs to the Kleene closure). The last instruction (7) avoids (6) to add successive $\lambda$ to the backtracking stack: if $\lambda$ is already the top of $B$, no extra $\lambda$ is added; B remains the same.
\end{Definition}

In next example, we present the execution steps for a matching success case.

\begin{Example}
	Let us consider the alphabet $\Sigma = \{a, b\}$, the RE $ab (a + b)$ and the word $abb$. For the proposed VM, we have:
	\[
		\begin{array}{rcl}
			& & \langle ab(a+b) \,;\, [] \,;\, [] \,;\, abb \rangle \\
			(4) & \rightarrow & \langle a \,;\, [b(a+b)]:[] \,;\, [] \,;\, abb \rangle \\
			(2) & \rightarrow & \langle \lambda \,;\, [b(a+b)]:[] \,;\, [] \,;\, bb \rangle \\
			(1) & \rightarrow & \langle b \,;\, [(a+b)]:[] \,;\, [] \,;\, bb \rangle \\
			(2) & \rightarrow & \langle \lambda \,;\, [(a+b)]:[] \,;\, [] \,;\, b \rangle \\
			(1) & \rightarrow & \langle (a+b) \,;\, [] \,;\, [] \,;\, b \rangle \\
			(5) & \rightarrow & \langle a \,;\, [] \,;\, [(b,[],b)]:[] \,;\, b \rangle \\
			(3) & \rightarrow & \langle b \,;\, [] \,;\, [] \,;\, b \rangle \\
			(2) & \rightarrow & \langle \lambda \,;\, [] \,;\, [] \,;\, \lambda \rangle \\
		\end{array}
	\]
	where the number in the left hand side shows which instruction of the semantics in Definition \ref{definition:MachineSemantis} was used to get the corresponding state on the right hand side. The initial configuration is $\langle ab(a+b) \,;\, [] \,;\, [] \,;\, abb \rangle$ and the final state is $\langle \lambda \,;\, [] \,;\, [] \,;\, \lambda \rangle$, which means there is no more REs to be executed and the whole word $w$ was successfully matched.
\end{Example}

For more than 10,000 REs and strings generated by \textit{QuickCheck}, we noticed that our VM's semantics was not working properly in cases where a Kleene star operator is followed by a concatenation. A step by step execution when this problem arises is shown in next example.

\begin{Example}
	Let it be the alphabet $\Sigma = \{a,b\}$, the RE $a*a$ and the word $aaa$:
	\[
		\begin{array}{rcl}
		& & \langle a^*a \,;\, [] \,;\, [] \,;\, aaa \rangle \\
			(4) & \rightarrow & \langle a^* \,;\, a:[] \,;\, [] \,;\, aaa \rangle \\
			(6) & \rightarrow & \langle a \,;\, a^*:a:[] \,;\, \lambda:[] \,;\, aaa \rangle \\
			(2) & \rightarrow & \langle \lambda \,;\, a^*:a:[] \,;\, \lambda:[] \,;\, aa \rangle \\
			(1) & \rightarrow & \langle a^* \,;\, a:[] \,;\, \lambda:[] \,;\, aa \rangle \\
			(7) & \rightarrow & \langle a \,;\, a^*:a:[] \,;\, \lambda:[] \,;\, aa \rangle \\
			(2) & \rightarrow & \langle \lambda \,;\, a^*:a:[] \,;\, \lambda:[] \,;\, a \rangle \\
			(1) & \rightarrow & \langle a^* \,;\, a:[] \,;\, \lambda:[] \,;\, a \rangle \\
			(7) & \rightarrow & \langle a \,;\, a^*:a:[] \,;\, \lambda:[] \,;\, a \rangle \\
			(2) & \rightarrow & \langle \lambda \,;\, a^*:a:[] \,;\, \lambda:[] \,;\, \lambda \rangle \\
			(1) & \rightarrow & \langle a^* \,;\, a:[] \,;\, \lambda:[] \,;\, \lambda \rangle \\
			(7) & \rightarrow & \langle a \,;\, a^*:a:[] \,;\, \lambda:[] \,;\, \lambda \rangle \, \text{(Error)} \\\\
		\end{array}
	\]
	the word $aaa$ is accepted by the RE $a^*a$, but it is not accepted by our VM. That happens because the way the VM's semantics is defined does not specify whether the star ($^*$) operator stops matching for cases like this; thus, it will consume the word as much as possible and will always try to match a non-empty RE with $\lambda$, which will always fail.
\end{Example}

%-----------------------------------------------------------------------------------------------------------


\chapter{Schedule and Expected Results}\label{chapter:schedule}

This chapter presents the next steps for the development of this dissertation and estimated deadlines as well. They are not precise and can change regardless of our will, depending on the results of this research and the acceptance (or not) of the products we intend to build by the related events and publications.

The estimated remaining activities are enumerated below:

\begin{enumerate}
	\item Define the small-step semantics for the RE VM.
	\item Develop informal correctness proofs about the developed semantics - equivalence with standard RE semantics.
	\item Implement a type for the developed semantics in Haskell. 
	\item Use property-based testing to verify the correctness of the developed algorithm.
	\item Formal verification of the semantics using Coq proof assistant.
	\item Formal verification of the RE parsing algorithm using Coq proof assistant.
	\item Use of the verified algorithm in RE-based text search tools.
	\item Realization of experiments.
	\item Writing of research papers and dissertation.
	\item Dissertation defense.
\end{enumerate}

The activities listed above are summarized in the next table. The deadlines are just an estimation. Things may change depending on the development of the work. For example, the dissertation defense can take place before March 2019.

\begin{table}[]
	\centering
	\caption{Estimated remaining dissertation activities}
	\label{my-label}
	\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|}
		\cline{2-13}
		\multicolumn{1}{l|}{}      & \multicolumn{12}{c|}{MONTHS}                                                                                                                                                                                                                                                                                                      \\ \hline
		\multicolumn{1}{|c|}{ACT.} & \multicolumn{1}{l|}{APR} & \multicolumn{1}{l|}{MAY} & \multicolumn{1}{l|}{JUN} & \multicolumn{1}{l|}{JUL} & \multicolumn{1}{l|}{AUG} & \multicolumn{1}{l|}{SEP} & \multicolumn{1}{l|}{OCT} & \multicolumn{1}{l|}{NOV} & \multicolumn{1}{l|}{DEC} & \multicolumn{1}{l|}{JAN} & \multicolumn{1}{l|}{FEB} & \multicolumn{1}{l|}{MAR} \\ \hline
		\multicolumn{1}{|c|}{1}    & X                        & X                        & X                        &                          &                          &                          &                          &                          &                          &                          &                          &                          \\ \hline
		\multicolumn{1}{|c|}{2}    &                          &                          & X                        & X                        &                          &                          &                          &                          &                          &                          &                          &                          \\ \hline
		\multicolumn{1}{|c|}{3}    &                          &                          &                          & X                        & X                        & X                        &                          &                          &                          &                          &                          &                          \\ \hline
		\multicolumn{1}{|c|}{4}    &                          &                          &                          &                          & X                        & X                        &                          &                          &                          &                          &                          &                          \\ \hline
		\multicolumn{1}{|c|}{5}    &                          &                          &                          &                          &                          & X                        & X                        & X                        &                          &                          &                          &                          \\ \hline
		\multicolumn{1}{|c|}{6}    &                          &                          &                          &                          &                          & X                        & X                        & X                        &                          &                          &                          &                          \\ \hline
		\multicolumn{1}{|c|}{7}    &                          &                          &                          &                          &                          &                          &                          & X                        & X                        &                          &                          &                          \\ \hline
		\multicolumn{1}{|c|}{8}    &                          &                          &                          &                          &                          &                          &                          & X                        & X                        & X                        &                          &                          \\ \hline
		\multicolumn{1}{|c|}{9}    & X                        & X                        & X                        & X                        & X                        & X                        & X                        & X                        & X                        & X                        & X                        &                          \\ \hline
		\multicolumn{1}{|c|}{10}   &                          &                          &                          &                          &                          &                          &                          &                          &                          &                          &                          & X                        \\ \hline
	\end{tabular}
\end{table}

Along the development of this work, we intend to produce some papers with eventual relevant contributions and submit them to one (or more) of the following events or publications:

\begin{itemize}
	\item Brazilian Symposium on Programming Languages.
	\item Brazilian Symposium on Formal Methods.
	\item Journal of the Brazilian Computer Society.
	\item Applied and Theoretical Informatics Magazine (RITA - \textit{Revista de Informática Teórica e Aplicada}).
\end{itemize}



%-----------------------------------------------------------------------------------------------------------


\chapter{Conclusion}\label{chapter:conclusion}

This work proposes the development of a VM for parsing RLs, in such a way that a RE is merely a program executed by the VM over the input string. We also intend to verify theoretical properties of the semantics of the developed VM and prove the correctness of its parsing algorithm. 

We presented a preliminary version of our parsing machine, implemented it in Haskell and used \textit{QuickCheck} over our implementation in order to perform property-based tests: \textit{QuickCheck} generated REs and strings (both randomly) that can be parsed by those REs.

We noticed that our preliminary semantics still does not cover all cases. Specially, it is not working properly on REs that use the Kleene star operator followed by a concatenation (for example, cases like $a^*a$). We are already working on another version of the VM, which covers  all cases. In the final version of this dissertation, we intend to present the second version of our VM, provide its formal correctness and show comparison results with other approaches for RE parsing.

All implementation codes of this work can be found at \begin{center}\url{https://github.com/thalesad/Implementations}\end{center} and the \LaTeX\ code of this proposal can be found at \begin{center} \url{https://github.com/thalesad/Dissertation} \end{center}.


\bibliographystyle{plain}
\bibliography{Thales}

\appendix

\chapter{Correctness of the \ensuremath{\F{accept}} function}\label{appendix:accept}


Fisher et. al.~\cite{Fischer2010} presents a simple and elegant function for parsing a string using a RE. It relies on two
auxiliary functions that break an input string into its parts. The first is function \ensuremath{\F{split}} which decompose the input string in
a prefix and a suffix.
\begin{hscode}\SaveRestoreHook
	\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
	\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
	\>[B]{}\F{split}\C{::}[\mskip1.5mu \V{a}\mskip1.5mu]\to [\mskip1.5mu ([\mskip1.5mu \V{a}\mskip1.5mu],[\mskip1.5mu \V{a}\mskip1.5mu])\mskip1.5mu]{}\<[E]%
	\\
	\>[B]{}\F{split}\;[\mskip1.5mu \mskip1.5mu]\mathrel{=}[\mskip1.5mu ([\mskip1.5mu \mskip1.5mu],[\mskip1.5mu \mskip1.5mu])\mskip1.5mu]{}\<[E]%
	\\
	\>[B]{}\F{split}\;(\V{c}\mathbin{:}\V{cs})\mathrel{=}([\mskip1.5mu \mskip1.5mu],\V{c}\mathbin{:}\V{cs})\mathbin{:}[\mskip1.5mu (\V{c}\mathbin{:}\V{s1},\V{s2})\mid (\V{s1},\V{s2})\leftarrow \F{split}\;\V{cs}\mskip1.5mu]{}\<[E]%
	\ColumnHook
\end{hscode}\resethooks
Function \ensuremath{\F{split}} has the following correctness property.
\begin{Lemma}
	Let \ensuremath{\V{xs}} be an arbitrary list. For all \ensuremath{\V{ys}}, \ensuremath{\V{zs}} such that \ensuremath{(\V{ys},\V{zs})} $\in$ \ensuremath{\F{split}\;\V{xs}}, we have that \ensuremath{\V{xs}\equiv \V{ys}\plus \V{zs}}.
\end{Lemma}
\begin{proof}
	By induction on the structure of \ensuremath{\V{xs}}. 
\end{proof}
Function \ensuremath{\F{parts}} decomposes a string into a list of its parts. Such property is expressed by the following lemma.
\begin{Lemma}
	Let \ensuremath{\V{xs}} be an arbitrary list. For all \ensuremath{\V{yss}} such that \ensuremath{\V{yss}} $\in$ \ensuremath{\F{parts}\;\V{xs}}, we have that \ensuremath{\F{concat}\;\V{yss}\equiv \V{xs}}.
\end{Lemma}
\begin{proof}
	By induction on the structure of \ensuremath{\V{xs}}.
\end{proof}
Finally, function \ensuremath{\F{accept}} is defined by recursion on the input RE using functions \ensuremath{\F{parts}} and \ensuremath{\F{split}} in the
Kleene star and concatenation cases. The correctness of \ensuremath{\F{accept}} states that it returns true only when the
input string is in input RE's language, as stated in the next theorem.
\begin{Theorem}
	For all \ensuremath{\V{s}} and \ensuremath{\V{e}}, \ensuremath{\F{accept}\;\V{e}\;\V{s}\equiv \C{True}} if, and only if, $s\in\sembrackets{e}$.
\end{Theorem}
\begin{proof}
	$\,$\\
	\begin{itemize}
		\item[$(\to)$]: By induction on the structure of \ensuremath{\V{e}} using lemmas about \ensuremath{\F{parts}} and \ensuremath{\F{split}}.
		\item[$(\leftarrow)$]: By induction on the derivation of $s \in\sembrackets{e}$.
	\end{itemize}
\end{proof}

\end{document}